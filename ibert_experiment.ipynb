{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47lAsvQJmyBa",
        "outputId": "af02cf34-7aec-442c-fd3a-096396670842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBq9qgM_nuq-",
        "outputId": "b43237e9-516f-45bf-94bc-1d9c6eb89e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/coms6998_project\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/My Drive/coms6998_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpQn5mcSn7F5",
        "outputId": "e3a575e9-4d37-460c-d890-f1d71f0d411a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'I-BERT' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kssteven418/I-BERT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xkmyfM8n-dp",
        "outputId": "f3f84cb7-6d7b-450f-caec-e5e2315d278c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/coms6998_project/I-BERT\n"
          ]
        }
      ],
      "source": [
        "%cd I-BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW2HXlTGoAnZ",
        "outputId": "36c4f57c-f841-463b-e31c-cf56f60c5524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/gdrive/My%20Drive/coms6998_project/I-BERT\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from fairseq==0.9.0) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fairseq==0.9.0) (1.21.6)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.8/dist-packages (from fairseq==0.9.0) (0.5.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from fairseq==0.9.0) (0.29.32)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.8/dist-packages (from fairseq==0.9.0) (1.15.1)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 26.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fairseq==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from fairseq==0.9.0) (2022.6.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi->fairseq==0.9.0) (2.21)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu->fairseq==0.9.0) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu->fairseq==0.9.0) (4.9.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->fairseq==0.9.0) (4.4.0)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed colorama-0.4.6 fairseq portalocker-2.6.0 sacrebleu-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --editable ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yavZbHhtpGpV",
        "outputId": "161ce2d1-2fad-4f84-a62f-e7dadbe81b7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘models’: File exists\n"
          ]
        }
      ],
      "source": [
        "%mkdir models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpxrUEIPpqTw",
        "outputId": "d2704eb7-704e-44e9-c2a3-3fcbe971b2fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/coms6998_project/I-BERT/models\n"
          ]
        }
      ],
      "source": [
        "%cd models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd7GKNUbpRYV",
        "outputId": "cf2ec6e8-c404-4a36-9b1a-dd552b534ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-12 00:05:40--  https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231160875 (220M) [application/gzip]\n",
            "Saving to: ‘roberta.base.tar.gz’\n",
            "\n",
            "roberta.base.tar.gz 100%[===================>] 220.45M  56.9MB/s    in 4.7s    \n",
            "\n",
            "2022-12-12 00:05:45 (47.3 MB/s) - ‘roberta.base.tar.gz’ saved [231160875/231160875]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download pretrained roberta base model\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5dbkZTnpU74",
        "outputId": "5715f57e-8b0a-42b9-b07b-efc6d83c7b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "roberta.base/\n",
            "roberta.base/dict.txt\n",
            "roberta.base/model.pt\n",
            "roberta.base/NOTE\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf roberta.base.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cTd7R4ypXLy",
        "outputId": "591ba7a4-9c8c-4deb-bdf0-f294dbbd0685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/coms6998_project/I-BERT\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zej1oQIyp5kL",
        "outputId": "a139162d-3b73-4a5d-a4be-8a2f32c6e1e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'GLUE-baselines'...\n",
            "remote: Enumerating objects: 891, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 891 (delta 1), reused 3 (delta 1), pack-reused 886\u001b[K\n",
            "Receiving objects: 100% (891/891), 1.48 MiB | 8.74 MiB/s, done.\n",
            "Resolving deltas: 100% (610/610), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nyu-mll/GLUE-baselines.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "an0Txys-vPNh"
      },
      "outputs": [],
      "source": [
        "#download glue dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-X_jbuVqJ1X",
        "outputId": "e0596d8a-3d09-40f9-ace6-e2661c4d4b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "\tError downloading standard development IDs for MRPC. You will need to manually split your data.\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "Downloading and extracting STS...\n",
            "\tCompleted!\n",
            "Downloading and extracting MNLI...\n",
            "\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.\n",
            "\tCompleted!\n",
            "Downloading and extracting QNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n",
            "Downloading and extracting WNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting diagnostic...\n",
            "\tCompleted!\n"
          ]
        }
      ],
      "source": [
        "!python GLUE-baselines/download_glue_data.py --data_dir glue_data --tasks all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQPgZiZMqfKc",
        "outputId": "3f8d57f0-c527-4e93-c6b2-f5686150e418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/coms6998_project/I-BERT\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G2Wa5dowujE5"
      },
      "outputs": [],
      "source": [
        "# preprocess glue dataset for tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9myoNv4UykY3",
        "outputId": "e4451432-a1d2-4a0e-ad0d-ed980e28c2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/env/python\n"
          ]
        }
      ],
      "source": [
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDNpvrccqlFG",
        "outputId": "6c76e9e4-381b-4aa0-de8b-462e0485da71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-12 01:06:50--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘encoder.json’ not modified on server. Omitting download.\n",
            "\n",
            "--2022-12-12 01:06:51--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘vocab.bpe’ not modified on server. Omitting download.\n",
            "\n",
            "--2022-12-12 01:06:51--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘dict.txt’ not modified on server. Omitting download.\n",
            "\n",
            "Preprocessing MNLI\n",
            "Raw data as downloaded from glue website: glue_data/MNLI\n",
            "BPE encoding train/input0\n",
            "processed 10000 lines\n",
            "processed 20000 lines\n",
            "processed 30000 lines\n",
            "processed 40000 lines\n",
            "processed 50000 lines\n",
            "processed 60000 lines\n",
            "processed 70000 lines\n",
            "processed 80000 lines\n",
            "processed 90000 lines\n",
            "processed 100000 lines\n",
            "processed 110000 lines\n",
            "processed 120000 lines\n",
            "processed 130000 lines\n",
            "processed 140000 lines\n",
            "processed 150000 lines\n",
            "processed 160000 lines\n",
            "processed 170000 lines\n",
            "processed 180000 lines\n",
            "processed 190000 lines\n",
            "processed 200000 lines\n",
            "processed 210000 lines\n",
            "processed 220000 lines\n",
            "processed 230000 lines\n",
            "processed 240000 lines\n",
            "processed 250000 lines\n",
            "processed 260000 lines\n",
            "processed 270000 lines\n",
            "processed 280000 lines\n",
            "processed 290000 lines\n",
            "processed 300000 lines\n",
            "processed 310000 lines\n",
            "processed 320000 lines\n",
            "processed 330000 lines\n",
            "processed 340000 lines\n",
            "processed 350000 lines\n",
            "processed 360000 lines\n",
            "processed 370000 lines\n",
            "processed 380000 lines\n",
            "processed 390000 lines\n",
            "BPE encoding train/input1\n",
            "processed 10000 lines\n",
            "processed 20000 lines\n",
            "processed 30000 lines\n",
            "processed 40000 lines\n",
            "processed 50000 lines\n",
            "processed 60000 lines\n",
            "processed 70000 lines\n",
            "processed 80000 lines\n",
            "processed 90000 lines\n",
            "processed 100000 lines\n",
            "processed 110000 lines\n",
            "processed 120000 lines\n",
            "processed 130000 lines\n",
            "processed 140000 lines\n",
            "processed 150000 lines\n",
            "processed 160000 lines\n",
            "processed 170000 lines\n",
            "processed 180000 lines\n",
            "processed 190000 lines\n",
            "processed 200000 lines\n",
            "processed 210000 lines\n",
            "processed 220000 lines\n",
            "processed 230000 lines\n",
            "processed 240000 lines\n",
            "processed 250000 lines\n",
            "processed 260000 lines\n",
            "processed 270000 lines\n",
            "processed 280000 lines\n",
            "processed 290000 lines\n",
            "processed 300000 lines\n",
            "processed 310000 lines\n",
            "processed 320000 lines\n",
            "processed 330000 lines\n",
            "processed 340000 lines\n",
            "processed 350000 lines\n",
            "processed 360000 lines\n",
            "processed 370000 lines\n",
            "processed 380000 lines\n",
            "processed 390000 lines\n",
            "BPE encoding dev_matched/input0\n",
            "BPE encoding dev_matched/input1\n",
            "BPE encoding dev_mismatched/input0\n",
            "BPE encoding dev_mismatched/input1\n",
            "BPE encoding test_matched/input0\n",
            "BPE encoding test_matched/input1\n",
            "BPE encoding test_mismatched/input0\n",
            "BPE encoding test_mismatched/input1\n",
            "2022-12-12 01:07:53 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-12-12 01:07:53 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='MNLI-bin/input0', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict='dict.txt', suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref='glue_data/MNLI/processed/test_matched.input0,glue_data/MNLI/processed/test_mismatched.input0', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='glue_data/MNLI/processed/train.input0', use_plasma_view=False, user_dir=None, validpref='glue_data/MNLI/processed/dev_matched.input0,glue_data/MNLI/processed/dev_mismatched.input0', wandb_project=None, workers=60)\n",
            "2022-12-12 01:07:54 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:05 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/train.input0: 392702 sents, 9957937 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:05 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:06 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/dev_matched.input0: 9815 sents, 242648 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:06 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:08 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/dev_mismatched.input0: 9832 sents, 251325 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:08 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:10 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/test_matched.input0: 9796 sents, 245720 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:10 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:12 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/test_mismatched.input0: 9847 sents, 250325 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:12 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to MNLI-bin/input0\n",
            "2022-12-12 01:08:14 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-12-12 01:08:15 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='MNLI-bin/input1', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict='dict.txt', suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref='glue_data/MNLI/processed/test_matched.input1,glue_data/MNLI/processed/test_mismatched.input1', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='glue_data/MNLI/processed/train.input1', use_plasma_view=False, user_dir=None, validpref='glue_data/MNLI/processed/dev_matched.input1,glue_data/MNLI/processed/dev_mismatched.input1', wandb_project=None, workers=60)\n",
            "2022-12-12 01:08:15 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:22 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/train.input1: 392702 sents, 5154465 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:22 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:24 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/dev_matched.input1: 9815 sents, 128091 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:24 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:26 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/dev_mismatched.input1: 9832 sents, 137668 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:27 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/test_matched.input1: 9796 sents, 127652 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:27 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-12 01:08:29 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/test_mismatched.input1: 9847 sents, 137578 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:29 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to MNLI-bin/input1\n",
            "2022-12-12 01:08:32 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-12-12 01:08:32 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='MNLI-bin/label', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict=None, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='glue_data/MNLI/processed/train.label', use_plasma_view=False, user_dir=None, validpref='glue_data/MNLI/processed/dev_matched.label,glue_data/MNLI/processed/dev_mismatched.label', wandb_project=None, workers=60)\n",
            "2022-12-12 01:08:33 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2022-12-12 01:08:37 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/train.label: 392702 sents, 785404 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:37 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2022-12-12 01:08:38 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/dev_matched.label: 9815 sents, 19630 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:38 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2022-12-12 01:08:39 | INFO | fairseq_cli.preprocess | [None] glue_data/MNLI/processed/dev_mismatched.label: 9832 sents, 19664 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-12 01:08:39 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to MNLI-bin/label\n"
          ]
        }
      ],
      "source": [
        "!bash ./examples/roberta/preprocess_GLUE_tasks.sh glue_data "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./examples/roberta/preprocess_GLUE_tasks.sh glue_data CoLA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Wy6Gj-YQ29",
        "outputId": "d44cc91c-99e3-4c3d-b2bd-90d358f8f587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-13 16:48:00--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘encoder.json’ not modified on server. Omitting download.\n",
            "\n",
            "--2022-12-13 16:48:01--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘vocab.bpe’ not modified on server. Omitting download.\n",
            "\n",
            "--2022-12-13 16:48:01--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘dict.txt’ not modified on server. Omitting download.\n",
            "\n",
            "Preprocessing CoLA\n",
            "Raw data as downloaded from glue website: glue_data/CoLA\n",
            "BPE encoding train/input0\n",
            "BPE encoding dev/input0\n",
            "BPE encoding test/input0\n",
            "2022-12-13 16:48:15 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-12-13 16:48:16 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='CoLA-bin/input0', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict='dict.txt', suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref='glue_data/CoLA/processed/test.input0', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='glue_data/CoLA/processed/train.input0', use_plasma_view=False, user_dir=None, validpref='glue_data/CoLA/processed/dev.input0', wandb_project=None, workers=60)\n",
            "2022-12-13 16:48:16 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-13 16:48:18 | INFO | fairseq_cli.preprocess | [None] glue_data/CoLA/processed/train.input0: 8551 sents, 88681 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-13 16:48:18 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-13 16:48:19 | INFO | fairseq_cli.preprocess | [None] glue_data/CoLA/processed/dev.input0: 1043 sents, 11079 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-13 16:48:19 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-13 16:48:21 | INFO | fairseq_cli.preprocess | [None] glue_data/CoLA/processed/test.input0: 1063 sents, 11175 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-13 16:48:21 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to CoLA-bin/input0\n",
            "2022-12-13 16:48:24 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-12-13 16:48:24 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='CoLA-bin/label', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict=None, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='glue_data/CoLA/processed/train.label', use_plasma_view=False, user_dir=None, validpref='glue_data/CoLA/processed/dev.label', wandb_project=None, workers=60)\n",
            "2022-12-13 16:48:25 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2022-12-13 16:48:26 | INFO | fairseq_cli.preprocess | [None] glue_data/CoLA/processed/train.label: 8551 sents, 17102 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-13 16:48:26 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2022-12-13 16:48:27 | INFO | fairseq_cli.preprocess | [None] glue_data/CoLA/processed/dev.label: 1043 sents, 2086 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-13 16:48:27 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to CoLA-bin/label\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3isIHKB3aqH",
        "outputId": "b0c43c8d-788c-4237-9a9d-62421c33106a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 24.5 MB/s \n",
            "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 79.8 MB/s \n",
            "\u001b[?25hCollecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fairseq) (4.64.1)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 82.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from fairseq) (1.13.0+cu116)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from fairseq) (0.29.32)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.8/dist-packages (from fairseq) (2.3.1)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from fairseq) (0.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fairseq) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from fairseq) (2022.6.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.8/dist-packages (from fairseq) (1.15.1)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 81.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (5.10.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.8/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from omegaconf<2.1->fairseq) (4.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq) (3.11.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141231 sha256=ca700b63e7bf879bbc9ff25dde370f6a733391ad6cbbfa074a31cc35d1ffe552\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: omegaconf, antlr4-python3-runtime, hydra-core, bitarray, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.6.0 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./examples/roberta/preprocess_GLUE_tasks.sh glue_data SST-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApDsjxvozauy",
        "outputId": "389219c4-2470-4b75-ed04-7916d48718e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-14 18:47:59--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘encoder.json’ not modified on server. Omitting download.\n",
            "\n",
            "--2022-12-14 18:48:00--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘vocab.bpe’ not modified on server. Omitting download.\n",
            "\n",
            "--2022-12-14 18:48:01--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘dict.txt’ not modified on server. Omitting download.\n",
            "\n",
            "Preprocessing SST-2\n",
            "Raw data as downloaded from glue website: glue_data/SST-2\n",
            "BPE encoding train/input0\n",
            "processed 10000 lines\n",
            "processed 20000 lines\n",
            "processed 30000 lines\n",
            "processed 40000 lines\n",
            "processed 50000 lines\n",
            "processed 60000 lines\n",
            "BPE encoding dev/input0\n",
            "BPE encoding test/input0\n",
            "2022-12-14 18:48:18 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-12-14 18:48:18 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='SST-2-bin/input0', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict='dict.txt', suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref='glue_data/SST-2/processed/test.input0', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='glue_data/SST-2/processed/train.input0', use_plasma_view=False, user_dir=None, validpref='glue_data/SST-2/processed/dev.input0', wandb_project=None, workers=60)\n",
            "2022-12-14 18:48:18 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-14 18:48:21 | INFO | fairseq_cli.preprocess | [None] glue_data/SST-2/processed/train.input0: 67349 sents, 832407 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-14 18:48:21 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-14 18:48:22 | INFO | fairseq_cli.preprocess | [None] glue_data/SST-2/processed/dev.input0: 872 sents, 20936 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-14 18:48:22 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2022-12-14 18:48:24 | INFO | fairseq_cli.preprocess | [None] glue_data/SST-2/processed/test.input0: 1821 sents, 43269 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-14 18:48:24 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to SST-2-bin/input0\n",
            "2022-12-14 18:48:27 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-12-14 18:48:27 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='SST-2-bin/label', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang=None, srcdict=None, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='glue_data/SST-2/processed/train.label', use_plasma_view=False, user_dir=None, validpref='glue_data/SST-2/processed/dev.label', wandb_project=None, workers=60)\n",
            "2022-12-14 18:48:27 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2022-12-14 18:48:28 | INFO | fairseq_cli.preprocess | [None] glue_data/SST-2/processed/train.label: 67349 sents, 134698 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-14 18:48:28 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2022-12-14 18:48:29 | INFO | fairseq_cli.preprocess | [None] glue_data/SST-2/processed/dev.label: 872 sents, 1744 tokens, 0.0% replaced (by <unk>)\n",
            "2022-12-14 18:48:29 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to SST-2-bin/label\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdql5_zk4UKr"
      },
      "outputs": [],
      "source": [
        "!git fetch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -D ibert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6G_99PCd3EF",
        "outputId": "7a399e5c-1e00-41e0-e6ba-79978eea3668"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted branch ibert (was 1b09c75).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change branch\n",
        "!git branch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oaMj386doaF",
        "outputId": "ff0ccc97-fc48-489f-8ffe-3a6bb20065c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mibert\u001b[m\n",
            "  ibert-base\u001b[m\n",
            "  origin/ibert\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CDlDBFD4XNl",
        "outputId": "022f00fc-4a92-4a28-9763-aa92390b6a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M\texamples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh\n",
            "M\texamples/roberta/preprocess_GLUE_tasks.sh\n",
            "M\texamples/roberta/preprocess_RACE.sh\n",
            "M\texamples/speech_recognition/datasets/prepare-librispeech.sh\n",
            "M\tfairseq/checkpoint_utils.py\n",
            "Branch 'ibert' set up to track remote branch 'ibert' from 'origin'.\n",
            "Switched to a new branch 'ibert'\n"
          ]
        }
      ],
      "source": [
        "!git checkout ibert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvxNmORj4mRr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafe1d53-54ee-4116-9b0d-f9fd8d567956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M\texamples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh\n",
            "M\texamples/roberta/preprocess_GLUE_tasks.sh\n",
            "M\texamples/roberta/preprocess_RACE.sh\n",
            "M\texamples/speech_recognition/datasets/prepare-librispeech.sh\n",
            "M\tfairseq/checkpoint_utils.py\n",
            "M\tfairseq/trainer.py\n",
            "M\tfairseq_cli/train.py\n",
            "M\trun.py\n",
            "Already on 'ibert'\n",
            "Your branch is up to date with 'origin/ibert'.\n"
          ]
        }
      ],
      "source": [
        "!git checkout ibert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub31w6M44q5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d86709-11e4-4ddb-f031-71b19269aa66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid_subset: valid\n",
            "valid_interval_updates: None\n",
            "Finetuning from the checkpoint: checkpoint_best_CoLA.pt\n",
            "2022-12-13 17:45:08 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='CoLA-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, force_dequant='none', fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_file='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506.log', log_format=None, log_interval=100, lr=[1e-06], lr_scheduler='polynomial_decay', max_epoch=12, max_positions=512, max_sentences=16, max_sentences_valid=16, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=True, no_shuffle=False, nprocs_per_node=1, num_classes=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_mode='symmetric', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_best_CoLA.pt', save_dir='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=5336, tpu=False, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.1)\n",
            "2022-12-13 17:45:08 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
            "2022-12-13 17:45:08 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2022-12-13 17:45:08 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/input0/valid\n",
            "2022-12-13 17:45:08 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/label/valid\n",
            "2022-12-13 17:45:08 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1043\n",
            "2022-12-13 17:45:08 | INFO | fairseq.modules.transformer_sentence_encoder | Dropout 0.1, attn dropout 0.1, act dropout 0.0\n",
            "2022-12-13 17:45:10 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerSentenceEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): QuantEmbedding()\n",
            "      (embed_positions): QuantEmbedding()\n",
            "      (embed_positions_act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (1): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (2): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (3): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (4): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (5): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (6): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (7): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (8): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (9): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (10): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (11): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (emb_layer_norm): IntLayerNorm(\n",
            "        (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2022-12-13 17:45:10 | INFO | fairseq_cli.train | task: sentence_prediction (SentencePredictionTask)\n",
            "2022-12-13 17:45:10 | INFO | fairseq_cli.train | model: roberta_base (RobertaModel)\n",
            "2022-12-13 17:45:10 | INFO | fairseq_cli.train | criterion: sentence_prediction (SentencePredictionCriterion)\n",
            "2022-12-13 17:45:10 | INFO | fairseq_cli.train | num. model params: 125288795 (num. trained: 125288795)\n",
            "2022-12-13 17:45:10 | INFO | fairseq_cli.train | quantize: symmetric\n",
            "2022-12-13 17:45:12 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2022-12-13 17:45:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-13 17:45:12 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          \n",
            "2022-12-13 17:45:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-13 17:45:12 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-12-13 17:45:12 | INFO | fairseq_cli.train | max tokens per GPU = 4400 and max sentences per GPU = 16\n",
            "2022-12-13 17:45:14 | INFO | fairseq.trainer | loaded checkpoint checkpoint_best_CoLA.pt (epoch 10 @ 0 updates)\n",
            "2022-12-13 17:45:14 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n",
            "2022-12-13 17:45:14 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-12-13 17:45:14 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/input0/train\n",
            "2022-12-13 17:45:14 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/label/train\n",
            "2022-12-13 17:45:14 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 8551\n",
            "epoch 001:   0% 0/535 [00:00<?, ?it/s]2022-12-13 17:45:15 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-12-13 17:45:15 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 1\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-13 17:45:17 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "/content/gdrive/MyDrive/coms6998_project/I-BERT/fairseq/utils.py:305: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:   0% 1/535 [00:02<24:46,  2.78s/it]2022-12-13 17:45:18 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 001:  18% 94/535 [00:56<04:13,  1.74it/s]2022-12-13 17:46:11 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 001: 100% 534/535 [05:10<00:00,  1.75it/s, loss=0.913, nll_loss=0.081, accuracy=68.1, tp=34, tn=1049, fp=46, fn=462, false=508, wps=312.6, ups=1.73, wpb=180.5, bsz=15.9, num_updates=500, lr=9.06297e-07, gnorm=25.442, train_wall=57, wall=293]2022-12-13 17:50:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 2/66 [00:01<00:31,  2.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 3/66 [00:01<00:29,  2.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 4/66 [00:01<00:28,  2.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 6/66 [00:02<00:28,  2.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 8/66 [00:03<00:27,  2.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 15/66 [00:06<00:23,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 18/66 [00:08<00:22,  2.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 20/66 [00:09<00:21,  2.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 21/66 [00:09<00:21,  2.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 26/66 [00:12<00:18,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 29/66 [00:13<00:17,  2.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 31/66 [00:14<00:15,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 36/66 [00:16<00:14,  2.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 39/66 [00:18<00:12,  2.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 44/66 [00:20<00:09,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 50/66 [00:23<00:07,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 55/66 [00:25<00:04,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 61/66 [00:28<00:02,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.22it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 17:50:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.901 | nll_loss 0.077 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 400.6 | wpb 183.7 | bsz 15.8 | num_updates 535\n",
            "2022-12-13 17:50:56 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 17:51:22 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint1.pt (epoch 1 @ 535 updates, score 69.2) (writing took 26.425131331000102 seconds)\n",
            "2022-12-13 17:51:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-12-13 17:51:22 | INFO | train | epoch 001 | loss 0.957 | nll_loss 0.084 | accuracy 66.6 | tp 283 | tn 5411 | fp 612 | fn 2245 | false 2857 | wps 266.2 | ups 1.46 | wpb 181.7 | bsz 16 | num_updates 535 | lr 8.99738e-07 | gnorm 34.533 | train_wall 309 | wall 370\n",
            "epoch 002:   0% 0/535 [00:00<?, ?it/s]2022-12-13 17:51:22 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002:  33% 177/535 [01:45<03:25,  1.74it/s, loss=0.894, nll_loss=0.078, accuracy=68.9, tp=9, tn=1094, fp=15, fn=482, false=497, wps=316.7, ups=1.72, wpb=184.2, bsz=16, num_updates=700, lr=8.68816e-07, gnorm=18.336, train_wall=58, wall=469]2022-12-13 17:53:08 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 002: 100% 534/535 [05:10<00:00,  1.74it/s, loss=0.86, nll_loss=0.076, accuracy=70.5, tp=8, tn=1120, fp=16, fn=456, false=472, wps=314.8, ups=1.74, wpb=181, bsz=16, num_updates=1000, lr=8.12594e-07, gnorm=20.938, train_wall=57, wall=641]2022-12-13 17:56:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 1/66 [00:00<00:37,  1.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 4/66 [00:01<00:28,  2.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 7/66 [00:03<00:26,  2.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 9/66 [00:04<00:25,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 11/66 [00:05<00:24,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 15/66 [00:06<00:23,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 20/66 [00:09<00:20,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 22/66 [00:10<00:19,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 26/66 [00:11<00:18,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 31/66 [00:14<00:15,  2.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 35/66 [00:15<00:14,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 37/66 [00:16<00:13,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 44/66 [00:20<00:09,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 46/66 [00:20<00:09,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 48/66 [00:21<00:08,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 50/66 [00:22<00:07,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 55/66 [00:24<00:04,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 57/66 [00:25<00:04,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 59/66 [00:26<00:03,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 61/66 [00:27<00:02,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 66/66 [00:29<00:00,  2.23it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 17:57:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.886 | nll_loss 0.076 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 405.3 | wpb 183.7 | bsz 15.8 | num_updates 1070 | best_accuracy 69.2\n",
            "2022-12-13 17:57:03 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 17:57:18 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint2.pt (epoch 2 @ 1070 updates, score 69.1) (writing took 14.937254768000457 seconds)\n",
            "2022-12-13 17:57:18 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-12-13 17:57:18 | INFO | train | epoch 002 | loss 0.877 | nll_loss 0.077 | accuracy 70.1 | tp 48 | tn 5943 | fp 80 | fn 2480 | false 2560 | wps 272.8 | ups 1.5 | wpb 181.7 | bsz 16 | num_updates 1070 | lr 7.99475e-07 | gnorm 20.792 | train_wall 310 | wall 726\n",
            "epoch 003:   0% 0/535 [00:00<?, ?it/s]2022-12-13 17:57:18 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 534/535 [05:13<00:00,  1.74it/s, loss=0.884, nll_loss=0.078, accuracy=68.1, tp=14, tn=1075, fp=19, fn=492, false=511, wps=311.6, ups=1.72, wpb=181.5, bsz=16, num_updates=1600, lr=7.0015e-07, gnorm=25.839, train_wall=58, wall=1037]2022-12-13 18:02:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 1/66 [00:00<00:38,  1.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.13it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  23% 15/66 [00:07<00:23,  2.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 16/66 [00:07<00:23,  2.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  30% 20/66 [00:09<00:20,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 22/66 [00:10<00:19,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  39% 26/66 [00:12<00:18,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 31/66 [00:14<00:16,  2.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 44/66 [00:20<00:10,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 50/66 [00:23<00:07,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 55/66 [00:25<00:05,  2.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 61/66 [00:28<00:02,  2.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.21it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:03:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.881 | nll_loss 0.076 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 400.6 | wpb 183.7 | bsz 15.8 | num_updates 1605 | best_accuracy 69.2\n",
            "2022-12-13 18:03:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:03:17 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint3.pt (epoch 3 @ 1605 updates, score 69.1) (writing took 14.979563453000083 seconds)\n",
            "2022-12-13 18:03:17 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-12-13 18:03:17 | INFO | train | epoch 003 | loss 0.866 | nll_loss 0.076 | accuracy 70.2 | tp 67 | tn 5939 | fp 84 | fn 2461 | false 2545 | wps 270.9 | ups 1.49 | wpb 181.7 | bsz 16 | num_updates 1605 | lr 6.99213e-07 | gnorm 22.496 | train_wall 312 | wall 1085\n",
            "epoch 004:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:03:17 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 534/535 [05:13<00:00,  1.72it/s, loss=0.88, nll_loss=0.077, accuracy=69.2, tp=11, tn=1097, fp=11, fn=481, false=492, wps=313.8, ups=1.72, wpb=182.3, bsz=16, num_updates=2100, lr=6.06447e-07, gnorm=20.805, train_wall=58, wall=1376]2022-12-13 18:08:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 1/66 [00:00<00:38,  1.69it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.05it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.18it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.18it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 15/66 [00:06<00:23,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  30% 20/66 [00:09<00:20,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 26/66 [00:11<00:18,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 31/66 [00:14<00:15,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 33/66 [00:15<00:14,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 37/66 [00:16<00:13,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 44/66 [00:20<00:10,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 48/66 [00:21<00:08,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76% 50/66 [00:22<00:07,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 55/66 [00:25<00:04,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 59/66 [00:26<00:03,  2.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 61/66 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.21it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:09:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.901 | nll_loss 0.078 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 402.7 | wpb 183.7 | bsz 15.8 | num_updates 2140 | best_accuracy 69.2\n",
            "2022-12-13 18:09:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:09:17 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint4.pt (epoch 4 @ 2140 updates, score 69.1) (writing took 14.93009487800009 seconds)\n",
            "2022-12-13 18:09:17 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-12-13 18:09:17 | INFO | train | epoch 004 | loss 0.868 | nll_loss 0.076 | accuracy 70.3 | tp 57 | tn 5951 | fp 72 | fn 2471 | false 2543 | wps 270.4 | ups 1.49 | wpb 181.7 | bsz 16 | num_updates 2140 | lr 5.98951e-07 | gnorm 19.475 | train_wall 313 | wall 1445\n",
            "epoch 005:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:09:17 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 534/535 [05:11<00:00,  1.72it/s, loss=0.875, nll_loss=0.076, accuracy=69.5, tp=31, tn=1081, fp=23, fn=465, false=488, wps=317, ups=1.73, wpb=183.7, bsz=16, num_updates=2600, lr=5.12744e-07, gnorm=19.248, train_wall=58, wall=1713]2022-12-13 18:14:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   2% 1/66 [00:00<00:38,  1.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  18% 12/66 [00:05<00:25,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 15/66 [00:07<00:23,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  30% 20/66 [00:09<00:21,  2.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39% 26/66 [00:12<00:18,  2.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  47% 31/66 [00:14<00:16,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 39/66 [00:18<00:12,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  61% 40/66 [00:18<00:12,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 42/66 [00:19<00:11,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 44/66 [00:20<00:10,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.13it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76% 50/66 [00:23<00:07,  2.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 51/66 [00:23<00:07,  2.13it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  79% 52/66 [00:24<00:06,  2.14it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80% 53/66 [00:24<00:06,  2.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 54/66 [00:25<00:05,  2.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 55/66 [00:25<00:05,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 61/66 [00:28<00:02,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  95% 63/66 [00:29<00:01,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  98% 65/66 [00:30<00:00,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:14:59 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.9 | nll_loss 0.077 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 398.1 | wpb 183.7 | bsz 15.8 | num_updates 2675 | best_accuracy 69.2\n",
            "2022-12-13 18:14:59 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:15:15 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint5.pt (epoch 5 @ 2675 updates, score 69.1) (writing took 15.481332753999595 seconds)\n",
            "2022-12-13 18:15:15 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-12-13 18:15:15 | INFO | train | epoch 005 | loss 0.852 | nll_loss 0.075 | accuracy 70.7 | tp 95 | tn 5950 | fp 73 | fn 2433 | false 2506 | wps 271.8 | ups 1.5 | wpb 181.7 | bsz 16 | num_updates 2675 | lr 4.98688e-07 | gnorm 20.118 | train_wall 310 | wall 1802\n",
            "epoch 006:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:15:15 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 534/535 [05:12<00:00,  1.74it/s, loss=0.846, nll_loss=0.073, accuracy=70.3, tp=17, tn=1108, fp=28, fn=447, false=475, wps=318.8, ups=1.72, wpb=185, bsz=16, num_updates=3200, lr=4.003e-07, gnorm=20.166, train_wall=58, wall=2109]2022-12-13 18:20:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   2% 1/66 [00:00<00:38,  1.68it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.04it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.10it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.12it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.15it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.17it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  23% 15/66 [00:06<00:23,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  30% 20/66 [00:09<00:20,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 26/66 [00:12<00:18,  2.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47% 31/66 [00:14<00:15,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.18it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 44/66 [00:20<00:09,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  76% 50/66 [00:22<00:07,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 55/66 [00:25<00:04,  2.21it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.21it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.21it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 61/66 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:20:58 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.892 | nll_loss 0.077 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 401.5 | wpb 183.7 | bsz 15.8 | num_updates 3210 | best_accuracy 69.2\n",
            "2022-12-13 18:20:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:21:14 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint6.pt (epoch 6 @ 3210 updates, score 69.1) (writing took 16.42760206299954 seconds)\n",
            "2022-12-13 18:21:14 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-12-13 18:21:14 | INFO | train | epoch 006 | loss 0.856 | nll_loss 0.075 | accuracy 70.2 | tp 85 | tn 5916 | fp 107 | fn 2443 | false 2550 | wps 270.5 | ups 1.49 | wpb 181.7 | bsz 16 | num_updates 3210 | lr 3.98426e-07 | gnorm 20.307 | train_wall 311 | wall 2162\n",
            "epoch 007:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:21:14 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 534/535 [05:12<00:00,  1.73it/s, loss=0.841, nll_loss=0.073, accuracy=70.6, tp=27, tn=1103, fp=17, fn=453, false=470, wps=316.5, ups=1.72, wpb=184.4, bsz=16, num_updates=3700, lr=3.06597e-07, gnorm=22.491, train_wall=58, wall=2449]2022-12-13 18:26:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   2% 1/66 [00:00<00:37,  1.73it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.98it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.08it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.12it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.15it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.16it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.17it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  23% 15/66 [00:06<00:23,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  30% 20/66 [00:09<00:20,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39% 26/66 [00:11<00:18,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.17it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  47% 31/66 [00:14<00:16,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.16it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 44/66 [00:20<00:10,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.17it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  76% 50/66 [00:22<00:07,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 55/66 [00:25<00:05,  2.20it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.20it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 61/66 [00:28<00:02,  2.17it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.17it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.18it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:26:57 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.888 | nll_loss 0.076 | accuracy 69.3 | tp 2 | tn 721 | fp 0 | fn 320 | false 320 | wps 400.2 | wpb 183.7 | bsz 15.8 | num_updates 3745 | best_accuracy 69.3\n",
            "2022-12-13 18:26:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:27:27 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint7.pt (epoch 7 @ 3745 updates, score 69.3) (writing took 29.666484549000415 seconds)\n",
            "2022-12-13 18:27:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-12-13 18:27:27 | INFO | train | epoch 007 | loss 0.844 | nll_loss 0.074 | accuracy 70.2 | tp 128 | tn 5871 | fp 152 | fn 2400 | false 2552 | wps 260.6 | ups 1.43 | wpb 181.7 | bsz 16 | num_updates 3745 | lr 2.98163e-07 | gnorm 23.154 | train_wall 311 | wall 2535\n",
            "epoch 008:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:27:27 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008:  53% 283/535 [02:47<02:25,  1.73it/s, loss=0.839, nll_loss=0.073, accuracy=70.6, tp=14, tn=1115, fp=25, fn=446, false=471, wps=317.5, ups=1.73, wpb=183.8, bsz=16, num_updates=4000, lr=2.50375e-07, gnorm=21.818, train_wall=58, wall=2686]2022-12-13 18:30:15 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 7 -> 8\n",
            "epoch 008: 100% 534/535 [05:13<00:00,  1.73it/s, loss=0.855, nll_loss=0.075, accuracy=70.1, tp=38, tn=1084, fp=31, fn=447, false=478, wps=312, ups=1.71, wpb=181.9, bsz=16, num_updates=4200, lr=2.12894e-07, gnorm=23.437, train_wall=58, wall=2802]2022-12-13 18:32:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   2% 1/66 [00:00<00:39,  1.66it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   3% 2/66 [00:01<00:33,  1.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.04it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.09it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.12it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.14it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.16it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.16it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.17it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.16it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.17it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  23% 15/66 [00:07<00:23,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  30% 20/66 [00:09<00:21,  2.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.21it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  39% 26/66 [00:12<00:18,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  47% 31/66 [00:14<00:15,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 44/66 [00:20<00:10,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 50/66 [00:22<00:07,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.17it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 55/66 [00:25<00:05,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.17it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.18it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 61/66 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:33:11 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.879 | nll_loss 0.076 | accuracy 69.4 | tp 3 | tn 721 | fp 0 | fn 319 | false 319 | wps 401 | wpb 183.7 | bsz 15.8 | num_updates 4280 | best_accuracy 69.4\n",
            "2022-12-13 18:33:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:33:34 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint8.pt (epoch 8 @ 4280 updates, score 69.4) (writing took 22.910270153000056 seconds)\n",
            "2022-12-13 18:33:34 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-12-13 18:33:34 | INFO | train | epoch 008 | loss 0.842 | nll_loss 0.074 | accuracy 70.4 | tp 137 | tn 5881 | fp 142 | fn 2391 | false 2533 | wps 265 | ups 1.46 | wpb 181.7 | bsz 16 | num_updates 4280 | lr 1.97901e-07 | gnorm 22.395 | train_wall 312 | wall 2902\n",
            "epoch 009:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:33:34 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 534/535 [05:13<00:00,  1.73it/s, loss=0.839, nll_loss=0.074, accuracy=69.8, tp=32, tn=1084, fp=35, fn=449, false=484, wps=309.7, ups=1.71, wpb=180.7, bsz=16, num_updates=4800, lr=1.0045e-07, gnorm=23.933, train_wall=58, wall=3207]2022-12-13 18:38:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   2% 1/66 [00:00<00:38,  1.70it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.96it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.06it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.12it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.14it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.16it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.17it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  23% 15/66 [00:06<00:23,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  30% 20/66 [00:09<00:21,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39% 26/66 [00:11<00:18,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  47% 31/66 [00:14<00:15,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.21it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 37/66 [00:16<00:13,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 44/66 [00:20<00:10,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.16it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.17it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  76% 50/66 [00:22<00:07,  2.17it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 55/66 [00:25<00:05,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.17it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.17it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.17it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.17it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 61/66 [00:28<00:02,  2.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.20it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:39:18 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.879 | nll_loss 0.076 | accuracy 69.3 | tp 2 | tn 721 | fp 0 | fn 320 | false 320 | wps 400.6 | wpb 183.7 | bsz 15.8 | num_updates 4815 | best_accuracy 69.4\n",
            "2022-12-13 18:39:18 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:39:33 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint9.pt (epoch 9 @ 4815 updates, score 69.3) (writing took 15.278635249000217 seconds)\n",
            "2022-12-13 18:39:33 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-12-13 18:39:33 | INFO | train | epoch 009 | loss 0.836 | nll_loss 0.074 | accuracy 70.4 | tp 170 | tn 5850 | fp 173 | fn 2358 | false 2531 | wps 270.5 | ups 1.49 | wpb 181.7 | bsz 16 | num_updates 4815 | lr 9.76387e-08 | gnorm 24.101 | train_wall 312 | wall 3261\n",
            "epoch 010:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:39:33 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 534/535 [05:12<00:00,  1.73it/s, loss=0.817, nll_loss=0.072, accuracy=72.5, tp=37, tn=1123, fp=26, fn=414, false=440, wps=310.4, ups=1.71, wpb=181.2, bsz=16, num_updates=5300, lr=6.74663e-09, gnorm=23.517, train_wall=58, wall=3545]2022-12-13 18:44:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   2% 1/66 [00:00<00:38,  1.70it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.06it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.11it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.13it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.15it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.14it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.16it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.17it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.17it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  23% 15/66 [00:07<00:23,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  30% 20/66 [00:09<00:21,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39% 26/66 [00:12<00:18,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  47% 31/66 [00:14<00:16,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.17it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 44/66 [00:20<00:10,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76% 50/66 [00:23<00:07,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 55/66 [00:25<00:05,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 61/66 [00:28<00:02,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:45:17 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.892 | nll_loss 0.077 | accuracy 69.4 | tp 3 | tn 721 | fp 0 | fn 319 | false 319 | wps 400 | wpb 183.7 | bsz 15.8 | num_updates 5350 | best_accuracy 69.4\n",
            "2022-12-13 18:45:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:45:41 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint10.pt (epoch 10 @ 5350 updates, score 69.4) (writing took 24.183021498000016 seconds)\n",
            "2022-12-13 18:45:41 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-12-13 18:45:41 | INFO | train | epoch 010 | loss 0.833 | nll_loss 0.073 | accuracy 70.8 | tp 182 | tn 5873 | fp 150 | fn 2346 | false 2496 | wps 264.5 | ups 1.46 | wpb 181.7 | bsz 16 | num_updates 5350 | lr 0 | gnorm 23.504 | train_wall 311 | wall 3629\n",
            "epoch 011:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:45:41 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 534/535 [05:12<00:00,  1.73it/s, loss=0.838, nll_loss=0.073, accuracy=70.7, tp=36, tn=1095, fp=32, fn=437, false=469, wps=313.3, ups=1.71, wpb=183.4, bsz=16, num_updates=5800, lr=0, gnorm=20.581, train_wall=58, wall=3892]2022-12-13 18:50:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   2% 1/66 [00:00<00:37,  1.71it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.05it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.10it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.13it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 6/66 [00:02<00:27,  2.15it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.16it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.17it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  23% 15/66 [00:06<00:23,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  30% 20/66 [00:09<00:21,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  39% 26/66 [00:11<00:18,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  44% 29/66 [00:13<00:16,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  47% 31/66 [00:14<00:16,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 42/66 [00:19<00:11,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 44/66 [00:20<00:10,  2.17it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  76% 50/66 [00:22<00:07,  2.17it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.17it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  83% 55/66 [00:25<00:05,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  92% 61/66 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.21it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.21it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.20it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:51:24 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.884 | nll_loss 0.076 | accuracy 69.3 | tp 2 | tn 721 | fp 0 | fn 320 | false 320 | wps 401 | wpb 183.7 | bsz 15.8 | num_updates 5885 | best_accuracy 69.4\n",
            "2022-12-13 18:51:24 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:51:40 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint11.pt (epoch 11 @ 5885 updates, score 69.3) (writing took 15.959670001000632 seconds)\n",
            "2022-12-13 18:51:40 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-12-13 18:51:40 | INFO | train | epoch 011 | loss 0.835 | nll_loss 0.073 | accuracy 70.7 | tp 201 | tn 5846 | fp 177 | fn 2327 | false 2504 | wps 270.9 | ups 1.49 | wpb 181.7 | bsz 16 | num_updates 5885 | lr 0 | gnorm 22.697 | train_wall 311 | wall 3988\n",
            "epoch 012:   0% 0/535 [00:00<?, ?it/s]2022-12-13 18:51:40 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 534/535 [05:13<00:00,  1.72it/s, loss=0.843, nll_loss=0.074, accuracy=70.6, tp=34, tn=1096, fp=34, fn=436, false=470, wps=310.3, ups=1.7, wpb=182.7, bsz=16, num_updates=6400, lr=0, gnorm=26.158, train_wall=59, wall=4291]2022-12-13 18:56:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   2% 1/66 [00:00<00:38,  1.70it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   3% 2/66 [00:01<00:32,  1.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   5% 3/66 [00:01<00:30,  2.05it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   6% 4/66 [00:01<00:29,  2.08it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   8% 5/66 [00:02<00:28,  2.12it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 6/66 [00:02<00:28,  2.14it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  11% 7/66 [00:03<00:27,  2.15it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  12% 8/66 [00:03<00:26,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  14% 9/66 [00:04<00:26,  2.16it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  15% 10/66 [00:04<00:25,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  17% 11/66 [00:05<00:25,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  18% 12/66 [00:05<00:24,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  20% 13/66 [00:06<00:24,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  21% 14/66 [00:06<00:23,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  23% 15/66 [00:07<00:23,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  26% 17/66 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  27% 18/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  29% 19/66 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  30% 20/66 [00:09<00:20,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  32% 21/66 [00:09<00:20,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 22/66 [00:10<00:20,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  36% 24/66 [00:11<00:19,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39% 26/66 [00:12<00:18,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  41% 27/66 [00:12<00:17,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  42% 28/66 [00:12<00:17,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  44% 29/66 [00:13<00:17,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45% 30/66 [00:13<00:16,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  47% 31/66 [00:14<00:16,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  48% 32/66 [00:14<00:15,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 33/66 [00:15<00:15,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  52% 34/66 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  53% 35/66 [00:16<00:14,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  55% 36/66 [00:16<00:13,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  56% 37/66 [00:17<00:13,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  58% 38/66 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  61% 40/66 [00:18<00:11,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  64% 42/66 [00:19<00:10,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  65% 43/66 [00:19<00:10,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 44/66 [00:20<00:09,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  68% 45/66 [00:20<00:09,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  70% 46/66 [00:21<00:09,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  71% 47/66 [00:21<00:08,  2.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  73% 48/66 [00:22<00:08,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  74% 49/66 [00:22<00:07,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  76% 50/66 [00:23<00:07,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  77% 51/66 [00:23<00:06,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  79% 52/66 [00:23<00:06,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  80% 53/66 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 54/66 [00:24<00:05,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  83% 55/66 [00:25<00:05,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  85% 56/66 [00:25<00:04,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  86% 57/66 [00:26<00:04,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  88% 58/66 [00:26<00:03,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 59/66 [00:27<00:03,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  91% 60/66 [00:27<00:02,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  92% 61/66 [00:28<00:02,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  94% 62/66 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  95% 63/66 [00:28<00:01,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  97% 64/66 [00:29<00:00,  2.18it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  98% 65/66 [00:29<00:00,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 66/66 [00:30<00:00,  2.16it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-13 18:57:25 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.893 | nll_loss 0.077 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 399.8 | wpb 183.7 | bsz 15.8 | num_updates 6420 | best_accuracy 69.4\n",
            "2022-12-13 18:57:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-13 18:57:44 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1213-174506_ckpt/checkpoint12.pt (epoch 12 @ 6420 updates, score 69.2) (writing took 19.126472559999456 seconds)\n",
            "2022-12-13 18:57:44 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-12-13 18:57:44 | INFO | train | epoch 012 | loss 0.835 | nll_loss 0.073 | accuracy 70.6 | tp 198 | tn 5837 | fp 186 | fn 2330 | false 2516 | wps 267.1 | ups 1.47 | wpb 181.7 | bsz 16 | num_updates 6420 | lr 0 | gnorm 23.74 | train_wall 313 | wall 4352\n",
            "2022-12-13 18:57:44 | INFO | fairseq_cli.train | done training in 4349.6 seconds\n"
          ]
        }
      ],
      "source": [
        "# fine tuning roberta base for tasks\n",
        "!CUDA_VISIBLE_DEVICES=0 python run.py --arch roberta_base --task CoLA --restore-file checkpoint_best_CoLA.pt --lr 1e-6 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run.py --arch roberta_base --task CoLA --restore-file checkpoint_best_CoLA.pt --lr 1e-6 --force-dequant gelu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9wPaxAi_NFG",
        "outputId": "32d3e1db-1b11-4ff3-cfa3-18870d395d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid_subset: valid\n",
            "valid_interval_updates: None\n",
            "Finetuning from the checkpoint: checkpoint_best_CoLA.pt\n",
            "2022-12-14 23:37:40 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='CoLA-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, force_dequant='gelu', fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_file='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553.log', log_format=None, log_interval=100, lr=[1e-06], lr_scheduler='polynomial_decay', max_epoch=12, max_positions=512, max_sentences=16, max_sentences_valid=16, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=True, no_shuffle=False, nprocs_per_node=1, num_classes=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_mode='symmetric', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_best_CoLA.pt', save_dir='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=5336, tpu=False, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.1)\n",
            "2022-12-14 23:37:41 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
            "2022-12-14 23:37:41 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2022-12-14 23:37:42 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/input0/valid\n",
            "2022-12-14 23:37:42 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/label/valid\n",
            "2022-12-14 23:37:42 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1043\n",
            "2022-12-14 23:37:42 | INFO | fairseq.modules.transformer_sentence_encoder | Dropout 0.1, attn dropout 0.1, act dropout 0.0\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:43 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-14 23:37:44 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerSentenceEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): QuantEmbedding()\n",
            "      (embed_positions): QuantEmbedding()\n",
            "      (embed_positions_act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (1): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (2): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (3): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (4): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (5): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (6): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (7): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (8): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (9): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (10): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (11): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (emb_layer_norm): IntLayerNorm(\n",
            "        (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2022-12-14 23:37:44 | INFO | fairseq_cli.train | task: sentence_prediction (SentencePredictionTask)\n",
            "2022-12-14 23:37:44 | INFO | fairseq_cli.train | model: roberta_base (RobertaModel)\n",
            "2022-12-14 23:37:44 | INFO | fairseq_cli.train | criterion: sentence_prediction (SentencePredictionCriterion)\n",
            "2022-12-14 23:37:44 | INFO | fairseq_cli.train | num. model params: 125288795 (num. trained: 125288795)\n",
            "2022-12-14 23:37:44 | INFO | fairseq_cli.train | quantize: symmetric\n",
            "2022-12-14 23:37:48 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2022-12-14 23:37:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-14 23:37:48 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          \n",
            "2022-12-14 23:37:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-14 23:37:48 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-12-14 23:37:48 | INFO | fairseq_cli.train | max tokens per GPU = 4400 and max sentences per GPU = 16\n",
            "2022-12-14 23:37:58 | INFO | fairseq.trainer | loaded checkpoint checkpoint_best_CoLA.pt (epoch 10 @ 0 updates)\n",
            "2022-12-14 23:37:58 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n",
            "2022-12-14 23:37:58 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-12-14 23:37:59 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/input0/train\n",
            "2022-12-14 23:37:59 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/label/train\n",
            "2022-12-14 23:37:59 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 8551\n",
            "epoch 001:   0% 0/535 [00:00<?, ?it/s]2022-12-14 23:37:59 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-12-14 23:37:59 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 1\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-14 23:38:03 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT/fairseq/utils.py:305: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  25% 136/535 [01:05<03:03,  2.17it/s, loss=1.067, nll_loss=0.094, accuracy=61.4, tp=101, tn=882, fp=239, fn=378, false=617, wps=395.7, ups=2.19, wpb=181.1, bsz=16, num_updates=100, lr=9.81259e-07, gnorm=51.736, train_wall=49, wall=60]2022-12-14 23:39:05 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 001: 100% 534/535 [04:07<00:00,  2.22it/s, loss=0.908, nll_loss=0.08, accuracy=67.3, tp=26, tn=1045, fp=50, fn=470, false=520, wps=396, ups=2.19, wpb=180.5, bsz=15.9, num_updates=500, lr=9.06297e-07, gnorm=23.628, train_wall=45, wall=243]2022-12-14 23:42:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/66 [00:00<00:28,  2.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 3/66 [00:01<00:22,  2.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 4/66 [00:01<00:21,  2.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 5/66 [00:01<00:20,  2.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 7/66 [00:02<00:19,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 8/66 [00:02<00:19,  2.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 10/66 [00:03<00:18,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 13/66 [00:04<00:18,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 16/66 [00:05<00:16,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 19/66 [00:06<00:15,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 22/66 [00:07<00:15,  2.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 25/66 [00:08<00:14,  2.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 29/66 [00:09<00:12,  2.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 31/66 [00:10<00:11,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 32/66 [00:10<00:11,  2.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 35/66 [00:11<00:10,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-14 23:42:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.904 | nll_loss 0.078 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 540.5 | wpb 183.7 | bsz 15.8 | num_updates 535\n",
            "2022-12-14 23:42:29 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-14 23:42:53 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint1.pt (epoch 1 @ 535 updates, score 69.2) (writing took 23.705079709999836 seconds)\n",
            "2022-12-14 23:42:53 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-12-14 23:42:53 | INFO | train | epoch 001 | loss 0.934 | nll_loss 0.082 | accuracy 67.5 | tp 261 | tn 5507 | fp 516 | fn 2267 | false 2783 | wps 335 | ups 1.84 | wpb 181.7 | bsz 16 | num_updates 535 | lr 8.99738e-07 | gnorm 33.505 | train_wall 246 | wall 305\n",
            "epoch 002:   0% 0/535 [00:00<?, ?it/s]2022-12-14 23:42:53 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002:  33% 177/535 [01:25<02:42,  2.20it/s, loss=0.885, nll_loss=0.077, accuracy=68.8, tp=22, tn=1079, fp=30, fn=469, false=499, wps=388.4, ups=2.11, wpb=184.2, bsz=16, num_updates=700, lr=8.68816e-07, gnorm=21.513, train_wall=47, wall=384]2022-12-14 23:44:18 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 002: 100% 534/535 [04:08<00:00,  2.19it/s, loss=0.862, nll_loss=0.076, accuracy=70.8, tp=13, tn=1119, fp=17, fn=451, false=468, wps=395.5, ups=2.19, wpb=181, bsz=16, num_updates=1000, lr=8.12594e-07, gnorm=40.858, train_wall=45, wall=522]2022-12-14 23:47:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 1/66 [00:00<00:29,  2.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 3/66 [00:01<00:23,  2.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 4/66 [00:01<00:22,  2.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 8/66 [00:02<00:19,  2.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 13/66 [00:04<00:17,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 16/66 [00:05<00:16,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 19/66 [00:06<00:15,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 25/66 [00:08<00:13,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 29/66 [00:10<00:12,  2.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 31/66 [00:10<00:11,  2.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 32/66 [00:11<00:11,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 35/66 [00:12<00:10,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 40/66 [00:13<00:09,  2.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-14 23:47:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.88 | nll_loss 0.076 | accuracy 69.2 | tp 2 | tn 720 | fp 1 | fn 320 | false 321 | wps 536.4 | wpb 183.7 | bsz 15.8 | num_updates 1070 | best_accuracy 69.2\n",
            "2022-12-14 23:47:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-14 23:47:47 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint2.pt (epoch 2 @ 1070 updates, score 69.2) (writing took 22.12352147999991 seconds)\n",
            "2022-12-14 23:47:47 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-12-14 23:47:47 | INFO | train | epoch 002 | loss 0.877 | nll_loss 0.077 | accuracy 69.9 | tp 80 | tn 5898 | fp 125 | fn 2448 | false 2573 | wps 330.9 | ups 1.82 | wpb 181.7 | bsz 16 | num_updates 1070 | lr 7.99475e-07 | gnorm 25.283 | train_wall 247 | wall 599\n",
            "epoch 003:   0% 0/535 [00:00<?, ?it/s]2022-12-14 23:47:47 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 534/535 [04:07<00:00,  2.21it/s, loss=0.883, nll_loss=0.078, accuracy=67.9, tp=24, tn=1063, fp=31, fn=482, false=513, wps=397.7, ups=2.19, wpb=181.5, bsz=16, num_updates=1600, lr=7.0015e-07, gnorm=24.92, train_wall=45, wall=845]2022-12-14 23:51:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 1/66 [00:00<00:28,  2.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 3/66 [00:01<00:22,  2.76it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   6% 4/66 [00:01<00:21,  2.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 8/66 [00:02<00:19,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 10/66 [00:03<00:18,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 13/66 [00:04<00:17,  2.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 16/66 [00:05<00:17,  2.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 19/66 [00:06<00:16,  2.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 22/66 [00:07<00:15,  2.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 25/66 [00:08<00:13,  2.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 29/66 [00:09<00:12,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 31/66 [00:10<00:11,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  48% 32/66 [00:10<00:11,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 35/66 [00:11<00:10,  2.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 43/66 [00:14<00:08,  2.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-14 23:52:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.878 | nll_loss 0.076 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 538.8 | wpb 183.7 | bsz 15.8 | num_updates 1605 | best_accuracy 69.2\n",
            "2022-12-14 23:52:18 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-14 23:52:33 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint3.pt (epoch 3 @ 1605 updates, score 69.1) (writing took 15.137883735000287 seconds)\n",
            "2022-12-14 23:52:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-12-14 23:52:33 | INFO | train | epoch 003 | loss 0.861 | nll_loss 0.076 | accuracy 70.1 | tp 75 | tn 5918 | fp 105 | fn 2453 | false 2558 | wps 339.9 | ups 1.87 | wpb 181.7 | bsz 16 | num_updates 1605 | lr 6.99213e-07 | gnorm 24.263 | train_wall 247 | wall 885\n",
            "epoch 004:   0% 0/535 [00:00<?, ?it/s]2022-12-14 23:52:33 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004:   2% 13/535 [00:06<04:30,  1.93it/s]2022-12-14 23:52:40 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 7 -> 8\n",
            "epoch 004: 100% 534/535 [04:06<00:00,  2.21it/s, loss=0.867, nll_loss=0.076, accuracy=69.4, tp=20, tn=1090, fp=18, fn=472, false=490, wps=398, ups=2.18, wpb=182.3, bsz=16, num_updates=2100, lr=6.06447e-07, gnorm=18.815, train_wall=45, wall=1113]2022-12-14 23:56:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 1/66 [00:00<00:28,  2.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 3/66 [00:01<00:22,  2.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   6% 4/66 [00:01<00:22,  2.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 8/66 [00:02<00:19,  2.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 10/66 [00:03<00:18,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 13/66 [00:04<00:17,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 16/66 [00:05<00:16,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  29% 19/66 [00:06<00:15,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 25/66 [00:08<00:13,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 29/66 [00:09<00:12,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 31/66 [00:10<00:11,  2.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 32/66 [00:10<00:11,  2.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 35/66 [00:11<00:10,  2.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 38/66 [00:12<00:09,  2.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 41/66 [00:13<00:08,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.90it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-14 23:57:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.884 | nll_loss 0.076 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 539 | wpb 183.7 | bsz 15.8 | num_updates 2140 | best_accuracy 69.2\n",
            "2022-12-14 23:57:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-14 23:57:23 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint4.pt (epoch 4 @ 2140 updates, score 69.2) (writing took 20.508523975000116 seconds)\n",
            "2022-12-14 23:57:23 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-12-14 23:57:23 | INFO | train | epoch 004 | loss 0.861 | nll_loss 0.076 | accuracy 70.2 | tp 82 | tn 5923 | fp 100 | fn 2446 | false 2546 | wps 335.3 | ups 1.85 | wpb 181.7 | bsz 16 | num_updates 2140 | lr 5.98951e-07 | gnorm 19.651 | train_wall 245 | wall 1175\n",
            "epoch 005:   0% 0/535 [00:00<?, ?it/s]2022-12-14 23:57:23 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 534/535 [04:07<00:00,  2.19it/s, loss=0.854, nll_loss=0.074, accuracy=68.9, tp=36, tn=1067, fp=37, fn=460, false=497, wps=400.1, ups=2.18, wpb=183.7, bsz=16, num_updates=2600, lr=5.12744e-07, gnorm=20.096, train_wall=46, wall=1388]2022-12-15 00:01:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   2% 1/66 [00:00<00:28,  2.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 3/66 [00:01<00:22,  2.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 4/66 [00:01<00:21,  2.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 8/66 [00:02<00:19,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 13/66 [00:04<00:18,  2.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 16/66 [00:05<00:17,  2.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 19/66 [00:06<00:16,  2.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 22/66 [00:07<00:15,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 25/66 [00:08<00:14,  2.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39% 26/66 [00:09<00:13,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 28/66 [00:09<00:13,  2.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 29/66 [00:10<00:12,  2.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  47% 31/66 [00:10<00:12,  2.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  48% 32/66 [00:11<00:11,  2.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  53% 35/66 [00:12<00:10,  2.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.93it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:01:53 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.892 | nll_loss 0.077 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 534.9 | wpb 183.7 | bsz 15.8 | num_updates 2675 | best_accuracy 69.2\n",
            "2022-12-15 00:01:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:02:06 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint5.pt (epoch 5 @ 2675 updates, score 69.1) (writing took 12.576796970000032 seconds)\n",
            "2022-12-15 00:02:06 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-12-15 00:02:06 | INFO | train | epoch 005 | loss 0.846 | nll_loss 0.074 | accuracy 70.4 | tp 120 | tn 5900 | fp 123 | fn 2408 | false 2531 | wps 343.3 | ups 1.89 | wpb 181.7 | bsz 16 | num_updates 2675 | lr 4.98688e-07 | gnorm 21.37 | train_wall 246 | wall 1458\n",
            "epoch 006:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:02:06 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 534/535 [04:07<00:00,  2.19it/s, loss=0.829, nll_loss=0.072, accuracy=70.3, tp=26, tn=1099, fp=37, fn=438, false=475, wps=403.1, ups=2.18, wpb=185, bsz=16, num_updates=3200, lr=4.003e-07, gnorm=28.082, train_wall=46, wall=1702]2022-12-15 00:06:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   2% 1/66 [00:00<00:29,  2.24it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   3% 2/66 [00:00<00:25,  2.51it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   5% 3/66 [00:01<00:23,  2.68it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 4/66 [00:01<00:22,  2.77it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.82it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.87it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.88it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 8/66 [00:02<00:20,  2.88it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.87it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 13/66 [00:04<00:18,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 16/66 [00:05<00:17,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 19/66 [00:06<00:15,  2.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 25/66 [00:08<00:14,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 26/66 [00:09<00:13,  2.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 28/66 [00:09<00:13,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 29/66 [00:10<00:12,  2.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47% 31/66 [00:10<00:11,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 32/66 [00:11<00:11,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53% 35/66 [00:12<00:10,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 37/66 [00:12<00:10,  2.83it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  58% 38/66 [00:13<00:10,  2.67it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  59% 39/66 [00:13<00:10,  2.66it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 40/66 [00:13<00:09,  2.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.79it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.83it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 43/66 [00:14<00:08,  2.87it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 49/66 [00:17<00:05,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 52/66 [00:18<00:04,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 55/66 [00:19<00:03,  2.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 58/66 [00:20<00:02,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 61/66 [00:21<00:01,  2.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  97% 64/66 [00:22<00:00,  2.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:06:37 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.893 | nll_loss 0.077 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 532.4 | wpb 183.7 | bsz 15.8 | num_updates 3210 | best_accuracy 69.2\n",
            "2022-12-15 00:06:37 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:06:58 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint6.pt (epoch 6 @ 3210 updates, score 69.2) (writing took 20.474126699999943 seconds)\n",
            "2022-12-15 00:06:58 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-12-15 00:06:58 | INFO | train | epoch 006 | loss 0.841 | nll_loss 0.074 | accuracy 70.3 | tp 132 | tn 5881 | fp 142 | fn 2396 | false 2538 | wps 333.5 | ups 1.83 | wpb 181.7 | bsz 16 | num_updates 3210 | lr 3.98426e-07 | gnorm 22.22 | train_wall 247 | wall 1750\n",
            "epoch 007:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:06:58 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 534/535 [04:07<00:00,  2.20it/s, loss=0.843, nll_loss=0.073, accuracy=70.4, tp=42, tn=1085, fp=35, fn=438, false=473, wps=405.2, ups=2.2, wpb=184.4, bsz=16, num_updates=3700, lr=3.06597e-07, gnorm=22.615, train_wall=45, wall=1977]2022-12-15 00:11:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   2% 1/66 [00:00<00:28,  2.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.66it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   5% 3/66 [00:01<00:22,  2.79it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   6% 4/66 [00:01<00:21,  2.85it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.86it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.89it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.91it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 8/66 [00:02<00:20,  2.90it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  20% 13/66 [00:04<00:17,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24% 16/66 [00:05<00:16,  2.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  29% 19/66 [00:06<00:15,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 25/66 [00:08<00:13,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  42% 28/66 [00:09<00:13,  2.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  44% 29/66 [00:09<00:12,  2.91it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  47% 31/66 [00:10<00:11,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  48% 32/66 [00:10<00:11,  2.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  53% 35/66 [00:11<00:10,  2.91it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  58% 38/66 [00:12<00:09,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.98it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.99it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.98it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.99it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:11:28 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.914 | nll_loss 0.079 | accuracy 69.3 | tp 2 | tn 721 | fp 0 | fn 320 | false 320 | wps 540.3 | wpb 183.7 | bsz 15.8 | num_updates 3745 | best_accuracy 69.3\n",
            "2022-12-15 00:11:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:11:51 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint7.pt (epoch 7 @ 3745 updates, score 69.3) (writing took 22.896032062999893 seconds)\n",
            "2022-12-15 00:11:51 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-12-15 00:11:51 | INFO | train | epoch 007 | loss 0.833 | nll_loss 0.073 | accuracy 70.4 | tp 207 | tn 5817 | fp 206 | fn 2321 | false 2527 | wps 332 | ups 1.83 | wpb 181.7 | bsz 16 | num_updates 3745 | lr 2.98163e-07 | gnorm 23.766 | train_wall 246 | wall 2042\n",
            "epoch 008:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:11:51 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 534/535 [04:08<00:00,  2.15it/s, loss=0.85, nll_loss=0.075, accuracy=69.4, tp=57, tn=1053, fp=62, fn=428, false=490, wps=395.2, ups=2.17, wpb=181.9, bsz=16, num_updates=4200, lr=2.12894e-07, gnorm=27.599, train_wall=46, wall=2254]2022-12-15 00:15:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   2% 1/66 [00:00<00:28,  2.27it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.64it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   5% 3/66 [00:01<00:22,  2.75it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   6% 4/66 [00:01<00:21,  2.83it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.87it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.87it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.86it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 8/66 [00:02<00:20,  2.88it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.90it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  20% 13/66 [00:04<00:18,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24% 16/66 [00:05<00:17,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  29% 19/66 [00:06<00:16,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 25/66 [00:08<00:13,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  44% 29/66 [00:09<00:12,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  47% 31/66 [00:10<00:11,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  48% 32/66 [00:10<00:11,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  53% 35/66 [00:12<00:10,  2.90it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:16:22 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.911 | nll_loss 0.078 | accuracy 69.3 | tp 2 | tn 721 | fp 0 | fn 320 | false 320 | wps 539 | wpb 183.7 | bsz 15.8 | num_updates 4280 | best_accuracy 69.3\n",
            "2022-12-15 00:16:22 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:16:47 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint8.pt (epoch 8 @ 4280 updates, score 69.3) (writing took 25.262436854999578 seconds)\n",
            "2022-12-15 00:16:47 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-12-15 00:16:47 | INFO | train | epoch 008 | loss 0.831 | nll_loss 0.073 | accuracy 70.6 | tp 230 | tn 5803 | fp 220 | fn 2298 | false 2518 | wps 328.2 | ups 1.81 | wpb 181.7 | bsz 16 | num_updates 4280 | lr 1.97901e-07 | gnorm 25.191 | train_wall 247 | wall 2339\n",
            "epoch 009:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:16:47 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 534/535 [04:08<00:00,  2.15it/s, loss=0.83, nll_loss=0.073, accuracy=70.1, tp=51, tn=1071, fp=48, fn=430, false=478, wps=390.3, ups=2.16, wpb=180.7, bsz=16, num_updates=4800, lr=1.0045e-07, gnorm=25.783, train_wall=46, wall=2581]2022-12-15 00:20:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   2% 1/66 [00:00<00:28,  2.26it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   5% 3/66 [00:01<00:22,  2.74it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   6% 4/66 [00:01<00:22,  2.82it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.85it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.87it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 8/66 [00:02<00:20,  2.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 11/66 [00:03<00:19,  2.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  20% 13/66 [00:04<00:18,  2.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  24% 16/66 [00:05<00:17,  2.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  29% 19/66 [00:06<00:16,  2.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 25/66 [00:08<00:13,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  44% 29/66 [00:10<00:12,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  47% 31/66 [00:10<00:12,  2.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  48% 32/66 [00:11<00:11,  2.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  53% 35/66 [00:12<00:10,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.96it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.86it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.89it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.96it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.97it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.97it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.94it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:21:19 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.907 | nll_loss 0.078 | accuracy 69.3 | tp 3 | tn 720 | fp 1 | fn 319 | false 320 | wps 536.2 | wpb 183.7 | bsz 15.8 | num_updates 4815 | best_accuracy 69.3\n",
            "2022-12-15 00:21:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:21:40 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint9.pt (epoch 9 @ 4815 updates, score 69.3) (writing took 21.49298737500021 seconds)\n",
            "2022-12-15 00:21:40 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-12-15 00:21:40 | INFO | train | epoch 009 | loss 0.825 | nll_loss 0.073 | accuracy 70.7 | tp 245 | tn 5800 | fp 223 | fn 2283 | false 2506 | wps 331.2 | ups 1.82 | wpb 181.7 | bsz 16 | num_updates 4815 | lr 9.76387e-08 | gnorm 24.068 | train_wall 248 | wall 2632\n",
            "epoch 010:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:21:40 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010:   7% 37/535 [00:17<03:51,  2.15it/s]2022-12-15 00:21:58 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 010: 100% 534/535 [04:08<00:00,  2.19it/s, loss=0.8, nll_loss=0.071, accuracy=71.4, tp=55, tn=1088, fp=61, fn=396, false=457, wps=396, ups=2.19, wpb=181.2, bsz=16, num_updates=5300, lr=6.74663e-09, gnorm=29.127, train_wall=45, wall=2858]2022-12-15 00:25:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   2% 1/66 [00:00<00:29,  2.19it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   5% 3/66 [00:01<00:23,  2.71it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   6% 4/66 [00:01<00:22,  2.79it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.85it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.89it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 8/66 [00:02<00:19,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 13/66 [00:04<00:18,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  24% 16/66 [00:05<00:16,  2.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  29% 19/66 [00:06<00:16,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 25/66 [00:08<00:14,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  44% 29/66 [00:09<00:12,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  47% 31/66 [00:10<00:12,  2.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  48% 32/66 [00:11<00:11,  2.89it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  52% 34/66 [00:11<00:11,  2.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  53% 35/66 [00:12<00:10,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.87it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.89it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:26:12 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.903 | nll_loss 0.078 | accuracy 69.6 | tp 5 | tn 721 | fp 0 | fn 317 | false 317 | wps 536.2 | wpb 183.7 | bsz 15.8 | num_updates 5350 | best_accuracy 69.6\n",
            "2022-12-15 00:26:12 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:26:33 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint10.pt (epoch 10 @ 5350 updates, score 69.6) (writing took 20.7893506420005 seconds)\n",
            "2022-12-15 00:26:33 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-12-15 00:26:33 | INFO | train | epoch 010 | loss 0.82 | nll_loss 0.072 | accuracy 71.2 | tp 309 | tn 5781 | fp 242 | fn 2219 | false 2461 | wps 332.5 | ups 1.83 | wpb 181.7 | bsz 16 | num_updates 5350 | lr 0 | gnorm 26.55 | train_wall 247 | wall 2925\n",
            "epoch 011:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:26:33 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 534/535 [04:06<00:00,  2.18it/s, loss=0.826, nll_loss=0.072, accuracy=71.2, tp=59, tn=1081, fp=46, fn=414, false=460, wps=400.1, ups=2.18, wpb=183.4, bsz=16, num_updates=5800, lr=0, gnorm=23.429, train_wall=46, wall=3133]2022-12-15 00:30:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   2% 1/66 [00:00<00:29,  2.23it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.62it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 3/66 [00:01<00:22,  2.76it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   6% 4/66 [00:01<00:21,  2.84it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.86it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 6/66 [00:02<00:21,  2.85it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.89it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  12% 8/66 [00:02<00:19,  2.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  20% 13/66 [00:04<00:18,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  24% 16/66 [00:05<00:17,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  29% 19/66 [00:06<00:15,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  38% 25/66 [00:08<00:13,  2.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  44% 29/66 [00:09<00:12,  2.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  47% 31/66 [00:10<00:12,  2.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  48% 32/66 [00:11<00:11,  2.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  53% 35/66 [00:12<00:10,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  65% 43/66 [00:14<00:07,  2.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.94it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:31:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.889 | nll_loss 0.076 | accuracy 69.6 | tp 6 | tn 720 | fp 1 | fn 316 | false 317 | wps 537.8 | wpb 183.7 | bsz 15.8 | num_updates 5885 | best_accuracy 69.6\n",
            "2022-12-15 00:31:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:31:24 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint11.pt (epoch 11 @ 5885 updates, score 69.6) (writing took 21.363875095000367 seconds)\n",
            "2022-12-15 00:31:24 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-12-15 00:31:24 | INFO | train | epoch 011 | loss 0.824 | nll_loss 0.072 | accuracy 70.8 | tp 310 | tn 5742 | fp 281 | fn 2218 | false 2499 | wps 334.1 | ups 1.84 | wpb 181.7 | bsz 16 | num_updates 5885 | lr 0 | gnorm 25.107 | train_wall 245 | wall 3216\n",
            "epoch 012:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:31:24 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 534/535 [04:10<00:00,  2.19it/s, loss=0.83, nll_loss=0.073, accuracy=71.1, tp=52, tn=1086, fp=44, fn=418, false=462, wps=398.5, ups=2.18, wpb=182.7, bsz=16, num_updates=6400, lr=0, gnorm=23.296, train_wall=46, wall=3458]2022-12-15 00:35:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   2% 1/66 [00:00<00:29,  2.21it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   3% 2/66 [00:00<00:24,  2.56it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   5% 3/66 [00:01<00:23,  2.71it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   6% 4/66 [00:01<00:22,  2.81it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   8% 5/66 [00:01<00:21,  2.86it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 6/66 [00:02<00:20,  2.88it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  11% 7/66 [00:02<00:20,  2.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  12% 8/66 [00:02<00:19,  2.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  14% 9/66 [00:03<00:19,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  15% 10/66 [00:03<00:19,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  17% 11/66 [00:03<00:18,  2.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  18% 12/66 [00:04<00:18,  2.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  20% 13/66 [00:04<00:18,  2.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  21% 14/66 [00:04<00:17,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  23% 15/66 [00:05<00:17,  2.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  24% 16/66 [00:05<00:17,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  26% 17/66 [00:05<00:16,  2.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  27% 18/66 [00:06<00:16,  2.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  29% 19/66 [00:06<00:16,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  30% 20/66 [00:06<00:15,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  32% 21/66 [00:07<00:15,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 22/66 [00:07<00:14,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  35% 23/66 [00:07<00:14,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  36% 24/66 [00:08<00:14,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  38% 25/66 [00:08<00:13,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39% 26/66 [00:08<00:13,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  41% 27/66 [00:09<00:13,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  42% 28/66 [00:09<00:12,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  44% 29/66 [00:09<00:12,  2.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45% 30/66 [00:10<00:12,  2.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  47% 31/66 [00:10<00:11,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  48% 32/66 [00:11<00:11,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 33/66 [00:11<00:11,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  52% 34/66 [00:11<00:10,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  53% 35/66 [00:12<00:10,  2.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  55% 36/66 [00:12<00:10,  2.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  56% 37/66 [00:12<00:09,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  58% 38/66 [00:13<00:09,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  59% 39/66 [00:13<00:09,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  61% 40/66 [00:13<00:08,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  62% 41/66 [00:14<00:08,  2.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  64% 42/66 [00:14<00:08,  2.81it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  65% 43/66 [00:14<00:08,  2.83it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 44/66 [00:15<00:07,  2.85it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  68% 45/66 [00:15<00:07,  2.88it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  70% 46/66 [00:15<00:06,  2.89it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  71% 47/66 [00:16<00:06,  2.88it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  73% 48/66 [00:16<00:06,  2.89it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  74% 49/66 [00:16<00:05,  2.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  76% 50/66 [00:17<00:05,  2.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  77% 51/66 [00:17<00:05,  2.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  79% 52/66 [00:17<00:04,  2.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  80% 53/66 [00:18<00:04,  2.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 54/66 [00:18<00:04,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  83% 55/66 [00:18<00:03,  2.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  85% 56/66 [00:19<00:03,  2.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  86% 57/66 [00:19<00:03,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  88% 58/66 [00:19<00:02,  2.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 59/66 [00:20<00:02,  2.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  91% 60/66 [00:20<00:02,  2.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  92% 61/66 [00:20<00:01,  2.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  94% 62/66 [00:21<00:01,  2.97it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  95% 63/66 [00:21<00:01,  2.97it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  97% 64/66 [00:21<00:00,  2.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  98% 65/66 [00:22<00:00,  2.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 66/66 [00:22<00:00,  2.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:35:58 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.919 | nll_loss 0.079 | accuracy 69.4 | tp 3 | tn 721 | fp 0 | fn 319 | false 319 | wps 536 | wpb 183.7 | bsz 15.8 | num_updates 6420 | best_accuracy 69.6\n",
            "2022-12-15 00:35:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:36:11 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1214-233553_ckpt/checkpoint12.pt (epoch 12 @ 6420 updates, score 69.4) (writing took 13.560358147000443 seconds)\n",
            "2022-12-15 00:36:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-12-15 00:36:11 | INFO | train | epoch 012 | loss 0.828 | nll_loss 0.073 | accuracy 70.7 | tp 289 | tn 5756 | fp 267 | fn 2239 | false 2506 | wps 338.2 | ups 1.86 | wpb 181.7 | bsz 16 | num_updates 6420 | lr 0 | gnorm 25.231 | train_wall 249 | wall 3503\n",
            "2022-12-15 00:36:11 | INFO | fairseq_cli.train | done training in 3492.3 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run.py --arch roberta_base --task CoLA --restore-file checkpoint_best_CoLA.pt --lr 1e-6 --force-dequant layernorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4mvGkwBQX1Z",
        "outputId": "55993a92-c648-48ba-90ef-070843fb098b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid_subset: valid\n",
            "valid_interval_updates: None\n",
            "Finetuning from the checkpoint: checkpoint_best_CoLA.pt\n",
            "2022-12-15 00:51:22 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='CoLA-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, force_dequant='layernorm', fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_file='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041.log', log_format=None, log_interval=100, lr=[1e-06], lr_scheduler='polynomial_decay', max_epoch=12, max_positions=512, max_sentences=16, max_sentences_valid=16, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=True, no_shuffle=False, nprocs_per_node=1, num_classes=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_mode='symmetric', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_best_CoLA.pt', save_dir='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=5336, tpu=False, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.1)\n",
            "2022-12-15 00:51:22 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
            "2022-12-15 00:51:23 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2022-12-15 00:51:23 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/input0/valid\n",
            "2022-12-15 00:51:23 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/label/valid\n",
            "2022-12-15 00:51:23 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1043\n",
            "2022-12-15 00:51:23 | INFO | fairseq.modules.transformer_sentence_encoder | Dropout 0.1, attn dropout 0.1, act dropout 0.0\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 00:51:24 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerSentenceEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): QuantEmbedding()\n",
            "      (embed_positions): QuantEmbedding()\n",
            "      (embed_positions_act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (1): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (2): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (3): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (4): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (5): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (6): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (7): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (8): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (9): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (10): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (11): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (emb_layer_norm): IntLayerNorm(\n",
            "        (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2022-12-15 00:51:24 | INFO | fairseq_cli.train | task: sentence_prediction (SentencePredictionTask)\n",
            "2022-12-15 00:51:24 | INFO | fairseq_cli.train | model: roberta_base (RobertaModel)\n",
            "2022-12-15 00:51:24 | INFO | fairseq_cli.train | criterion: sentence_prediction (SentencePredictionCriterion)\n",
            "2022-12-15 00:51:24 | INFO | fairseq_cli.train | num. model params: 125288795 (num. trained: 125288795)\n",
            "2022-12-15 00:51:24 | INFO | fairseq_cli.train | quantize: symmetric\n",
            "2022-12-15 00:51:28 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2022-12-15 00:51:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-15 00:51:28 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          \n",
            "2022-12-15 00:51:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-15 00:51:28 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-12-15 00:51:28 | INFO | fairseq_cli.train | max tokens per GPU = 4400 and max sentences per GPU = 16\n",
            "2022-12-15 00:51:39 | INFO | fairseq.trainer | loaded checkpoint checkpoint_best_CoLA.pt (epoch 10 @ 0 updates)\n",
            "2022-12-15 00:51:39 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n",
            "2022-12-15 00:51:39 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-12-15 00:51:40 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/input0/train\n",
            "2022-12-15 00:51:40 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/label/train\n",
            "2022-12-15 00:51:40 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 8551\n",
            "epoch 001:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:51:40 | INFO | fairseq.trainer | begin training epoch 1\n",
            "/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT/fairseq/utils.py:305: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 534/535 [04:27<00:00,  1.89it/s, loss=0.923, nll_loss=0.081, accuracy=67.6, tp=37, tn=1039, fp=56, fn=459, false=515, wps=362.5, ups=2.01, wpb=180.5, bsz=15.9, num_updates=500, lr=9.06297e-07, gnorm=25.243, train_wall=49, wall=262]2022-12-15 00:56:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/66 [00:00<00:31,  2.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.36it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 3/66 [00:01<00:25,  2.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 5/66 [00:02<00:24,  2.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 7/66 [00:02<00:22,  2.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 9/66 [00:03<00:21,  2.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 11/66 [00:04<00:20,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 14/66 [00:05<00:19,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 18/66 [00:06<00:18,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 19/66 [00:07<00:17,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 22/66 [00:08<00:16,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 27/66 [00:10<00:14,  2.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 31/66 [00:11<00:13,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 32/66 [00:12<00:12,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 36/66 [00:13<00:11,  2.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 40/66 [00:15<00:09,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 44/66 [00:16<00:08,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 45/66 [00:17<00:07,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 49/66 [00:18<00:06,  2.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 52/66 [00:19<00:05,  2.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 53/66 [00:20<00:04,  2.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 54/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 57/66 [00:21<00:03,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 59/66 [00:22<00:02,  2.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 62/66 [00:23<00:01,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 65/66 [00:24<00:00,  2.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 00:56:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.895 | nll_loss 0.077 | accuracy 69.5 | tp 4 | tn 721 | fp 0 | fn 318 | false 318 | wps 479.2 | wpb 183.7 | bsz 15.8 | num_updates 535\n",
            "2022-12-15 00:56:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 00:56:59 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint1.pt (epoch 1 @ 535 updates, score 69.5) (writing took 25.98612765999951 seconds)\n",
            "2022-12-15 00:56:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-12-15 00:56:59 | INFO | train | epoch 001 | loss 0.951 | nll_loss 0.084 | accuracy 66.8 | tp 292 | tn 5416 | fp 607 | fn 2236 | false 2843 | wps 306 | ups 1.68 | wpb 181.7 | bsz 16 | num_updates 535 | lr 8.99738e-07 | gnorm 35.156 | train_wall 267 | wall 331\n",
            "epoch 002:   0% 0/535 [00:00<?, ?it/s]2022-12-15 00:56:59 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002: 100% 534/535 [04:28<00:00,  2.03it/s, loss=0.873, nll_loss=0.077, accuracy=70.8, tp=9, tn=1123, fp=13, fn=455, false=468, wps=363.7, ups=2.01, wpb=181, bsz=16, num_updates=1000, lr=8.12594e-07, gnorm=18.457, train_wall=49, wall=565]2022-12-15 01:01:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 1/66 [00:00<00:33,  1.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 3/66 [00:01<00:26,  2.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 5/66 [00:02<00:23,  2.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 7/66 [00:02<00:22,  2.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 9/66 [00:03<00:21,  2.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 14/66 [00:05<00:19,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 19/66 [00:07<00:17,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 22/66 [00:08<00:16,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 27/66 [00:10<00:14,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 31/66 [00:11<00:13,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 32/66 [00:12<00:12,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 36/66 [00:13<00:11,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 40/66 [00:15<00:10,  2.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 44/66 [00:16<00:08,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 45/66 [00:17<00:07,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 49/66 [00:18<00:06,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 53/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 54/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 57/66 [00:21<00:03,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 59/66 [00:22<00:02,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 62/66 [00:23<00:01,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 65/66 [00:24<00:00,  2.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:01:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.887 | nll_loss 0.076 | accuracy 69 | tp 0 | tn 720 | fp 1 | fn 322 | false 323 | wps 478.9 | wpb 183.7 | bsz 15.8 | num_updates 1070 | best_accuracy 69.5\n",
            "2022-12-15 01:01:54 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:02:09 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint2.pt (epoch 2 @ 1070 updates, score 69.0) (writing took 15.22558897899944 seconds)\n",
            "2022-12-15 01:02:09 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-12-15 01:02:09 | INFO | train | epoch 002 | loss 0.882 | nll_loss 0.078 | accuracy 70.1 | tp 52 | tn 5945 | fp 78 | fn 2476 | false 2554 | wps 313.8 | ups 1.73 | wpb 181.7 | bsz 16 | num_updates 1070 | lr 7.99475e-07 | gnorm 20.57 | train_wall 267 | wall 641\n",
            "epoch 003:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:02:09 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 534/535 [04:27<00:00,  2.02it/s, loss=0.876, nll_loss=0.077, accuracy=68.2, tp=18, tn=1074, fp=20, fn=488, false=508, wps=365.7, ups=2.01, wpb=181.5, bsz=16, num_updates=1600, lr=7.0015e-07, gnorm=18.911, train_wall=49, wall=906]2022-12-15 01:06:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 1/66 [00:00<00:32,  1.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 3/66 [00:01<00:25,  2.45it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 5/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 7/66 [00:02<00:22,  2.57it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  14% 9/66 [00:03<00:21,  2.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 14/66 [00:05<00:19,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 18/66 [00:06<00:18,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 19/66 [00:07<00:17,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 22/66 [00:08<00:16,  2.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  41% 27/66 [00:10<00:14,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 31/66 [00:11<00:13,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  48% 32/66 [00:12<00:12,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 36/66 [00:13<00:11,  2.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  61% 40/66 [00:15<00:09,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 44/66 [00:16<00:08,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 45/66 [00:17<00:07,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 49/66 [00:18<00:06,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 53/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 54/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  86% 57/66 [00:21<00:03,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 59/66 [00:22<00:02,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  94% 62/66 [00:23<00:01,  2.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 65/66 [00:24<00:00,  2.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.64it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:07:03 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.888 | nll_loss 0.076 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 479 | wpb 183.7 | bsz 15.8 | num_updates 1605 | best_accuracy 69.5\n",
            "2022-12-15 01:07:03 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:07:17 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint3.pt (epoch 3 @ 1605 updates, score 69.2) (writing took 14.436658923999858 seconds)\n",
            "2022-12-15 01:07:17 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-12-15 01:07:17 | INFO | train | epoch 003 | loss 0.86 | nll_loss 0.076 | accuracy 70.5 | tp 66 | tn 5963 | fp 60 | fn 2462 | false 2522 | wps 315.8 | ups 1.74 | wpb 181.7 | bsz 16 | num_updates 1605 | lr 6.99213e-07 | gnorm 19.779 | train_wall 266 | wall 949\n",
            "epoch 004:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:07:17 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 534/535 [04:29<00:00,  1.96it/s, loss=0.868, nll_loss=0.076, accuracy=69.7, tp=19, tn=1096, fp=12, fn=473, false=485, wps=366.4, ups=2.01, wpb=182.3, bsz=16, num_updates=2100, lr=6.06447e-07, gnorm=21.311, train_wall=49, wall=1198]2022-12-15 01:11:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 1/66 [00:00<00:32,  1.99it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 3/66 [00:01<00:25,  2.46it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.52it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 5/66 [00:02<00:23,  2.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 7/66 [00:02<00:23,  2.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.56it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  14% 9/66 [00:03<00:22,  2.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 14/66 [00:05<00:19,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  29% 19/66 [00:07<00:18,  2.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 22/66 [00:08<00:16,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  41% 27/66 [00:10<00:15,  2.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 31/66 [00:12<00:13,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 32/66 [00:12<00:13,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 36/66 [00:13<00:11,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  61% 40/66 [00:15<00:09,  2.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 44/66 [00:16<00:08,  2.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 45/66 [00:17<00:07,  2.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  74% 49/66 [00:18<00:06,  2.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80% 53/66 [00:20<00:04,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 54/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  86% 57/66 [00:21<00:03,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 59/66 [00:22<00:02,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  94% 62/66 [00:23<00:01,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98% 65/66 [00:24<00:00,  2.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:12:12 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.901 | nll_loss 0.077 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 478.8 | wpb 183.7 | bsz 15.8 | num_updates 2140 | best_accuracy 69.5\n",
            "2022-12-15 01:12:12 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:12:26 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint4.pt (epoch 4 @ 2140 updates, score 69.2) (writing took 13.423189830999945 seconds)\n",
            "2022-12-15 01:12:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-12-15 01:12:26 | INFO | train | epoch 004 | loss 0.856 | nll_loss 0.075 | accuracy 70.4 | tp 87 | tn 5934 | fp 89 | fn 2441 | false 2530 | wps 315.2 | ups 1.73 | wpb 181.7 | bsz 16 | num_updates 2140 | lr 5.98951e-07 | gnorm 21.224 | train_wall 268 | wall 1257\n",
            "epoch 005:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:12:26 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 534/535 [04:27<00:00,  2.02it/s, loss=0.858, nll_loss=0.075, accuracy=69.2, tp=35, tn=1073, fp=31, fn=461, false=492, wps=368.3, ups=2, wpb=183.7, bsz=16, num_updates=2600, lr=5.12744e-07, gnorm=25.134, train_wall=50, wall=1488]2022-12-15 01:16:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   2% 1/66 [00:00<00:33,  1.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 3/66 [00:01<00:25,  2.44it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 5/66 [00:02<00:23,  2.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 7/66 [00:02<00:22,  2.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  14% 9/66 [00:03<00:21,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 11/66 [00:04<00:20,  2.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 14/66 [00:05<00:19,  2.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  27% 18/66 [00:06<00:18,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 19/66 [00:07<00:18,  2.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 22/66 [00:08<00:17,  2.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 27/66 [00:10<00:14,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  47% 31/66 [00:11<00:13,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  48% 32/66 [00:12<00:13,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 36/66 [00:13<00:11,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  61% 40/66 [00:15<00:09,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 44/66 [00:16<00:08,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 45/66 [00:17<00:08,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  74% 49/66 [00:18<00:06,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80% 53/66 [00:20<00:04,  2.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 54/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  86% 57/66 [00:21<00:03,  2.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 59/66 [00:22<00:02,  2.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94% 62/66 [00:23<00:01,  2.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  98% 65/66 [00:24<00:00,  2.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.62it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:17:19 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.883 | nll_loss 0.076 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 478.5 | wpb 183.7 | bsz 15.8 | num_updates 2675 | best_accuracy 69.5\n",
            "2022-12-15 01:17:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:17:32 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint5.pt (epoch 5 @ 2675 updates, score 69.2) (writing took 13.352892758000053 seconds)\n",
            "2022-12-15 01:17:32 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-12-15 01:17:32 | INFO | train | epoch 005 | loss 0.845 | nll_loss 0.074 | accuracy 70.4 | tp 121 | tn 5899 | fp 124 | fn 2407 | false 2531 | wps 317 | ups 1.74 | wpb 181.7 | bsz 16 | num_updates 2675 | lr 4.98688e-07 | gnorm 23.86 | train_wall 266 | wall 1564\n",
            "epoch 006:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:17:32 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 534/535 [04:28<00:00,  2.00it/s, loss=0.83, nll_loss=0.072, accuracy=71.1, tp=30, tn=1107, fp=29, fn=434, false=463, wps=372.5, ups=2.01, wpb=185, bsz=16, num_updates=3200, lr=4.003e-07, gnorm=23.941, train_wall=49, wall=1827]2022-12-15 01:22:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   2% 1/66 [00:00<00:32,  1.99it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   3% 2/66 [00:00<00:29,  2.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   5% 3/66 [00:01<00:26,  2.36it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 4/66 [00:01<00:25,  2.46it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 5/66 [00:02<00:24,  2.52it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 7/66 [00:02<00:22,  2.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.58it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  14% 9/66 [00:03<00:22,  2.59it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  15% 10/66 [00:04<00:22,  2.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 12/66 [00:04<00:21,  2.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 13/66 [00:05<00:21,  2.45it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  21% 14/66 [00:05<00:21,  2.46it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  23% 15/66 [00:06<00:20,  2.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  26% 17/66 [00:06<00:19,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.59it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 19/66 [00:07<00:18,  2.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  30% 20/66 [00:07<00:18,  2.53it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  32% 21/66 [00:08<00:18,  2.38it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 22/66 [00:08<00:18,  2.39it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 23/66 [00:09<00:17,  2.46it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 25/66 [00:10<00:16,  2.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  41% 27/66 [00:10<00:15,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 28/66 [00:11<00:14,  2.59it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.59it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47% 31/66 [00:12<00:13,  2.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 32/66 [00:12<00:13,  2.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 33/66 [00:13<00:12,  2.62it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 36/66 [00:14<00:11,  2.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.64it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 40/66 [00:15<00:09,  2.65it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 41/66 [00:16<00:09,  2.64it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.51it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 43/66 [00:16<00:09,  2.48it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 44/66 [00:17<00:08,  2.53it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 45/66 [00:17<00:08,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  70% 46/66 [00:18<00:07,  2.59it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.59it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 49/66 [00:19<00:06,  2.59it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 51/66 [00:20<00:05,  2.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  80% 53/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 54/66 [00:21<00:04,  2.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  86% 57/66 [00:22<00:03,  2.44it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.39it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 59/66 [00:23<00:02,  2.46it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.51it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.52it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  94% 62/66 [00:24<00:01,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.53it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  97% 64/66 [00:25<00:00,  2.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 65/66 [00:25<00:00,  2.58it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:22:27 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.883 | nll_loss 0.076 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 468.6 | wpb 183.7 | bsz 15.8 | num_updates 3210 | best_accuracy 69.5\n",
            "2022-12-15 01:22:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:22:41 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint6.pt (epoch 6 @ 3210 updates, score 69.2) (writing took 14.037998098000571 seconds)\n",
            "2022-12-15 01:22:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-12-15 01:22:41 | INFO | train | epoch 006 | loss 0.843 | nll_loss 0.074 | accuracy 70.4 | tp 140 | tn 5884 | fp 139 | fn 2388 | false 2527 | wps 314.9 | ups 1.73 | wpb 181.7 | bsz 16 | num_updates 3210 | lr 3.98426e-07 | gnorm 23.883 | train_wall 267 | wall 1873\n",
            "epoch 007:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:22:41 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 534/535 [04:29<00:00,  2.00it/s, loss=0.847, nll_loss=0.073, accuracy=70.1, tp=35, tn=1087, fp=33, fn=445, false=478, wps=368.6, ups=2, wpb=184.4, bsz=16, num_updates=3700, lr=3.06597e-07, gnorm=26.837, train_wall=50, wall=2121]2022-12-15 01:27:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   2% 1/66 [00:00<00:33,  1.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.29it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   5% 3/66 [00:01<00:26,  2.42it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.49it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8% 5/66 [00:02<00:24,  2.53it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.55it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 7/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  14% 9/66 [00:03<00:22,  2.59it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  21% 14/66 [00:05<00:19,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.59it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  29% 19/66 [00:07<00:18,  2.57it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 22/66 [00:08<00:17,  2.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  41% 27/66 [00:10<00:15,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  47% 31/66 [00:12<00:13,  2.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  48% 32/66 [00:12<00:12,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  55% 36/66 [00:13<00:11,  2.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 40/66 [00:15<00:10,  2.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 44/66 [00:16<00:08,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  68% 45/66 [00:17<00:08,  2.62it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73% 48/66 [00:18<00:07,  2.57it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  74% 49/66 [00:18<00:06,  2.49it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.53it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.55it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.57it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  80% 53/66 [00:20<00:05,  2.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 54/66 [00:20<00:04,  2.59it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  86% 57/66 [00:22<00:03,  2.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 59/66 [00:22<00:02,  2.59it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.59it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  94% 62/66 [00:23<00:01,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  98% 65/66 [00:25<00:00,  2.60it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:27:37 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.902 | nll_loss 0.078 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 475.7 | wpb 183.7 | bsz 15.8 | num_updates 3745 | best_accuracy 69.5\n",
            "2022-12-15 01:27:37 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:27:52 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint7.pt (epoch 7 @ 3745 updates, score 69.2) (writing took 14.674505682000017 seconds)\n",
            "2022-12-15 01:27:52 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-12-15 01:27:52 | INFO | train | epoch 007 | loss 0.835 | nll_loss 0.073 | accuracy 70.3 | tp 186 | tn 5822 | fp 201 | fn 2342 | false 2543 | wps 313 | ups 1.72 | wpb 181.7 | bsz 16 | num_updates 3745 | lr 2.98163e-07 | gnorm 24.718 | train_wall 269 | wall 2183\n",
            "epoch 008:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:27:52 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 534/535 [04:31<00:00,  2.00it/s, loss=0.848, nll_loss=0.075, accuracy=69.7, tp=47, tn=1068, fp=47, fn=438, false=485, wps=364.6, ups=2, wpb=181.9, bsz=16, num_updates=4200, lr=2.12894e-07, gnorm=27.476, train_wall=50, wall=2415]2022-12-15 01:32:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   2% 1/66 [00:00<00:32,  1.99it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   5% 3/66 [00:01<00:25,  2.45it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.50it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8% 5/66 [00:02<00:24,  2.54it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 7/66 [00:02<00:22,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  14% 9/66 [00:03<00:22,  2.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  21% 14/66 [00:05<00:20,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.62it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  29% 19/66 [00:07<00:18,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 22/66 [00:08<00:16,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  41% 27/66 [00:10<00:14,  2.62it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.62it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.63it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  47% 31/66 [00:12<00:13,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  48% 32/66 [00:12<00:13,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  53% 35/66 [00:13<00:12,  2.50it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  55% 36/66 [00:14<00:11,  2.53it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  61% 40/66 [00:15<00:10,  2.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.54it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 44/66 [00:17<00:08,  2.53it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  68% 45/66 [00:17<00:08,  2.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  73% 48/66 [00:18<00:07,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  74% 49/66 [00:19<00:06,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.53it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.45it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  80% 53/66 [00:20<00:05,  2.44it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 54/66 [00:21<00:04,  2.49it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.51it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  86% 57/66 [00:22<00:03,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 59/66 [00:23<00:02,  2.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.62it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.63it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  94% 62/66 [00:24<00:01,  2.63it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.64it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.62it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  98% 65/66 [00:25<00:00,  2.63it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.64it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:32:49 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.907 | nll_loss 0.078 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 473 | wpb 183.7 | bsz 15.8 | num_updates 4280 | best_accuracy 69.5\n",
            "2022-12-15 01:32:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:33:04 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint8.pt (epoch 8 @ 4280 updates, score 69.2) (writing took 15.025524669999868 seconds)\n",
            "2022-12-15 01:33:04 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-12-15 01:33:04 | INFO | train | epoch 008 | loss 0.834 | nll_loss 0.073 | accuracy 70.5 | tp 206 | tn 5821 | fp 202 | fn 2322 | false 2524 | wps 311.4 | ups 1.71 | wpb 181.7 | bsz 16 | num_updates 4280 | lr 1.97901e-07 | gnorm 26.445 | train_wall 270 | wall 2495\n",
            "epoch 009:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:33:04 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 534/535 [04:30<00:00,  1.99it/s, loss=0.837, nll_loss=0.074, accuracy=70.6, tp=54, tn=1076, fp=43, fn=427, false=470, wps=359.9, ups=1.99, wpb=180.7, bsz=16, num_updates=4800, lr=1.0045e-07, gnorm=24.154, train_wall=50, wall=2759]2022-12-15 01:37:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   2% 1/66 [00:00<00:32,  1.99it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   5% 3/66 [00:01<00:25,  2.44it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.51it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8% 5/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 7/66 [00:02<00:22,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  14% 9/66 [00:03<00:22,  2.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  21% 14/66 [00:05<00:20,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.62it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  29% 19/66 [00:07<00:17,  2.62it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.62it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 22/66 [00:08<00:16,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  41% 27/66 [00:10<00:15,  2.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  47% 31/66 [00:12<00:13,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  48% 32/66 [00:12<00:13,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  55% 36/66 [00:13<00:11,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  61% 40/66 [00:15<00:09,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 44/66 [00:17<00:08,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  68% 45/66 [00:17<00:08,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.55it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  74% 49/66 [00:19<00:06,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  80% 53/66 [00:20<00:05,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 54/66 [00:20<00:04,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  86% 57/66 [00:22<00:03,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 59/66 [00:22<00:02,  2.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  94% 62/66 [00:24<00:01,  2.61it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  98% 65/66 [00:25<00:00,  2.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.59it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:38:00 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.894 | nll_loss 0.077 | accuracy 69.2 | tp 2 | tn 720 | fp 1 | fn 320 | false 321 | wps 474.3 | wpb 183.7 | bsz 15.8 | num_updates 4815 | best_accuracy 69.5\n",
            "2022-12-15 01:38:00 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:38:16 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint9.pt (epoch 9 @ 4815 updates, score 69.2) (writing took 15.710606606999136 seconds)\n",
            "2022-12-15 01:38:16 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-12-15 01:38:16 | INFO | train | epoch 009 | loss 0.83 | nll_loss 0.073 | accuracy 70.7 | tp 228 | tn 5819 | fp 204 | fn 2300 | false 2504 | wps 311.8 | ups 1.72 | wpb 181.7 | bsz 16 | num_updates 4815 | lr 9.76387e-08 | gnorm 25.843 | train_wall 269 | wall 2807\n",
            "epoch 010:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:38:16 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 534/535 [04:31<00:00,  1.95it/s, loss=0.809, nll_loss=0.071, accuracy=72.3, tp=51, tn=1106, fp=43, fn=400, false=443, wps=356, ups=1.96, wpb=181.2, bsz=16, num_updates=5300, lr=6.74663e-09, gnorm=22.218, train_wall=51, wall=3054]2022-12-15 01:42:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   2% 1/66 [00:00<00:33,  1.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   5% 3/66 [00:01<00:26,  2.42it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.49it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 5/66 [00:02<00:24,  2.53it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 7/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.56it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  14% 9/66 [00:03<00:22,  2.56it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  15% 10/66 [00:04<00:21,  2.56it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.53it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  18% 12/66 [00:04<00:21,  2.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  21% 14/66 [00:05<00:20,  2.56it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.57it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.56it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  26% 17/66 [00:06<00:19,  2.57it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.59it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  29% 19/66 [00:07<00:18,  2.50it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.36it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  32% 21/66 [00:08<00:18,  2.37it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 22/66 [00:08<00:18,  2.44it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  35% 23/66 [00:09<00:17,  2.43it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.48it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 25/66 [00:10<00:16,  2.53it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  41% 27/66 [00:10<00:15,  2.57it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  42% 28/66 [00:11<00:14,  2.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  47% 31/66 [00:12<00:13,  2.59it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  48% 32/66 [00:12<00:13,  2.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 33/66 [00:13<00:12,  2.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.57it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  53% 35/66 [00:13<00:12,  2.57it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  55% 36/66 [00:14<00:11,  2.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.61it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  58% 38/66 [00:15<00:10,  2.61it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  61% 40/66 [00:15<00:09,  2.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 41/66 [00:16<00:09,  2.61it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 44/66 [00:17<00:08,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 45/66 [00:17<00:07,  2.63it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  70% 46/66 [00:18<00:07,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  74% 49/66 [00:19<00:06,  2.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  77% 51/66 [00:20<00:05,  2.57it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  80% 53/66 [00:20<00:05,  2.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 54/66 [00:21<00:04,  2.59it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.59it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  86% 57/66 [00:22<00:03,  2.58it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 59/66 [00:23<00:02,  2.60it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  94% 62/66 [00:24<00:01,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.62it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.63it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  98% 65/66 [00:25<00:00,  2.57it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.59it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:43:13 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.906 | nll_loss 0.078 | accuracy 69.2 | tp 3 | tn 719 | fp 2 | fn 319 | false 321 | wps 470.8 | wpb 183.7 | bsz 15.8 | num_updates 5350 | best_accuracy 69.5\n",
            "2022-12-15 01:43:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:43:28 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint10.pt (epoch 10 @ 5350 updates, score 69.2) (writing took 14.366819867000231 seconds)\n",
            "2022-12-15 01:43:28 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-12-15 01:43:28 | INFO | train | epoch 010 | loss 0.829 | nll_loss 0.073 | accuracy 71.1 | tp 269 | tn 5808 | fp 215 | fn 2259 | false 2474 | wps 311.7 | ups 1.71 | wpb 181.7 | bsz 16 | num_updates 5350 | lr 0 | gnorm 26.395 | train_wall 270 | wall 3119\n",
            "epoch 011:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:43:28 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 534/535 [04:33<00:00,  2.01it/s, loss=0.825, nll_loss=0.072, accuracy=70.7, tp=57, tn=1074, fp=53, fn=416, false=469, wps=360.8, ups=1.97, wpb=183.4, bsz=16, num_updates=5800, lr=0, gnorm=27.048, train_wall=51, wall=3350]2022-12-15 01:48:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   2% 1/66 [00:00<00:32,  1.99it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 3/66 [00:01<00:25,  2.45it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   8% 5/66 [00:02<00:23,  2.56it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.58it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  11% 7/66 [00:02<00:22,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.56it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  14% 9/66 [00:03<00:23,  2.40it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  15% 10/66 [00:04<00:23,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  17% 11/66 [00:04<00:22,  2.43it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  18% 12/66 [00:04<00:21,  2.49it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  21% 14/66 [00:05<00:20,  2.55it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  23% 15/66 [00:06<00:19,  2.58it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.60it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.61it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  29% 19/66 [00:07<00:17,  2.61it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.62it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.61it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 22/66 [00:08<00:16,  2.62it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  35% 23/66 [00:09<00:16,  2.60it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.48it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  38% 25/66 [00:09<00:17,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  39% 26/66 [00:10<00:17,  2.27it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  41% 27/66 [00:10<00:17,  2.21it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  42% 28/66 [00:11<00:17,  2.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  44% 29/66 [00:11<00:17,  2.15it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 30/66 [00:12<00:16,  2.13it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  47% 31/66 [00:12<00:15,  2.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.30it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 33/66 [00:13<00:13,  2.37it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  52% 34/66 [00:13<00:13,  2.44it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  53% 35/66 [00:14<00:12,  2.49it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  55% 36/66 [00:14<00:11,  2.53it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 37/66 [00:15<00:11,  2.55it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  58% 38/66 [00:15<00:10,  2.56it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  61% 40/66 [00:16<00:10,  2.58it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  62% 41/66 [00:16<00:09,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 42/66 [00:17<00:09,  2.58it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  65% 43/66 [00:17<00:09,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 44/66 [00:17<00:08,  2.55it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  68% 45/66 [00:18<00:08,  2.57it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  70% 46/66 [00:18<00:07,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  73% 48/66 [00:19<00:06,  2.61it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  74% 49/66 [00:19<00:06,  2.58it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  76% 50/66 [00:20<00:06,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  77% 51/66 [00:20<00:05,  2.57it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  80% 53/66 [00:21<00:05,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 54/66 [00:21<00:04,  2.61it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  83% 55/66 [00:22<00:04,  2.60it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  85% 56/66 [00:22<00:03,  2.62it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  86% 57/66 [00:22<00:03,  2.61it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  88% 58/66 [00:23<00:03,  2.60it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 59/66 [00:23<00:02,  2.60it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.60it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  92% 61/66 [00:24<00:01,  2.60it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  94% 62/66 [00:24<00:01,  2.59it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  95% 63/66 [00:25<00:01,  2.57it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  97% 64/66 [00:25<00:00,  2.39it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  98% 65/66 [00:26<00:00,  2.30it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 66/66 [00:26<00:00,  2.37it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:48:28 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.898 | nll_loss 0.077 | accuracy 69.3 | tp 2 | tn 721 | fp 0 | fn 320 | false 320 | wps 457.3 | wpb 183.7 | bsz 15.8 | num_updates 5885 | best_accuracy 69.5\n",
            "2022-12-15 01:48:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:48:44 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint11.pt (epoch 11 @ 5885 updates, score 69.3) (writing took 16.033436182000514 seconds)\n",
            "2022-12-15 01:48:44 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-12-15 01:48:44 | INFO | train | epoch 011 | loss 0.824 | nll_loss 0.072 | accuracy 70.7 | tp 278 | tn 5767 | fp 256 | fn 2250 | false 2506 | wps 307.2 | ups 1.69 | wpb 181.7 | bsz 16 | num_updates 5885 | lr 0 | gnorm 24.566 | train_wall 272 | wall 3436\n",
            "epoch 012:   0% 0/535 [00:00<?, ?it/s]2022-12-15 01:48:44 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 534/535 [04:29<00:00,  2.00it/s, loss=0.838, nll_loss=0.073, accuracy=71.6, tp=46, tn=1099, fp=31, fn=424, false=455, wps=364.5, ups=1.99, wpb=182.7, bsz=16, num_updates=6400, lr=0, gnorm=26.061, train_wall=50, wall=3696]2022-12-15 01:53:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   2% 1/66 [00:00<00:33,  1.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   3% 2/66 [00:00<00:27,  2.29it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   5% 3/66 [00:01<00:25,  2.42it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   6% 4/66 [00:01<00:24,  2.50it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   8% 5/66 [00:02<00:24,  2.53it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 6/66 [00:02<00:23,  2.57it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  11% 7/66 [00:02<00:23,  2.55it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  12% 8/66 [00:03<00:22,  2.57it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  14% 9/66 [00:03<00:22,  2.59it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  15% 10/66 [00:03<00:21,  2.59it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  17% 11/66 [00:04<00:21,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  18% 12/66 [00:04<00:20,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  20% 13/66 [00:05<00:20,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  21% 14/66 [00:05<00:19,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  23% 15/66 [00:05<00:19,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  24% 16/66 [00:06<00:19,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  26% 17/66 [00:06<00:18,  2.63it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  27% 18/66 [00:07<00:18,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  29% 19/66 [00:07<00:17,  2.63it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  30% 20/66 [00:07<00:17,  2.63it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  32% 21/66 [00:08<00:17,  2.57it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 22/66 [00:08<00:17,  2.59it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  35% 23/66 [00:08<00:16,  2.57it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  36% 24/66 [00:09<00:16,  2.58it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  38% 25/66 [00:09<00:15,  2.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39% 26/66 [00:10<00:15,  2.58it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  41% 27/66 [00:10<00:14,  2.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  42% 28/66 [00:10<00:14,  2.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  44% 29/66 [00:11<00:14,  2.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45% 30/66 [00:11<00:13,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  47% 31/66 [00:12<00:13,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  48% 32/66 [00:12<00:12,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 33/66 [00:12<00:12,  2.63it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  52% 34/66 [00:13<00:12,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  53% 35/66 [00:13<00:11,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  55% 36/66 [00:13<00:11,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  56% 37/66 [00:14<00:11,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  58% 38/66 [00:14<00:10,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  59% 39/66 [00:15<00:10,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  61% 40/66 [00:15<00:09,  2.63it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  62% 41/66 [00:15<00:09,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  64% 42/66 [00:16<00:09,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  65% 43/66 [00:16<00:08,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 44/66 [00:16<00:08,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  68% 45/66 [00:17<00:08,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  70% 46/66 [00:17<00:07,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  71% 47/66 [00:18<00:07,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  73% 48/66 [00:18<00:06,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  74% 49/66 [00:18<00:06,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  76% 50/66 [00:19<00:06,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  77% 51/66 [00:19<00:05,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  79% 52/66 [00:20<00:05,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  80% 53/66 [00:20<00:04,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 54/66 [00:20<00:04,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  83% 55/66 [00:21<00:04,  2.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  85% 56/66 [00:21<00:03,  2.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  86% 57/66 [00:21<00:03,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  88% 58/66 [00:22<00:03,  2.59it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 59/66 [00:22<00:02,  2.59it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  91% 60/66 [00:23<00:02,  2.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  92% 61/66 [00:23<00:01,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  94% 62/66 [00:23<00:01,  2.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  95% 63/66 [00:24<00:01,  2.62it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  97% 64/66 [00:24<00:00,  2.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  98% 65/66 [00:25<00:00,  2.59it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 66/66 [00:25<00:00,  2.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 01:53:39 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.913 | nll_loss 0.079 | accuracy 69.4 | tp 3 | tn 721 | fp 0 | fn 319 | false 319 | wps 477.5 | wpb 183.7 | bsz 15.8 | num_updates 6420 | best_accuracy 69.5\n",
            "2022-12-15 01:53:39 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 01:53:55 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-005041_ckpt/checkpoint12.pt (epoch 12 @ 6420 updates, score 69.4) (writing took 15.504014816999188 seconds)\n",
            "2022-12-15 01:53:55 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-12-15 01:53:55 | INFO | train | epoch 012 | loss 0.831 | nll_loss 0.073 | accuracy 71 | tp 268 | tn 5799 | fp 224 | fn 2260 | false 2484 | wps 313 | ups 1.72 | wpb 181.7 | bsz 16 | num_updates 6420 | lr 0 | gnorm 26.655 | train_wall 268 | wall 3746\n",
            "2022-12-15 01:53:55 | INFO | fairseq_cli.train | done training in 3735.4 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run.py --arch roberta_base --task CoLA --restore-file checkpoint_best_CoLA.pt --lr 1e-6 --force-dequant softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UsZs0yQkiqm",
        "outputId": "b9b1c33b-69ae-485c-9963-2351b44dd586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid_subset: valid\n",
            "valid_interval_updates: None\n",
            "Finetuning from the checkpoint: checkpoint_best_CoLA.pt\n",
            "2022-12-15 02:18:04 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='CoLA-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, force_dequant='softmax', fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_file='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721.log', log_format=None, log_interval=100, lr=[1e-06], lr_scheduler='polynomial_decay', max_epoch=12, max_positions=512, max_sentences=16, max_sentences_valid=16, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=True, no_shuffle=False, nprocs_per_node=1, num_classes=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_mode='symmetric', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_best_CoLA.pt', save_dir='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=5336, tpu=False, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.1)\n",
            "2022-12-15 02:18:04 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
            "2022-12-15 02:18:04 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2022-12-15 02:18:04 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/input0/valid\n",
            "2022-12-15 02:18:04 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/label/valid\n",
            "2022-12-15 02:18:04 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1043\n",
            "2022-12-15 02:18:04 | INFO | fairseq.modules.transformer_sentence_encoder | Dropout 0.1, attn dropout 0.1, act dropout 0.0\n",
            "2022-12-15 02:18:04 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:05 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 02:18:06 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerSentenceEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): QuantEmbedding()\n",
            "      (embed_positions): QuantEmbedding()\n",
            "      (embed_positions_act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (1): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (2): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (3): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (4): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (5): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (6): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (7): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (8): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (9): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (10): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (11): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (emb_layer_norm): IntLayerNorm(\n",
            "        (activation): QuantAct(activation_bit=32, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2022-12-15 02:18:06 | INFO | fairseq_cli.train | task: sentence_prediction (SentencePredictionTask)\n",
            "2022-12-15 02:18:06 | INFO | fairseq_cli.train | model: roberta_base (RobertaModel)\n",
            "2022-12-15 02:18:06 | INFO | fairseq_cli.train | criterion: sentence_prediction (SentencePredictionCriterion)\n",
            "2022-12-15 02:18:06 | INFO | fairseq_cli.train | num. model params: 125288795 (num. trained: 125288795)\n",
            "2022-12-15 02:18:06 | INFO | fairseq_cli.train | quantize: symmetric\n",
            "2022-12-15 02:18:08 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2022-12-15 02:18:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-15 02:18:08 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          \n",
            "2022-12-15 02:18:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-15 02:18:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-12-15 02:18:08 | INFO | fairseq_cli.train | max tokens per GPU = 4400 and max sentences per GPU = 16\n",
            "2022-12-15 02:18:19 | INFO | fairseq.trainer | loaded checkpoint checkpoint_best_CoLA.pt (epoch 10 @ 0 updates)\n",
            "2022-12-15 02:18:19 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n",
            "2022-12-15 02:18:19 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-12-15 02:18:19 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/input0/train\n",
            "2022-12-15 02:18:19 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/label/train\n",
            "2022-12-15 02:18:19 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 8551\n",
            "epoch 001:   0% 0/535 [00:00<?, ?it/s]2022-12-15 02:18:19 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-12-15 02:18:20 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 1\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 0 -> 6\n",
            "/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT/fairseq/utils.py:305: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:   0% 1/535 [00:02<26:03,  2.93s/it]2022-12-15 02:18:22 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "2022-12-15 02:18:23 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 001:   0% 2/535 [00:03<13:42,  1.54s/it]2022-12-15 02:18:23 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 001:  25% 136/535 [01:15<03:36,  1.84it/s, loss=1.396, nll_loss=0.123, accuracy=58.6, tp=126, tn=812, fp=309, fn=353, false=662, wps=334.3, ups=1.85, wpb=181.1, bsz=16, num_updates=100, lr=9.81259e-07, gnorm=74.227, train_wall=56, wall=68]2022-12-15 02:19:35 | INFO | fairseq.quantization.utils.quant_modules | Dynamic shift adjustment: 6 -> 7\n",
            "epoch 001: 100% 534/535 [04:50<00:00,  1.86it/s, loss=0.922, nll_loss=0.081, accuracy=67.9, tp=46, tn=1034, fp=61, fn=450, false=511, wps=335, ups=1.86, wpb=180.5, bsz=15.9, num_updates=500, lr=9.06297e-07, gnorm=33.907, train_wall=54, wall=283]2022-12-15 02:23:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/66 [00:00<00:34,  1.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 3/66 [00:01<00:28,  2.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 4/66 [00:01<00:27,  2.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 5/66 [00:02<00:26,  2.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 9/66 [00:03<00:24,  2.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 10/66 [00:04<00:23,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 16/66 [00:06<00:21,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 17/66 [00:07<00:20,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 22/66 [00:09<00:18,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 23/66 [00:09<00:18,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 24/66 [00:10<00:17,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 30/66 [00:12<00:15,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 31/66 [00:13<00:15,  2.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 37/66 [00:15<00:12,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 38/66 [00:16<00:11,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 44/66 [00:18<00:09,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 45/66 [00:19<00:08,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 51/66 [00:21<00:06,  2.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 52/66 [00:22<00:05,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 58/66 [00:24<00:03,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 59/66 [00:25<00:03,  2.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 65/66 [00:27<00:00,  2.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 02:23:38 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.898 | nll_loss 0.077 | accuracy 69.2 | tp 4 | tn 718 | fp 3 | fn 318 | false 321 | wps 428 | wpb 183.7 | bsz 15.8 | num_updates 535\n",
            "2022-12-15 02:23:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 02:24:00 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint1.pt (epoch 1 @ 535 updates, score 69.2) (writing took 21.492538555001374 seconds)\n",
            "2022-12-15 02:24:00 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-12-15 02:24:00 | INFO | train | epoch 001 | loss 1.026 | nll_loss 0.09 | accuracy 65.7 | tp 352 | tn 5263 | fp 760 | fn 2176 | false 2936 | wps 287.4 | ups 1.58 | wpb 181.7 | bsz 16 | num_updates 535 | lr 8.99738e-07 | gnorm 42.919 | train_wall 289 | wall 352\n",
            "epoch 002:   0% 0/535 [00:00<?, ?it/s]2022-12-15 02:24:00 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002: 100% 534/535 [04:50<00:00,  1.87it/s, loss=0.863, nll_loss=0.076, accuracy=70.9, tp=10, tn=1124, fp=12, fn=454, false=466, wps=337.4, ups=1.86, wpb=181, bsz=16, num_updates=1000, lr=8.12594e-07, gnorm=26.238, train_wall=53, wall=605]2022-12-15 02:28:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 2/66 [00:01<00:31,  2.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 3/66 [00:01<00:29,  2.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 4/66 [00:01<00:28,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 5/66 [00:02<00:27,  2.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 9/66 [00:04<00:24,  2.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 15/66 [00:06<00:22,  2.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 16/66 [00:07<00:22,  2.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 17/66 [00:07<00:21,  2.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 22/66 [00:09<00:18,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 23/66 [00:10<00:18,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 29/66 [00:12<00:16,  2.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 30/66 [00:13<00:15,  2.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 31/66 [00:13<00:15,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 36/66 [00:15<00:13,  2.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 37/66 [00:16<00:12,  2.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 38/66 [00:16<00:12,  2.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 39/66 [00:17<00:11,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 44/66 [00:19<00:09,  2.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 45/66 [00:19<00:09,  2.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 46/66 [00:20<00:08,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 51/66 [00:22<00:06,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 52/66 [00:22<00:06,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 53/66 [00:23<00:05,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 58/66 [00:25<00:03,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 59/66 [00:25<00:02,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 60/66 [00:26<00:02,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 65/66 [00:28<00:00,  2.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.33it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 02:29:19 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.888 | nll_loss 0.076 | accuracy 69.4 | tp 4 | tn 720 | fp 1 | fn 318 | false 319 | wps 424.6 | wpb 183.7 | bsz 15.8 | num_updates 1070 | best_accuracy 69.4\n",
            "2022-12-15 02:29:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 02:29:43 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint2.pt (epoch 2 @ 1070 updates, score 69.4) (writing took 23.88922271299998 seconds)\n",
            "2022-12-15 02:29:43 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-12-15 02:29:43 | INFO | train | epoch 002 | loss 0.879 | nll_loss 0.077 | accuracy 70.1 | tp 67 | tn 5929 | fp 94 | fn 2461 | false 2555 | wps 283 | ups 1.56 | wpb 181.7 | bsz 16 | num_updates 1070 | lr 7.99475e-07 | gnorm 24.664 | train_wall 289 | wall 696\n",
            "epoch 003:   0% 0/535 [00:00<?, ?it/s]2022-12-15 02:29:43 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 534/535 [04:50<00:00,  1.88it/s, loss=0.878, nll_loss=0.077, accuracy=68.3, tp=15, tn=1078, fp=16, fn=491, false=507, wps=336.9, ups=1.86, wpb=181.5, bsz=16, num_updates=1600, lr=7.0015e-07, gnorm=22.598, train_wall=54, wall=984]2022-12-15 02:34:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.10it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 3/66 [00:01<00:28,  2.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   6% 4/66 [00:01<00:27,  2.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 5/66 [00:02<00:26,  2.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  14% 9/66 [00:03<00:24,  2.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 16/66 [00:06<00:21,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 17/66 [00:07<00:20,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 19/66 [00:08<00:19,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 22/66 [00:09<00:18,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 23/66 [00:09<00:18,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 30/66 [00:12<00:15,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 31/66 [00:13<00:15,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 37/66 [00:15<00:12,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 38/66 [00:16<00:11,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 44/66 [00:18<00:09,  2.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 45/66 [00:19<00:08,  2.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  77% 51/66 [00:21<00:06,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 52/66 [00:22<00:05,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 58/66 [00:24<00:03,  2.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 59/66 [00:25<00:02,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 65/66 [00:27<00:00,  2.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.31it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 02:35:03 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.878 | nll_loss 0.076 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 428 | wpb 183.7 | bsz 15.8 | num_updates 1605 | best_accuracy 69.4\n",
            "2022-12-15 02:35:03 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 02:35:20 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint3.pt (epoch 3 @ 1605 updates, score 69.2) (writing took 16.96167242699994 seconds)\n",
            "2022-12-15 02:35:20 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-12-15 02:35:20 | INFO | train | epoch 003 | loss 0.867 | nll_loss 0.076 | accuracy 70.4 | tp 62 | tn 5960 | fp 63 | fn 2466 | false 2529 | wps 288.8 | ups 1.59 | wpb 181.7 | bsz 16 | num_updates 1605 | lr 6.99213e-07 | gnorm 22.034 | train_wall 290 | wall 1032\n",
            "epoch 004:   0% 0/535 [00:00<?, ?it/s]2022-12-15 02:35:20 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 534/535 [04:49<00:00,  1.86it/s, loss=0.88, nll_loss=0.077, accuracy=69.4, tp=10, tn=1100, fp=8, fn=482, false=490, wps=338.9, ups=1.86, wpb=182.3, bsz=16, num_updates=2100, lr=6.06447e-07, gnorm=21.107, train_wall=53, wall=1300]2022-12-15 02:40:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 3/66 [00:01<00:28,  2.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   6% 4/66 [00:01<00:27,  2.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 5/66 [00:02<00:26,  2.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.30it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  14% 9/66 [00:03<00:24,  2.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 12/66 [00:05<00:22,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 16/66 [00:06<00:21,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 17/66 [00:07<00:20,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  29% 19/66 [00:08<00:19,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 22/66 [00:09<00:18,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 23/66 [00:09<00:18,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 24/66 [00:10<00:17,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 30/66 [00:12<00:15,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 31/66 [00:13<00:14,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 37/66 [00:15<00:12,  2.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 38/66 [00:16<00:11,  2.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 44/66 [00:18<00:09,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 45/66 [00:19<00:09,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 51/66 [00:21<00:06,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 52/66 [00:22<00:05,  2.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 58/66 [00:24<00:03,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 59/66 [00:25<00:03,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98% 65/66 [00:27<00:00,  2.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 02:40:38 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.882 | nll_loss 0.076 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 428.3 | wpb 183.7 | bsz 15.8 | num_updates 2140 | best_accuracy 69.4\n",
            "2022-12-15 02:40:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 02:40:54 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint4.pt (epoch 4 @ 2140 updates, score 69.2) (writing took 15.766426707999926 seconds)\n",
            "2022-12-15 02:40:54 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-12-15 02:40:54 | INFO | train | epoch 004 | loss 0.866 | nll_loss 0.076 | accuracy 70.2 | tp 50 | tn 5957 | fp 66 | fn 2478 | false 2544 | wps 291.4 | ups 1.6 | wpb 181.7 | bsz 16 | num_updates 2140 | lr 5.98951e-07 | gnorm 20.422 | train_wall 288 | wall 1366\n",
            "epoch 005:   0% 0/535 [00:00<?, ?it/s]2022-12-15 02:40:54 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 534/535 [04:49<00:00,  1.86it/s, loss=0.874, nll_loss=0.076, accuracy=68.6, tp=15, tn=1082, fp=22, fn=481, false=503, wps=340.4, ups=1.85, wpb=183.7, bsz=16, num_updates=2600, lr=5.12744e-07, gnorm=21.878, train_wall=54, wall=1616]2022-12-15 02:45:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.79it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 3/66 [00:01<00:28,  2.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 4/66 [00:01<00:27,  2.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 5/66 [00:02<00:26,  2.30it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 6/66 [00:02<00:25,  2.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.30it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  14% 9/66 [00:03<00:24,  2.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 16/66 [00:06<00:21,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  26% 17/66 [00:07<00:20,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 19/66 [00:08<00:19,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 22/66 [00:09<00:18,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 23/66 [00:09<00:18,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 24/66 [00:10<00:17,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39% 26/66 [00:11<00:16,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 30/66 [00:12<00:15,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  47% 31/66 [00:13<00:14,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 37/66 [00:15<00:12,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 38/66 [00:16<00:11,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 44/66 [00:18<00:09,  2.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 45/66 [00:19<00:09,  2.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 48/66 [00:20<00:08,  2.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 51/66 [00:21<00:06,  2.30it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  79% 52/66 [00:22<00:06,  2.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.31it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88% 58/66 [00:24<00:03,  2.34it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 59/66 [00:25<00:02,  2.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.37it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.37it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.37it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  98% 65/66 [00:27<00:00,  2.37it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.37it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 02:46:12 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.891 | nll_loss 0.077 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 428.2 | wpb 183.7 | bsz 15.8 | num_updates 2675 | best_accuracy 69.4\n",
            "2022-12-15 02:46:12 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 02:46:28 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint5.pt (epoch 5 @ 2675 updates, score 69.1) (writing took 15.455631074999474 seconds)\n",
            "2022-12-15 02:46:28 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-12-15 02:46:28 | INFO | train | epoch 005 | loss 0.856 | nll_loss 0.075 | accuracy 70.5 | tp 70 | tn 5955 | fp 68 | fn 2458 | false 2526 | wps 290.9 | ups 1.6 | wpb 181.7 | bsz 16 | num_updates 2675 | lr 4.98688e-07 | gnorm 24.502 | train_wall 289 | wall 1700\n",
            "epoch 006:   0% 0/535 [00:00<?, ?it/s]2022-12-15 02:46:28 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 534/535 [04:49<00:00,  1.84it/s, loss=0.852, nll_loss=0.074, accuracy=70.5, tp=17, tn=1111, fp=25, fn=447, false=472, wps=342.3, ups=1.85, wpb=185, bsz=16, num_updates=3200, lr=4.003e-07, gnorm=19.468, train_wall=54, wall=1985]2022-12-15 02:51:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.76it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.07it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   5% 3/66 [00:01<00:28,  2.19it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 4/66 [00:01<00:27,  2.24it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 5/66 [00:02<00:26,  2.26it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 8/66 [00:03<00:24,  2.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  14% 9/66 [00:03<00:24,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  15% 10/66 [00:04<00:23,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.36it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 16/66 [00:06<00:21,  2.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  26% 17/66 [00:07<00:21,  2.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.30it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 22/66 [00:09<00:19,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 23/66 [00:10<00:18,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.30it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.30it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 30/66 [00:13<00:15,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47% 31/66 [00:13<00:15,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 37/66 [00:16<00:12,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  58% 38/66 [00:16<00:11,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 44/66 [00:19<00:09,  2.29it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 45/66 [00:19<00:09,  2.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 51/66 [00:22<00:06,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 52/66 [00:22<00:05,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 58/66 [00:25<00:03,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 59/66 [00:25<00:02,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.33it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 65/66 [00:28<00:00,  2.34it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 02:51:47 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.882 | nll_loss 0.076 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 426.7 | wpb 183.7 | bsz 15.8 | num_updates 3210 | best_accuracy 69.4\n",
            "2022-12-15 02:51:47 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 02:52:01 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint6.pt (epoch 6 @ 3210 updates, score 69.1) (writing took 14.250185636999959 seconds)\n",
            "2022-12-15 02:52:01 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-12-15 02:52:01 | INFO | train | epoch 006 | loss 0.858 | nll_loss 0.075 | accuracy 70.3 | tp 73 | tn 5937 | fp 86 | fn 2455 | false 2541 | wps 292 | ups 1.61 | wpb 181.7 | bsz 16 | num_updates 3210 | lr 3.98426e-07 | gnorm 22.973 | train_wall 289 | wall 2033\n",
            "epoch 007:   0% 0/535 [00:00<?, ?it/s]2022-12-15 02:52:01 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 534/535 [04:50<00:00,  1.87it/s, loss=0.865, nll_loss=0.075, accuracy=69.9, tp=13, tn=1105, fp=15, fn=467, false=482, wps=339.9, ups=1.84, wpb=184.4, bsz=16, num_updates=3700, lr=3.06597e-07, gnorm=22.155, train_wall=54, wall=2300]2022-12-15 02:56:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.80it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.09it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   5% 3/66 [00:01<00:28,  2.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   6% 4/66 [00:01<00:27,  2.25it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8% 5/66 [00:02<00:26,  2.29it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.29it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  14% 9/66 [00:03<00:24,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  23% 15/66 [00:06<00:22,  2.29it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24% 16/66 [00:07<00:21,  2.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  26% 17/66 [00:07<00:21,  2.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 22/66 [00:09<00:19,  2.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  35% 23/66 [00:10<00:18,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 30/66 [00:13<00:15,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  47% 31/66 [00:13<00:15,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 37/66 [00:16<00:12,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  58% 38/66 [00:16<00:12,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 44/66 [00:19<00:09,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  68% 45/66 [00:19<00:09,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.34it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.34it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  77% 51/66 [00:22<00:06,  2.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  79% 52/66 [00:22<00:06,  2.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.32it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.34it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.34it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 58/66 [00:25<00:03,  2.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 59/66 [00:25<00:02,  2.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.36it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.36it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  98% 65/66 [00:28<00:00,  2.36it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.36it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 02:57:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.876 | nll_loss 0.075 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 426.2 | wpb 183.7 | bsz 15.8 | num_updates 3745 | best_accuracy 69.4\n",
            "2022-12-15 02:57:21 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 02:57:38 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint7.pt (epoch 7 @ 3745 updates, score 69.1) (writing took 16.778344325000944 seconds)\n",
            "2022-12-15 02:57:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-12-15 02:57:38 | INFO | train | epoch 007 | loss 0.852 | nll_loss 0.075 | accuracy 70.5 | tp 96 | tn 5931 | fp 92 | fn 2432 | false 2524 | wps 288.9 | ups 1.59 | wpb 181.7 | bsz 16 | num_updates 3745 | lr 2.98163e-07 | gnorm 24.373 | train_wall 290 | wall 2370\n",
            "epoch 008:   0% 0/535 [00:00<?, ?it/s]2022-12-15 02:57:38 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 534/535 [04:51<00:00,  1.85it/s, loss=0.87, nll_loss=0.077, accuracy=70, tp=30, tn=1090, fp=25, fn=455, false=480, wps=335.8, ups=1.85, wpb=181.9, bsz=16, num_updates=4200, lr=2.12894e-07, gnorm=26.041, train_wall=54, wall=2618]2022-12-15 03:02:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   2% 1/66 [00:00<00:37,  1.74it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   3% 2/66 [00:00<00:31,  2.06it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   5% 3/66 [00:01<00:28,  2.19it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   6% 4/66 [00:01<00:28,  2.20it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8% 5/66 [00:02<00:27,  2.25it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.28it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.29it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.29it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  14% 9/66 [00:04<00:24,  2.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.30it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24% 16/66 [00:07<00:21,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  26% 17/66 [00:07<00:21,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 22/66 [00:09<00:19,  2.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  35% 23/66 [00:10<00:18,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  44% 29/66 [00:12<00:16,  2.29it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 30/66 [00:13<00:15,  2.29it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  47% 31/66 [00:13<00:15,  2.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 37/66 [00:16<00:12,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  58% 38/66 [00:16<00:12,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 44/66 [00:19<00:09,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  68% 45/66 [00:19<00:08,  2.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  77% 51/66 [00:22<00:06,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  79% 52/66 [00:22<00:06,  2.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88% 58/66 [00:25<00:03,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 59/66 [00:25<00:02,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.32it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  98% 65/66 [00:28<00:00,  2.34it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 03:02:58 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.887 | nll_loss 0.076 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 425.6 | wpb 183.7 | bsz 15.8 | num_updates 4280 | best_accuracy 69.4\n",
            "2022-12-15 03:02:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 03:03:15 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint8.pt (epoch 8 @ 4280 updates, score 69.2) (writing took 17.224875380999947 seconds)\n",
            "2022-12-15 03:03:15 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-12-15 03:03:15 | INFO | train | epoch 008 | loss 0.846 | nll_loss 0.074 | accuracy 70.4 | tp 105 | tn 5919 | fp 104 | fn 2423 | false 2527 | wps 288.1 | ups 1.59 | wpb 181.7 | bsz 16 | num_updates 4280 | lr 1.97901e-07 | gnorm 23.368 | train_wall 290 | wall 2707\n",
            "epoch 009:   0% 0/535 [00:00<?, ?it/s]2022-12-15 03:03:15 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 534/535 [04:50<00:00,  1.85it/s, loss=0.847, nll_loss=0.075, accuracy=69.2, tp=21, tn=1087, fp=32, fn=460, false=492, wps=334, ups=1.85, wpb=180.7, bsz=16, num_updates=4800, lr=1.0045e-07, gnorm=23.938, train_wall=54, wall=2990]2022-12-15 03:08:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   2% 1/66 [00:00<00:35,  1.83it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.10it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   5% 3/66 [00:01<00:28,  2.20it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   6% 4/66 [00:01<00:27,  2.26it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8% 5/66 [00:02<00:26,  2.27it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.29it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.31it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  14% 9/66 [00:03<00:24,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  24% 16/66 [00:06<00:21,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  26% 17/66 [00:07<00:21,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.31it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 22/66 [00:09<00:18,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  35% 23/66 [00:09<00:18,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45% 30/66 [00:12<00:15,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  47% 31/66 [00:13<00:14,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 37/66 [00:15<00:12,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  58% 38/66 [00:16<00:11,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 44/66 [00:18<00:09,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  68% 45/66 [00:19<00:08,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.31it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.28it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.30it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  77% 51/66 [00:21<00:06,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  79% 52/66 [00:22<00:06,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.30it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.31it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.31it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88% 58/66 [00:25<00:03,  2.31it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 59/66 [00:25<00:03,  2.29it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.30it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.31it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  98% 65/66 [00:28<00:00,  2.35it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.35it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 03:08:34 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.876 | nll_loss 0.075 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 426.3 | wpb 183.7 | bsz 15.8 | num_updates 4815 | best_accuracy 69.4\n",
            "2022-12-15 03:08:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 03:08:51 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint9.pt (epoch 9 @ 4815 updates, score 69.1) (writing took 17.01156907099903 seconds)\n",
            "2022-12-15 03:08:51 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-12-15 03:08:51 | INFO | train | epoch 009 | loss 0.842 | nll_loss 0.074 | accuracy 70.6 | tp 115 | tn 5920 | fp 103 | fn 2413 | false 2516 | wps 289.3 | ups 1.59 | wpb 181.7 | bsz 16 | num_updates 4815 | lr 9.76387e-08 | gnorm 24.004 | train_wall 289 | wall 3043\n",
            "epoch 010:   0% 0/535 [00:00<?, ?it/s]2022-12-15 03:08:51 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 534/535 [04:54<00:00,  1.84it/s, loss=0.831, nll_loss=0.073, accuracy=72, tp=27, tn=1125, fp=24, fn=424, false=448, wps=328, ups=1.81, wpb=181.2, bsz=16, num_updates=5300, lr=6.74663e-09, gnorm=20.766, train_wall=55, wall=3310]2022-12-15 03:13:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.79it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.09it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   5% 3/66 [00:01<00:29,  2.12it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   6% 4/66 [00:01<00:28,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 5/66 [00:02<00:27,  2.26it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.30it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  14% 9/66 [00:04<00:24,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 11/66 [00:04<00:24,  2.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.33it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.33it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  24% 16/66 [00:07<00:21,  2.34it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  26% 17/66 [00:07<00:21,  2.33it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.34it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.30it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  30% 20/66 [00:08<00:21,  2.15it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  32% 21/66 [00:09<00:21,  2.09it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 22/66 [00:09<00:20,  2.16it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  35% 23/66 [00:10<00:19,  2.20it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.22it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 25/66 [00:11<00:18,  2.25it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.25it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  41% 27/66 [00:11<00:17,  2.27it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  44% 29/66 [00:12<00:16,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 30/66 [00:13<00:15,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  47% 31/66 [00:13<00:15,  2.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  48% 32/66 [00:14<00:14,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.30it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  56% 37/66 [00:16<00:12,  2.28it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  58% 38/66 [00:16<00:12,  2.18it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  59% 39/66 [00:17<00:12,  2.23it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.26it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 41/66 [00:18<00:11,  2.27it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 44/66 [00:19<00:09,  2.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 45/66 [00:19<00:09,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  70% 46/66 [00:20<00:08,  2.33it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.33it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  73% 48/66 [00:21<00:07,  2.34it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.34it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.34it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  77% 51/66 [00:22<00:06,  2.33it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  79% 52/66 [00:22<00:06,  2.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  80% 53/66 [00:23<00:05,  2.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.30it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 55/66 [00:24<00:04,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.23it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  86% 57/66 [00:25<00:04,  2.11it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88% 58/66 [00:25<00:03,  2.16it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 59/66 [00:26<00:03,  2.21it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  91% 60/66 [00:26<00:02,  2.25it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.27it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  94% 62/66 [00:27<00:01,  2.28it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.30it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  97% 64/66 [00:28<00:00,  2.31it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  98% 65/66 [00:28<00:00,  2.32it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 66/66 [00:29<00:00,  2.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 03:14:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.889 | nll_loss 0.077 | accuracy 69.3 | tp 2 | tn 721 | fp 0 | fn 320 | false 320 | wps 418.1 | wpb 183.7 | bsz 15.8 | num_updates 5350 | best_accuracy 69.4\n",
            "2022-12-15 03:14:16 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 03:14:34 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint10.pt (epoch 10 @ 5350 updates, score 69.3) (writing took 18.905998394999187 seconds)\n",
            "2022-12-15 03:14:34 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-12-15 03:14:34 | INFO | train | epoch 010 | loss 0.846 | nll_loss 0.074 | accuracy 70.5 | tp 129 | tn 5899 | fp 124 | fn 2399 | false 2523 | wps 283.3 | ups 1.56 | wpb 181.7 | bsz 16 | num_updates 5350 | lr 0 | gnorm 26.125 | train_wall 293 | wall 3387\n",
            "epoch 011:   0% 0/535 [00:00<?, ?it/s]2022-12-15 03:14:34 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 534/535 [04:49<00:00,  1.86it/s, loss=0.845, nll_loss=0.074, accuracy=70.5, tp=29, tn=1099, fp=28, fn=444, false=472, wps=341.5, ups=1.86, wpb=183.4, bsz=16, num_updates=5800, lr=0, gnorm=27.075, train_wall=53, wall=3631]2022-12-15 03:19:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.76it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   3% 2/66 [00:01<00:31,  2.03it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 3/66 [00:01<00:29,  2.16it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   6% 4/66 [00:01<00:28,  2.21it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   8% 5/66 [00:02<00:26,  2.26it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.29it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.31it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  12% 8/66 [00:03<00:24,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  14% 9/66 [00:04<00:24,  2.32it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.32it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  24% 16/66 [00:06<00:21,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  26% 17/66 [00:07<00:20,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.27it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  30% 20/66 [00:08<00:20,  2.30it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.31it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 22/66 [00:09<00:18,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  35% 23/66 [00:10<00:18,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  36% 24/66 [00:10<00:17,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 30/66 [00:13<00:15,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  47% 31/66 [00:13<00:14,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 37/66 [00:15<00:12,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  58% 38/66 [00:16<00:11,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  61% 40/66 [00:17<00:10,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.37it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  65% 43/66 [00:18<00:09,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 44/66 [00:18<00:09,  2.31it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  68% 45/66 [00:19<00:09,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  77% 51/66 [00:21<00:06,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  79% 52/66 [00:22<00:06,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.32it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  88% 58/66 [00:24<00:03,  2.31it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 59/66 [00:25<00:03,  2.32it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.33it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.35it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  98% 65/66 [00:27<00:00,  2.36it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.37it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 03:19:53 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.888 | nll_loss 0.076 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 427.9 | wpb 183.7 | bsz 15.8 | num_updates 5885 | best_accuracy 69.4\n",
            "2022-12-15 03:19:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 03:20:10 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint11.pt (epoch 11 @ 5885 updates, score 69.1) (writing took 17.130288671000017 seconds)\n",
            "2022-12-15 03:20:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-12-15 03:20:10 | INFO | train | epoch 011 | loss 0.841 | nll_loss 0.074 | accuracy 70.5 | tp 146 | tn 5883 | fp 140 | fn 2382 | false 2522 | wps 290.1 | ups 1.6 | wpb 181.7 | bsz 16 | num_updates 5885 | lr 0 | gnorm 24.267 | train_wall 288 | wall 3722\n",
            "epoch 012:   0% 0/535 [00:00<?, ?it/s]2022-12-15 03:20:10 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 534/535 [04:50<00:00,  1.86it/s, loss=0.841, nll_loss=0.074, accuracy=69.9, tp=17, tn=1101, fp=29, fn=453, false=482, wps=337.9, ups=1.85, wpb=182.7, bsz=16, num_updates=6400, lr=0, gnorm=23.964, train_wall=54, wall=4002]2022-12-15 03:25:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   2% 1/66 [00:00<00:36,  1.78it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   3% 2/66 [00:00<00:30,  2.07it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   5% 3/66 [00:01<00:29,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   6% 4/66 [00:01<00:27,  2.23it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   8% 5/66 [00:02<00:27,  2.25it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 6/66 [00:02<00:26,  2.27it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  11% 7/66 [00:03<00:25,  2.29it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  12% 8/66 [00:03<00:25,  2.30it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  14% 9/66 [00:04<00:24,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  15% 10/66 [00:04<00:24,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  17% 11/66 [00:04<00:23,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  18% 12/66 [00:05<00:23,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  20% 13/66 [00:05<00:22,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  21% 14/66 [00:06<00:22,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  23% 15/66 [00:06<00:21,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  24% 16/66 [00:06<00:21,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  26% 17/66 [00:07<00:21,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  27% 18/66 [00:07<00:20,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  29% 19/66 [00:08<00:20,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  30% 20/66 [00:08<00:19,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  32% 21/66 [00:09<00:19,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 22/66 [00:09<00:19,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  35% 23/66 [00:10<00:18,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  36% 24/66 [00:10<00:18,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  38% 25/66 [00:10<00:17,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39% 26/66 [00:11<00:17,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  41% 27/66 [00:11<00:16,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  42% 28/66 [00:12<00:16,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  44% 29/66 [00:12<00:15,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45% 30/66 [00:13<00:15,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  47% 31/66 [00:13<00:15,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  48% 32/66 [00:13<00:14,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 33/66 [00:14<00:14,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  52% 34/66 [00:14<00:13,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  53% 35/66 [00:15<00:13,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  55% 36/66 [00:15<00:12,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  56% 37/66 [00:16<00:12,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  58% 38/66 [00:16<00:12,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  59% 39/66 [00:16<00:11,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  61% 40/66 [00:17<00:11,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  62% 41/66 [00:17<00:10,  2.29it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  64% 42/66 [00:18<00:10,  2.26it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  65% 43/66 [00:18<00:10,  2.27it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 44/66 [00:19<00:09,  2.30it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  68% 45/66 [00:19<00:09,  2.30it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  70% 46/66 [00:19<00:08,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  71% 47/66 [00:20<00:08,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  73% 48/66 [00:20<00:07,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  74% 49/66 [00:21<00:07,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  76% 50/66 [00:21<00:06,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  77% 51/66 [00:22<00:06,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  79% 52/66 [00:22<00:06,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  80% 53/66 [00:22<00:05,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 54/66 [00:23<00:05,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  83% 55/66 [00:23<00:04,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  85% 56/66 [00:24<00:04,  2.31it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  86% 57/66 [00:24<00:03,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  88% 58/66 [00:25<00:03,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 59/66 [00:25<00:03,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  91% 60/66 [00:25<00:02,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  92% 61/66 [00:26<00:02,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  94% 62/66 [00:26<00:01,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  95% 63/66 [00:27<00:01,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  97% 64/66 [00:27<00:00,  2.33it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  98% 65/66 [00:28<00:00,  2.34it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 66/66 [00:28<00:00,  2.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 03:25:29 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.884 | nll_loss 0.076 | accuracy 69.3 | tp 2 | tn 721 | fp 0 | fn 320 | false 320 | wps 425.4 | wpb 183.7 | bsz 15.8 | num_updates 6420 | best_accuracy 69.4\n",
            "2022-12-15 03:25:29 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 03:25:46 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-021721_ckpt/checkpoint12.pt (epoch 12 @ 6420 updates, score 69.3) (writing took 16.94584907100034 seconds)\n",
            "2022-12-15 03:25:46 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-12-15 03:25:46 | INFO | train | epoch 012 | loss 0.842 | nll_loss 0.074 | accuracy 70.5 | tp 143 | tn 5889 | fp 134 | fn 2385 | false 2519 | wps 289 | ups 1.59 | wpb 181.7 | bsz 16 | num_updates 6420 | lr 0 | gnorm 22.368 | train_wall 289 | wall 4058\n",
            "2022-12-15 03:25:46 | INFO | fairseq_cli.train | done training in 4047.0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_cola_quantall.pt',\n",
        "    data_name_or_path='CoLA-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "with open('glue_data/CoLA/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[3], tokens[1]\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count > 100:\n",
        "          break\n",
        "print('average inference time', (end-start)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uka495VB3nQm",
        "outputId": "f246fde8-2df6-44b5-b2f8-da135f87ec45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "0.48031091690063477\n",
            "| Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run.py --arch roberta_base --task CoLA --restore-file checkpoints/checkpoint_best_CoLA.pt --lr 1e-6 --force-dequant nonlinear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVXboG6PA15V",
        "outputId": "a190b883-46c0-4907-e1a4-896a12517ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid_subset: valid\n",
            "valid_interval_updates: None\n",
            "Finetuning from the checkpoint: checkpoints/checkpoint_best_CoLA.pt\n",
            "2022-12-15 04:23:22 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='CoLA-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, force_dequant='nonlinear', fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_file='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317.log', log_format=None, log_interval=100, lr=[1e-06], lr_scheduler='polynomial_decay', max_epoch=12, max_positions=512, max_sentences=16, max_sentences_valid=16, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=True, no_shuffle=False, nprocs_per_node=1, num_classes=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_mode='symmetric', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoints/checkpoint_best_CoLA.pt', save_dir='outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=5336, tpu=False, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.1)\n",
            "2022-12-15 04:23:22 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
            "2022-12-15 04:23:22 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2022-12-15 04:23:22 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/input0/valid\n",
            "2022-12-15 04:23:22 | INFO | fairseq.data.data_utils | loaded 1043 examples from: CoLA-bin/label/valid\n",
            "2022-12-15 04:23:22 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 1043\n",
            "2022-12-15 04:23:22 | INFO | fairseq.modules.transformer_sentence_encoder | Dropout 0.1, attn dropout 0.1, act dropout 0.0\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize gelu\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize softmax\n",
            "2022-12-15 04:23:23 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:24 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 04:23:24 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerSentenceEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): QuantEmbedding()\n",
            "      (embed_positions): QuantEmbedding()\n",
            "      (embed_positions_act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (1): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (2): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (3): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (4): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (5): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (6): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (7): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (8): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (9): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (10): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (11): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU(\n",
            "            (activation_fn): GELU(approximate='none')\n",
            "          )\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (emb_layer_norm): IntLayerNorm(\n",
            "        (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2022-12-15 04:23:24 | INFO | fairseq_cli.train | task: sentence_prediction (SentencePredictionTask)\n",
            "2022-12-15 04:23:24 | INFO | fairseq_cli.train | model: roberta_base (RobertaModel)\n",
            "2022-12-15 04:23:24 | INFO | fairseq_cli.train | criterion: sentence_prediction (SentencePredictionCriterion)\n",
            "2022-12-15 04:23:24 | INFO | fairseq_cli.train | num. model params: 125288795 (num. trained: 125288795)\n",
            "2022-12-15 04:23:24 | INFO | fairseq_cli.train | quantize: symmetric\n",
            "2022-12-15 04:23:26 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2022-12-15 04:23:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-15 04:23:26 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          \n",
            "2022-12-15 04:23:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-15 04:23:26 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-12-15 04:23:26 | INFO | fairseq_cli.train | max tokens per GPU = 4400 and max sentences per GPU = 16\n",
            "dddddddddd\n",
            "None\n",
            "dddddddddd\n",
            "None\n",
            "2022-12-15 04:23:36 | INFO | fairseq.trainer | loaded checkpoint checkpoints/checkpoint_best_CoLA.pt (epoch 10 @ 0 updates)\n",
            "2022-12-15 04:23:36 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n",
            "2022-12-15 04:23:36 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-12-15 04:23:36 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/input0/train\n",
            "2022-12-15 04:23:36 | INFO | fairseq.data.data_utils | loaded 8551 examples from: CoLA-bin/label/train\n",
            "2022-12-15 04:23:36 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 8551\n",
            "epoch 001:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:23:37 | INFO | fairseq.trainer | begin training epoch 1\n",
            "/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT/fairseq/utils.py:305: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 534/535 [03:13<00:00,  2.82it/s, loss=0.906, nll_loss=0.08, accuracy=68.1, tp=28, tn=1056, fp=39, fn=468, false=507, wps=508.9, ups=2.82, wpb=180.5, bsz=15.9, num_updates=500, lr=9.06297e-07, gnorm=23.437, train_wall=35, wall=192]2022-12-15 04:26:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/66 [00:00<00:23,  2.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 5/66 [00:01<00:15,  3.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 8/66 [00:02<00:14,  3.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 11/66 [00:02<00:14,  3.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 19/66 [00:04<00:12,  3.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 23/66 [00:05<00:11,  3.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  41% 27/66 [00:07<00:09,  3.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 31/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 35/66 [00:09<00:07,  3.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:27:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.897 | nll_loss 0.077 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 719.7 | wpb 183.7 | bsz 15.8 | num_updates 535\n",
            "2022-12-15 04:27:07 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:27:33 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint1.pt (epoch 1 @ 535 updates, score 69.1) (writing took 25.383633722001832 seconds)\n",
            "2022-12-15 04:27:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-12-15 04:27:33 | INFO | train | epoch 001 | loss 0.933 | nll_loss 0.082 | accuracy 67.6 | tp 252 | tn 5528 | fp 495 | fn 2276 | false 2771 | wps 416.6 | ups 2.29 | wpb 181.7 | bsz 16 | num_updates 535 | lr 8.99738e-07 | gnorm 34.454 | train_wall 192 | wall 247\n",
            "epoch 002:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:27:33 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002: 100% 534/535 [03:12<00:00,  2.80it/s, loss=0.856, nll_loss=0.076, accuracy=71, tp=10, tn=1126, fp=10, fn=454, false=464, wps=508.9, ups=2.81, wpb=181, bsz=16, num_updates=1000, lr=8.12594e-07, gnorm=19.154, train_wall=35, wall=414]2022-12-15 04:30:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 1/66 [00:00<00:24,  2.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 5/66 [00:01<00:16,  3.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 8/66 [00:02<00:14,  3.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 11/66 [00:02<00:13,  3.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 19/66 [00:04<00:12,  3.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 23/66 [00:05<00:10,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 27/66 [00:06<00:09,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 31/66 [00:07<00:08,  3.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 35/66 [00:08<00:07,  3.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:31:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.874 | nll_loss 0.075 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 722.7 | wpb 183.7 | bsz 15.8 | num_updates 1070 | best_accuracy 69.2\n",
            "2022-12-15 04:31:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:31:26 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint2.pt (epoch 2 @ 1070 updates, score 69.2) (writing took 24.47784992200104 seconds)\n",
            "2022-12-15 04:31:26 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-12-15 04:31:26 | INFO | train | epoch 002 | loss 0.872 | nll_loss 0.077 | accuracy 69.8 | tp 60 | tn 5911 | fp 112 | fn 2468 | false 2580 | wps 415.8 | ups 2.29 | wpb 181.7 | bsz 16 | num_updates 1070 | lr 7.99475e-07 | gnorm 25.744 | train_wall 191 | wall 480\n",
            "epoch 003:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:31:26 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 534/535 [03:12<00:00,  2.83it/s, loss=0.881, nll_loss=0.078, accuracy=67.8, tp=12, tn=1073, fp=21, fn=494, false=515, wps=509.7, ups=2.81, wpb=181.5, bsz=16, num_updates=1600, lr=7.0015e-07, gnorm=19.763, train_wall=35, wall=672]2022-12-15 04:34:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 1/66 [00:00<00:24,  2.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 3/66 [00:00<00:18,  3.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   6% 4/66 [00:01<00:17,  3.65it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 5/66 [00:01<00:16,  3.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.77it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  12% 8/66 [00:02<00:15,  3.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 11/66 [00:02<00:14,  3.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 19/66 [00:04<00:11,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 23/66 [00:05<00:10,  3.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  41% 27/66 [00:07<00:09,  3.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  47% 31/66 [00:08<00:08,  3.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 35/66 [00:09<00:07,  3.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:34:57 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.875 | nll_loss 0.075 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 721.2 | wpb 183.7 | bsz 15.8 | num_updates 1605 | best_accuracy 69.2\n",
            "2022-12-15 04:34:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:35:10 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint3.pt (epoch 3 @ 1605 updates, score 69.1) (writing took 13.798933489000774 seconds)\n",
            "2022-12-15 04:35:10 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-12-15 04:35:10 | INFO | train | epoch 003 | loss 0.863 | nll_loss 0.076 | accuracy 70.1 | tp 65 | tn 5927 | fp 96 | fn 2463 | false 2559 | wps 434.3 | ups 2.39 | wpb 181.7 | bsz 16 | num_updates 1605 | lr 6.99213e-07 | gnorm 22.092 | train_wall 191 | wall 704\n",
            "epoch 004:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:35:10 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 534/535 [03:12<00:00,  2.80it/s, loss=0.872, nll_loss=0.077, accuracy=69.8, tp=20, tn=1096, fp=12, fn=472, false=484, wps=511.1, ups=2.8, wpb=182.3, bsz=16, num_updates=2100, lr=6.06447e-07, gnorm=23.043, train_wall=35, wall=883]2022-12-15 04:38:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 1/66 [00:00<00:24,  2.66it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.72it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 5/66 [00:01<00:15,  3.81it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.90it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  12% 8/66 [00:02<00:14,  3.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 11/66 [00:02<00:13,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  29% 19/66 [00:04<00:11,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 23/66 [00:05<00:10,  3.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  41% 27/66 [00:06<00:09,  3.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  47% 31/66 [00:07<00:08,  3.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 35/66 [00:08<00:07,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  59% 39/66 [00:09<00:06,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 43/66 [00:10<00:05,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.90it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.90it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.90it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:38:40 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.906 | nll_loss 0.078 | accuracy 69.1 | tp 0 | tn 721 | fp 0 | fn 322 | false 322 | wps 719.3 | wpb 183.7 | bsz 15.8 | num_updates 2140 | best_accuracy 69.2\n",
            "2022-12-15 04:38:40 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:38:55 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint4.pt (epoch 4 @ 2140 updates, score 69.1) (writing took 15.190239302999544 seconds)\n",
            "2022-12-15 04:38:55 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-12-15 04:38:55 | INFO | train | epoch 004 | loss 0.855 | nll_loss 0.075 | accuracy 70.6 | tp 102 | tn 5938 | fp 85 | fn 2426 | false 2511 | wps 433.1 | ups 2.38 | wpb 181.7 | bsz 16 | num_updates 2140 | lr 5.98951e-07 | gnorm 22.387 | train_wall 191 | wall 929\n",
            "epoch 005:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:38:55 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 534/535 [03:12<00:00,  2.84it/s, loss=0.853, nll_loss=0.074, accuracy=69.2, tp=41, tn=1066, fp=38, fn=455, false=493, wps=512.5, ups=2.79, wpb=183.7, bsz=16, num_updates=2600, lr=5.12744e-07, gnorm=25.411, train_wall=35, wall=1095]2022-12-15 04:42:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   2% 1/66 [00:00<00:24,  2.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 5/66 [00:01<00:16,  3.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  12% 8/66 [00:02<00:15,  3.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 11/66 [00:02<00:14,  3.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 15/66 [00:03<00:13,  3.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 19/66 [00:04<00:12,  3.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.85it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 23/66 [00:06<00:11,  3.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  41% 27/66 [00:07<00:09,  3.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  47% 31/66 [00:08<00:09,  3.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  53% 35/66 [00:09<00:07,  3.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 39/66 [00:10<00:07,  3.85it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.85it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.81it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.85it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.89it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.85it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.79it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.83it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  94% 62/66 [00:16<00:01,  3.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 66/66 [00:17<00:00,  3.95it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:42:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.908 | nll_loss 0.078 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 713.7 | wpb 183.7 | bsz 15.8 | num_updates 2675 | best_accuracy 69.2\n",
            "2022-12-15 04:42:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:42:47 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint5.pt (epoch 5 @ 2675 updates, score 69.2) (writing took 21.49733179100076 seconds)\n",
            "2022-12-15 04:42:47 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-12-15 04:42:47 | INFO | train | epoch 005 | loss 0.843 | nll_loss 0.074 | accuracy 70.4 | tp 144 | tn 5877 | fp 146 | fn 2384 | false 2530 | wps 419.7 | ups 2.31 | wpb 181.7 | bsz 16 | num_updates 2675 | lr 4.98688e-07 | gnorm 25.787 | train_wall 191 | wall 1161\n",
            "epoch 006:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:42:47 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 534/535 [03:13<00:00,  2.84it/s, loss=0.83, nll_loss=0.072, accuracy=70.6, tp=30, tn=1099, fp=37, fn=434, false=471, wps=518.3, ups=2.8, wpb=185, bsz=16, num_updates=3200, lr=4.003e-07, gnorm=23.807, train_wall=35, wall=1351]2022-12-15 04:46:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   2% 1/66 [00:00<00:24,  2.67it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.30it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.71it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 5/66 [00:01<00:17,  3.51it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 6/66 [00:01<00:16,  3.64it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  12% 8/66 [00:02<00:15,  3.81it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.87it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 11/66 [00:02<00:14,  3.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 19/66 [00:04<00:11,  3.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.87it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 23/66 [00:06<00:10,  3.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  41% 27/66 [00:07<00:09,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  47% 31/66 [00:08<00:08,  3.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53% 35/66 [00:09<00:07,  3.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.98it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.96it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.84it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.77it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.82it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.85it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.88it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.88it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.95it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.94it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.93it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:46:17 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.909 | nll_loss 0.078 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 716.8 | wpb 183.7 | bsz 15.8 | num_updates 3210 | best_accuracy 69.2\n",
            "2022-12-15 04:46:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:46:40 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint6.pt (epoch 6 @ 3210 updates, score 69.2) (writing took 22.82646295700033 seconds)\n",
            "2022-12-15 04:46:40 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-12-15 04:46:40 | INFO | train | epoch 006 | loss 0.843 | nll_loss 0.074 | accuracy 70.1 | tp 135 | tn 5859 | fp 164 | fn 2393 | false 2557 | wps 416.8 | ups 2.29 | wpb 181.7 | bsz 16 | num_updates 3210 | lr 3.98426e-07 | gnorm 24.358 | train_wall 192 | wall 1394\n",
            "epoch 007:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:46:40 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 534/535 [03:12<00:00,  2.83it/s, loss=0.851, nll_loss=0.074, accuracy=70.8, tp=42, tn=1091, fp=29, fn=438, false=467, wps=518.1, ups=2.81, wpb=184.4, bsz=16, num_updates=3700, lr=3.06597e-07, gnorm=23.248, train_wall=35, wall=1571]2022-12-15 04:49:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   2% 1/66 [00:00<00:25,  2.59it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.27it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.53it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.69it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8% 5/66 [00:01<00:16,  3.76it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.82it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.82it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  12% 8/66 [00:02<00:15,  3.86it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.90it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 11/66 [00:02<00:14,  3.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.98it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  29% 19/66 [00:04<00:11,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  35% 23/66 [00:05<00:10,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  41% 27/66 [00:06<00:09,  3.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  47% 31/66 [00:07<00:08,  3.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  53% 35/66 [00:09<00:07,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.97it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.99it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.99it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.91it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.85it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.82it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.86it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.89it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.92it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:50:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.9 | nll_loss 0.077 | accuracy 69.1 | tp 2 | tn 719 | fp 2 | fn 320 | false 322 | wps 720.9 | wpb 183.7 | bsz 15.8 | num_updates 3745 | best_accuracy 69.2\n",
            "2022-12-15 04:50:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:50:23 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint7.pt (epoch 7 @ 3745 updates, score 69.1) (writing took 13.592633831998683 seconds)\n",
            "2022-12-15 04:50:23 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-12-15 04:50:23 | INFO | train | epoch 007 | loss 0.836 | nll_loss 0.074 | accuracy 70.6 | tp 204 | tn 5834 | fp 189 | fn 2324 | false 2513 | wps 434.8 | ups 2.39 | wpb 181.7 | bsz 16 | num_updates 3745 | lr 2.98163e-07 | gnorm 24.343 | train_wall 191 | wall 1617\n",
            "epoch 008:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:50:23 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 534/535 [03:13<00:00,  2.79it/s, loss=0.852, nll_loss=0.075, accuracy=69.5, tp=51, tn=1061, fp=54, fn=434, false=488, wps=509.1, ups=2.8, wpb=181.9, bsz=16, num_updates=4200, lr=2.12894e-07, gnorm=34.6, train_wall=35, wall=1783]2022-12-15 04:53:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   2% 1/66 [00:00<00:23,  2.76it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.33it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.73it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8% 5/66 [00:01<00:15,  3.83it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.86it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.89it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  12% 8/66 [00:02<00:14,  3.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 11/66 [00:02<00:13,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  29% 19/66 [00:04<00:11,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  35% 23/66 [00:05<00:10,  3.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  41% 27/66 [00:06<00:09,  3.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.90it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.90it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.89it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  47% 31/66 [00:07<00:08,  3.91it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.90it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.81it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.81it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  53% 35/66 [00:09<00:08,  3.84it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.82it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.84it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.88it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.89it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.91it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.92it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.96it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.98it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.98it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.97it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.95it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:53:54 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.908 | nll_loss 0.078 | accuracy 69.5 | tp 4 | tn 721 | fp 0 | fn 318 | false 318 | wps 720.2 | wpb 183.7 | bsz 15.8 | num_updates 4280 | best_accuracy 69.5\n",
            "2022-12-15 04:53:54 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:54:24 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint8.pt (epoch 8 @ 4280 updates, score 69.5) (writing took 29.298986153000442 seconds)\n",
            "2022-12-15 04:54:24 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-12-15 04:54:24 | INFO | train | epoch 008 | loss 0.832 | nll_loss 0.073 | accuracy 70.3 | tp 221 | tn 5793 | fp 230 | fn 2307 | false 2537 | wps 404.9 | ups 2.23 | wpb 181.7 | bsz 16 | num_updates 4280 | lr 1.97901e-07 | gnorm 25.825 | train_wall 192 | wall 1858\n",
            "epoch 009:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:54:24 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 534/535 [03:14<00:00,  2.75it/s, loss=0.843, nll_loss=0.075, accuracy=69.4, tp=43, tn=1067, fp=52, fn=438, false=490, wps=504.7, ups=2.79, wpb=180.7, bsz=16, num_updates=4800, lr=1.0045e-07, gnorm=25.916, train_wall=35, wall=2047]2022-12-15 04:57:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   2% 1/66 [00:00<00:24,  2.68it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.71it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8% 5/66 [00:01<00:16,  3.78it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.84it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.87it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  12% 8/66 [00:02<00:15,  3.86it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.87it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.89it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 11/66 [00:02<00:14,  3.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  23% 15/66 [00:03<00:13,  3.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  29% 19/66 [00:04<00:11,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  35% 23/66 [00:05<00:10,  3.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  41% 27/66 [00:06<00:09,  3.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  47% 31/66 [00:08<00:09,  3.85it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.87it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.87it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  53% 35/66 [00:09<00:08,  3.82it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.85it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.86it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.96it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.96it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.86it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.91it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.85it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.79it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.84it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.83it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.84it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  94% 62/66 [00:16<00:01,  3.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.94it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.96it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 66/66 [00:17<00:00,  3.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 04:57:56 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.894 | nll_loss 0.077 | accuracy 69.2 | tp 1 | tn 721 | fp 0 | fn 321 | false 321 | wps 715.2 | wpb 183.7 | bsz 15.8 | num_updates 4815 | best_accuracy 69.5\n",
            "2022-12-15 04:57:56 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 04:58:13 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint9.pt (epoch 9 @ 4815 updates, score 69.2) (writing took 16.986505878998287 seconds)\n",
            "2022-12-15 04:58:13 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-12-15 04:58:13 | INFO | train | epoch 009 | loss 0.83 | nll_loss 0.073 | accuracy 70.7 | tp 244 | tn 5799 | fp 224 | fn 2284 | false 2508 | wps 424.3 | ups 2.33 | wpb 181.7 | bsz 16 | num_updates 4815 | lr 9.76387e-08 | gnorm 24.279 | train_wall 193 | wall 2087\n",
            "epoch 010:   0% 0/535 [00:00<?, ?it/s]2022-12-15 04:58:13 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 534/535 [03:14<00:00,  2.84it/s, loss=0.818, nll_loss=0.072, accuracy=71.7, tp=55, tn=1092, fp=57, fn=396, false=453, wps=480, ups=2.65, wpb=181.2, bsz=16, num_updates=5300, lr=6.74663e-09, gnorm=23.298, train_wall=37, wall=2264]2022-12-15 05:01:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   2% 1/66 [00:00<00:24,  2.64it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.29it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.57it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.70it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 5/66 [00:01<00:16,  3.78it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.84it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.88it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  12% 8/66 [00:02<00:14,  3.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 11/66 [00:02<00:13,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  29% 19/66 [00:04<00:11,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  35% 23/66 [00:05<00:10,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  41% 27/66 [00:06<00:09,  3.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  47% 31/66 [00:07<00:08,  3.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  53% 35/66 [00:08<00:07,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  59% 39/66 [00:09<00:06,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.91it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.92it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.94it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 05:01:44 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.897 | nll_loss 0.077 | accuracy 69.4 | tp 3 | tn 721 | fp 0 | fn 319 | false 319 | wps 722.9 | wpb 183.7 | bsz 15.8 | num_updates 5350 | best_accuracy 69.5\n",
            "2022-12-15 05:01:44 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 05:01:58 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint10.pt (epoch 10 @ 5350 updates, score 69.4) (writing took 14.074389790999703 seconds)\n",
            "2022-12-15 05:01:58 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-12-15 05:01:58 | INFO | train | epoch 010 | loss 0.824 | nll_loss 0.072 | accuracy 70.8 | tp 257 | tn 5793 | fp 230 | fn 2271 | false 2501 | wps 430.8 | ups 2.37 | wpb 181.7 | bsz 16 | num_updates 5350 | lr 0 | gnorm 25.053 | train_wall 193 | wall 2312\n",
            "epoch 011:   0% 0/535 [00:00<?, ?it/s]2022-12-15 05:01:58 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 534/535 [03:14<00:00,  2.84it/s, loss=0.821, nll_loss=0.072, accuracy=71.1, tp=51, tn=1087, fp=40, fn=422, false=462, wps=510.8, ups=2.79, wpb=183.4, bsz=16, num_updates=5800, lr=0, gnorm=24.645, train_wall=36, wall=2477]2022-12-15 05:05:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   2% 1/66 [00:00<00:25,  2.57it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   3% 2/66 [00:00<00:20,  3.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 3/66 [00:00<00:18,  3.46it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   6% 4/66 [00:01<00:17,  3.64it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   8% 5/66 [00:01<00:16,  3.73it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.80it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.84it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  12% 8/66 [00:02<00:15,  3.86it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.89it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.80it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  17% 11/66 [00:02<00:14,  3.82it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  18% 12/66 [00:03<00:14,  3.85it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.87it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.89it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  23% 15/66 [00:03<00:13,  3.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  29% 19/66 [00:05<00:11,  3.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  35% 23/66 [00:06<00:10,  3.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  41% 27/66 [00:07<00:09,  3.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  42% 28/66 [00:07<00:09,  3.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  47% 31/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.82it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  53% 35/66 [00:09<00:08,  3.87it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.89it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.98it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.98it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.92it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 05:05:30 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.897 | nll_loss 0.077 | accuracy 69.5 | tp 4 | tn 721 | fp 0 | fn 318 | false 318 | wps 718.2 | wpb 183.7 | bsz 15.8 | num_updates 5885 | best_accuracy 69.5\n",
            "2022-12-15 05:05:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 05:05:55 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint11.pt (epoch 11 @ 5885 updates, score 69.5) (writing took 24.815805232003186 seconds)\n",
            "2022-12-15 05:05:55 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-12-15 05:05:55 | INFO | train | epoch 011 | loss 0.817 | nll_loss 0.072 | accuracy 71 | tp 301 | tn 5774 | fp 249 | fn 2227 | false 2476 | wps 411.2 | ups 2.26 | wpb 181.7 | bsz 16 | num_updates 5885 | lr 0 | gnorm 24.871 | train_wall 193 | wall 2549\n",
            "epoch 012:   0% 0/535 [00:00<?, ?it/s]2022-12-15 05:05:55 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 534/535 [03:12<00:00,  2.84it/s, loss=0.825, nll_loss=0.072, accuracy=71.5, tp=55, tn=1089, fp=41, fn=415, false=456, wps=500.5, ups=2.74, wpb=182.7, bsz=16, num_updates=6400, lr=0, gnorm=24.878, train_wall=36, wall=2735]2022-12-15 05:09:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/66 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   2% 1/66 [00:00<00:24,  2.64it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   3% 2/66 [00:00<00:19,  3.30it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   5% 3/66 [00:00<00:17,  3.57it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   6% 4/66 [00:01<00:16,  3.73it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   8% 5/66 [00:01<00:16,  3.81it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 6/66 [00:01<00:15,  3.87it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  11% 7/66 [00:01<00:15,  3.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  12% 8/66 [00:02<00:14,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  14% 9/66 [00:02<00:14,  3.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  15% 10/66 [00:02<00:14,  3.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  17% 11/66 [00:02<00:13,  3.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  18% 12/66 [00:03<00:13,  3.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  20% 13/66 [00:03<00:13,  3.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  21% 14/66 [00:03<00:13,  3.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  23% 15/66 [00:03<00:12,  3.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  24% 16/66 [00:04<00:12,  3.97it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  26% 17/66 [00:04<00:12,  3.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  27% 18/66 [00:04<00:12,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  29% 19/66 [00:04<00:11,  3.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  30% 20/66 [00:05<00:11,  3.88it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  32% 21/66 [00:05<00:11,  3.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 22/66 [00:05<00:11,  3.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  35% 23/66 [00:05<00:10,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  36% 24/66 [00:06<00:10,  3.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  38% 25/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39% 26/66 [00:06<00:10,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  41% 27/66 [00:06<00:09,  3.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  42% 28/66 [00:07<00:10,  3.78it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  44% 29/66 [00:07<00:09,  3.82it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45% 30/66 [00:07<00:09,  3.85it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  47% 31/66 [00:08<00:09,  3.86it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  48% 32/66 [00:08<00:08,  3.86it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 33/66 [00:08<00:08,  3.87it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  52% 34/66 [00:08<00:08,  3.89it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  53% 35/66 [00:09<00:07,  3.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  55% 36/66 [00:09<00:07,  3.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  56% 37/66 [00:09<00:07,  3.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  58% 38/66 [00:09<00:07,  3.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  59% 39/66 [00:10<00:06,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  61% 40/66 [00:10<00:06,  3.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  62% 41/66 [00:10<00:06,  3.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  64% 42/66 [00:10<00:06,  3.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  65% 43/66 [00:11<00:05,  3.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 44/66 [00:11<00:05,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  68% 45/66 [00:11<00:05,  3.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  70% 46/66 [00:11<00:05,  3.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  71% 47/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  73% 48/66 [00:12<00:04,  3.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  74% 49/66 [00:12<00:04,  3.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  76% 50/66 [00:12<00:04,  3.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  77% 51/66 [00:13<00:03,  3.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  79% 52/66 [00:13<00:03,  3.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  80% 53/66 [00:13<00:03,  3.89it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 54/66 [00:13<00:03,  3.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  83% 55/66 [00:14<00:02,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  85% 56/66 [00:14<00:02,  3.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  86% 57/66 [00:14<00:02,  3.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  88% 58/66 [00:14<00:02,  3.96it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 59/66 [00:15<00:01,  3.88it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  91% 60/66 [00:15<00:01,  3.88it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  92% 61/66 [00:15<00:01,  3.90it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  94% 62/66 [00:15<00:01,  3.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  95% 63/66 [00:16<00:00,  3.94it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  97% 64/66 [00:16<00:00,  3.91it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  98% 65/66 [00:16<00:00,  3.93it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 66/66 [00:16<00:00,  3.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 05:09:25 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.914 | nll_loss 0.079 | accuracy 69.5 | tp 4 | tn 721 | fp 0 | fn 318 | false 318 | wps 718.7 | wpb 183.7 | bsz 15.8 | num_updates 6420 | best_accuracy 69.5\n",
            "2022-12-15 05:09:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 05:09:49 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/CoLA-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-042317_ckpt/checkpoint12.pt (epoch 12 @ 6420 updates, score 69.5) (writing took 24.499690122000175 seconds)\n",
            "2022-12-15 05:09:49 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-12-15 05:09:49 | INFO | train | epoch 012 | loss 0.825 | nll_loss 0.073 | accuracy 70.7 | tp 263 | tn 5783 | fp 240 | fn 2265 | false 2505 | wps 414.8 | ups 2.28 | wpb 181.7 | bsz 16 | num_updates 6420 | lr 0 | gnorm 25.479 | train_wall 191 | wall 2783\n",
            "2022-12-15 05:09:49 | INFO | fairseq_cli.train | done training in 2773.0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python run.py --arch roberta_base --task SST-2 --restore-file checkpoints/checkpoint_best_SST-2.pt --lr 1e-6 --force-dequant layernorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKbaCbEYlwF-",
        "outputId": "d9dfa42f-0a7f-4769-d824-32f5e072a076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid_subset: valid\n",
            "valid_interval_updates: None\n",
            "Finetuning from the checkpoint: checkpoints/checkpoint_best_SST-2.pt\n",
            "2022-12-15 17:08:54 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='SST-2-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, force_dequant='layernorm', fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_file='outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852.log', log_format=None, log_interval=100, lr=[1e-06], lr_scheduler='polynomial_decay', max_epoch=12, max_positions=512, max_sentences=32, max_sentences_valid=32, max_tokens=4400, max_tokens_valid=4400, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=True, no_shuffle=False, nprocs_per_node=1, num_classes=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_mode='symmetric', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoints/checkpoint_best_SST-2.pt', save_dir='outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=20935, tpu=False, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.1)\n",
            "2022-12-15 17:08:54 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
            "2022-12-15 17:08:54 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2022-12-15 17:08:54 | INFO | fairseq.data.data_utils | loaded 872 examples from: SST-2-bin/input0/valid\n",
            "2022-12-15 17:08:54 | INFO | fairseq.data.data_utils | loaded 872 examples from: SST-2-bin/label/valid\n",
            "2022-12-15 17:08:54 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 872\n",
            "2022-12-15 17:08:54 | INFO | fairseq.modules.transformer_sentence_encoder | Dropout 0.1, attn dropout 0.1, act dropout 0.0\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:55 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:56 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:56 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:56 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:56 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:56 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:56 | INFO | fairseq.quantization.utils.quant_modules | Force dequantize layernorm\n",
            "2022-12-15 17:08:56 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerSentenceEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): QuantEmbedding()\n",
            "      (embed_positions): QuantEmbedding()\n",
            "      (embed_positions_act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (1): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (2): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (3): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (4): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (5): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (6): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (7): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (8): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (9): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (10): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "        (11): TransformerSentenceEncoderLayer(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (activation_fn_approx): IntGELU()\n",
            "          (input_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (v_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (q_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "            (k_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (v_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (q_proj_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (softmax): IntSoftmax(\n",
            "              (act): QuantAct(activation_bit=16, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            )\n",
            "            (attn_probs_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (attn_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "            (out_proj): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          )\n",
            "          (pre_self_attn_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (self_attn_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "          (fc1_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc2_act): QuantAct(activation_bit=8, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (fc1): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (fc2): (QuantLinear() weight_bit=8, quant_mode=symmetric)\n",
            "          (pre_final_layer_norm_act): QuantAct(activation_bit=22, quant_mode: symmetric, Act_min: 0.00, Act_max: 0.00)\n",
            "          (final_layer_norm): IntLayerNorm(\n",
            "            (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (emb_layer_norm): IntLayerNorm(\n",
            "        (activation): QuantAct(activation_bit=32, quant_mode: none, Act_min: 0.00, Act_max: 0.00)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2022-12-15 17:08:56 | INFO | fairseq_cli.train | task: sentence_prediction (SentencePredictionTask)\n",
            "2022-12-15 17:08:56 | INFO | fairseq_cli.train | model: roberta_base (RobertaModel)\n",
            "2022-12-15 17:08:56 | INFO | fairseq_cli.train | criterion: sentence_prediction (SentencePredictionCriterion)\n",
            "2022-12-15 17:08:56 | INFO | fairseq_cli.train | num. model params: 125288795 (num. trained: 125288795)\n",
            "2022-12-15 17:08:56 | INFO | fairseq_cli.train | quantize: symmetric\n",
            "2022-12-15 17:08:58 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2022-12-15 17:08:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-15 17:08:58 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          \n",
            "2022-12-15 17:08:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-12-15 17:08:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-12-15 17:08:58 | INFO | fairseq_cli.train | max tokens per GPU = 4400 and max sentences per GPU = 32\n",
            "dddddddddd\n",
            "None\n",
            "dddddddddd\n",
            "None\n",
            "2022-12-15 17:09:00 | INFO | fairseq.trainer | loaded checkpoint checkpoints/checkpoint_best_SST-2.pt (epoch 13 @ 0 updates)\n",
            "2022-12-15 17:09:00 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n",
            "2022-12-15 17:09:00 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-12-15 17:09:00 | INFO | fairseq.data.data_utils | loaded 67349 examples from: SST-2-bin/input0/train\n",
            "2022-12-15 17:09:00 | INFO | fairseq.data.data_utils | loaded 67349 examples from: SST-2-bin/label/train\n",
            "2022-12-15 17:09:00 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 67349\n",
            "epoch 001:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 17:09:01 | INFO | fairseq.trainer | begin training epoch 1\n",
            "/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT/fairseq/utils.py:305: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 2104/2105 [17:33<00:00,  2.00it/s, loss=0.37, nll_loss=0.028, accuracy=89.5, tp=1261, tn=1602, fp=161, fn=176, false=337, wps=849.6, ups=1.99, wpb=426.9, bsz=32, num_updates=2100, lr=8.9969e-07, gnorm=23.473, train_wall=50, wall=1054]2022-12-15 17:26:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  2.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 15/28 [00:05<00:05,  2.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 20/28 [00:07<00:03,  2.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.63it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 17:26:46 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.35 | nll_loss 0.014 | accuracy 91.3 | tp 382 | tn 414 | fp 30 | fn 46 | false 76 | wps 1994.8 | wpb 778.9 | bsz 31.1 | num_updates 2105\n",
            "2022-12-15 17:26:46 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 17:27:05 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint1.pt (epoch 1 @ 2105 updates, score 91.3) (writing took 19.68497269799991 seconds)\n",
            "2022-12-15 17:27:05 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-12-15 17:27:05 | INFO | train | epoch 001 | loss 0.476 | nll_loss 0.036 | accuracy 85.7 | tp 24748 | tn 33002 | fp 4567 | fn 5032 | false 9599 | wps 830.8 | ups 1.94 | wpb 427.4 | bsz 32 | num_updates 2105 | lr 8.99451e-07 | gnorm 24.871 | train_wall 1047 | wall 1087\n",
            "epoch 002:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 17:27:05 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002: 100% 2104/2105 [17:40<00:00,  1.97it/s, loss=0.287, nll_loss=0.022, accuracy=92, tp=1287, tn=1656, fp=127, fn=130, false=257, wps=830.1, ups=1.99, wpb=418.1, bsz=32, num_updates=4200, lr=7.99379e-07, gnorm=20.497, train_wall=50, wall=2144]2022-12-15 17:44:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.50it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 15/28 [00:06<00:05,  2.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 20/28 [00:08<00:03,  2.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.54it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 17:44:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.3 | nll_loss 0.012 | accuracy 92.7 | tp 391 | tn 417 | fp 27 | fn 37 | false 64 | wps 1968.9 | wpb 778.9 | bsz 31.1 | num_updates 4210 | best_accuracy 92.7\n",
            "2022-12-15 17:44:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 17:45:19 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint2.pt (epoch 2 @ 4210 updates, score 92.7) (writing took 20.572670933000154 seconds)\n",
            "2022-12-15 17:45:19 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-12-15 17:45:19 | INFO | train | epoch 002 | loss 0.316 | nll_loss 0.024 | accuracy 91 | tp 26717 | tn 34595 | fp 2974 | fn 3063 | false 6037 | wps 823 | ups 1.93 | wpb 427.4 | bsz 32 | num_updates 4210 | lr 7.98901e-07 | gnorm 21.967 | train_wall 1054 | wall 2181\n",
            "epoch 003:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 17:45:19 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 2104/2105 [17:41<00:00,  2.02it/s, loss=0.252, nll_loss=0.019, accuracy=92.8, tp=1300, tn=1671, fp=117, fn=112, false=229, wps=851.1, ups=1.99, wpb=427.8, bsz=32, num_updates=6300, lr=6.99069e-07, gnorm=18.325, train_wall=50, wall=3235]2022-12-15 18:03:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  54% 15/28 [00:06<00:05,  2.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 20/28 [00:08<00:03,  2.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.57it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.58it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 18:03:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.306 | nll_loss 0.012 | accuracy 92.9 | tp 393 | tn 417 | fp 27 | fn 35 | false 62 | wps 1980 | wpb 778.9 | bsz 31.1 | num_updates 6315 | best_accuracy 92.9\n",
            "2022-12-15 18:03:12 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 18:03:32 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint3.pt (epoch 3 @ 6315 updates, score 92.9) (writing took 20.38014332900002 seconds)\n",
            "2022-12-15 18:03:32 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-12-15 18:03:32 | INFO | train | epoch 003 | loss 0.269 | nll_loss 0.02 | accuracy 92.4 | tp 27274 | tn 34985 | fp 2584 | fn 2506 | false 5090 | wps 823 | ups 1.93 | wpb 427.4 | bsz 32 | num_updates 6315 | lr 6.98352e-07 | gnorm 19.766 | train_wall 1054 | wall 3274\n",
            "epoch 004:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 18:03:32 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 2104/2105 [17:42<00:00,  1.98it/s, loss=0.237, nll_loss=0.018, accuracy=93.5, tp=1295, tn=1698, fp=109, fn=98, false=207, wps=838.8, ups=1.99, wpb=421.6, bsz=32, num_updates=8400, lr=5.98758e-07, gnorm=19.914, train_wall=50, wall=4327]2022-12-15 18:21:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.41it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.44it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.52it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.56it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.56it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.54it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  54% 15/28 [00:06<00:05,  2.52it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 20/28 [00:07<00:03,  2.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.61it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 18:21:27 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.288 | nll_loss 0.012 | accuracy 93.2 | tp 396 | tn 417 | fp 27 | fn 32 | false 59 | wps 1984.1 | wpb 778.9 | bsz 31.1 | num_updates 8420 | best_accuracy 93.2\n",
            "2022-12-15 18:21:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 18:21:49 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint4.pt (epoch 4 @ 8420 updates, score 93.2) (writing took 22.89525140099977 seconds)\n",
            "2022-12-15 18:21:49 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-12-15 18:21:49 | INFO | train | epoch 004 | loss 0.244 | nll_loss 0.018 | accuracy 93.2 | tp 27532 | tn 35219 | fp 2350 | fn 2248 | false 4598 | wps 819.9 | ups 1.92 | wpb 427.4 | bsz 32 | num_updates 8420 | lr 5.97803e-07 | gnorm 18.898 | train_wall 1056 | wall 4371\n",
            "epoch 005:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 18:21:49 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 2104/2105 [17:39<00:00,  1.99it/s, loss=0.212, nll_loss=0.016, accuracy=94.6, tp=1303, tn=1725, fp=91, fn=81, false=172, wps=837.7, ups=1.99, wpb=421.3, bsz=32, num_updates=10500, lr=4.98448e-07, gnorm=17.339, train_wall=50, wall=5418]2022-12-15 18:39:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  54% 15/28 [00:05<00:05,  2.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 20/28 [00:07<00:03,  2.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 23/28 [00:09<00:02,  2.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.55it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 18:39:40 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.294 | nll_loss 0.012 | accuracy 93.7 | tp 397 | tn 420 | fp 24 | fn 31 | false 55 | wps 1981.9 | wpb 778.9 | bsz 31.1 | num_updates 10525 | best_accuracy 93.7\n",
            "2022-12-15 18:39:40 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 18:40:02 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint5.pt (epoch 5 @ 10525 updates, score 93.7) (writing took 21.538950258000114 seconds)\n",
            "2022-12-15 18:40:02 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-12-15 18:40:02 | INFO | train | epoch 005 | loss 0.226 | nll_loss 0.017 | accuracy 93.9 | tp 27758 | tn 35457 | fp 2112 | fn 2022 | false 4134 | wps 823.7 | ups 1.93 | wpb 427.4 | bsz 32 | num_updates 10525 | lr 4.97253e-07 | gnorm 17.768 | train_wall 1052 | wall 5464\n",
            "epoch 006:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 18:40:02 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 2104/2105 [17:43<00:00,  1.99it/s, loss=0.198, nll_loss=0.015, accuracy=94.8, tp=1347, tn=1687, fp=90, fn=76, false=166, wps=834.4, ups=1.97, wpb=422.8, bsz=32, num_updates=12600, lr=3.98137e-07, gnorm=17.338, train_wall=50, wall=6512]2022-12-15 18:57:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.93it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.23it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.38it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.45it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.47it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.51it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.53it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  54% 15/28 [00:05<00:05,  2.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 20/28 [00:07<00:03,  2.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.53it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.53it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.55it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.57it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.61it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 18:57:57 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.307 | nll_loss 0.012 | accuracy 93.5 | tp 394 | tn 421 | fp 23 | fn 34 | false 57 | wps 1990.3 | wpb 778.9 | bsz 31.1 | num_updates 12630 | best_accuracy 93.7\n",
            "2022-12-15 18:57:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 18:58:10 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint6.pt (epoch 6 @ 12630 updates, score 93.5) (writing took 13.41592836000018 seconds)\n",
            "2022-12-15 18:58:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-12-15 18:58:10 | INFO | train | epoch 006 | loss 0.215 | nll_loss 0.016 | accuracy 94.1 | tp 27840 | tn 35548 | fp 2021 | fn 1940 | false 3961 | wps 826.9 | ups 1.93 | wpb 427.4 | bsz 32 | num_updates 12630 | lr 3.96704e-07 | gnorm 17.612 | train_wall 1056 | wall 6552\n",
            "epoch 007:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 18:58:10 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 2104/2105 [17:39<00:00,  2.01it/s, loss=0.2, nll_loss=0.015, accuracy=94.8, tp=1316, tn=1716, fp=81, fn=87, false=168, wps=848.7, ups=1.99, wpb=427, bsz=32, num_updates=14700, lr=2.97827e-07, gnorm=17.037, train_wall=50, wall=7594]2022-12-15 19:15:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.28it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.40it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.45it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.48it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.50it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.53it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.55it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  36% 10/28 [00:04<00:06,  2.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.57it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.57it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  54% 15/28 [00:05<00:05,  2.55it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.53it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.55it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.55it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.52it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  71% 20/28 [00:07<00:03,  2.54it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.53it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.52it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.57it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.55it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.58it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 19:16:01 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.297 | nll_loss 0.012 | accuracy 93 | tp 392 | tn 419 | fp 25 | fn 36 | false 61 | wps 1987.4 | wpb 778.9 | bsz 31.1 | num_updates 14735 | best_accuracy 93.7\n",
            "2022-12-15 19:16:01 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 19:16:14 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint7.pt (epoch 7 @ 14735 updates, score 93.0) (writing took 13.632629537998582 seconds)\n",
            "2022-12-15 19:16:14 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-12-15 19:16:14 | INFO | train | epoch 007 | loss 0.213 | nll_loss 0.016 | accuracy 94.2 | tp 27845 | tn 35593 | fp 1976 | fn 1935 | false 3911 | wps 829.8 | ups 1.94 | wpb 427.4 | bsz 32 | num_updates 14735 | lr 2.96155e-07 | gnorm 17.289 | train_wall 1052 | wall 7636\n",
            "epoch 008:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 19:16:14 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 2104/2105 [17:35<00:00,  2.01it/s, loss=0.22, nll_loss=0.016, accuracy=94.4, tp=1306, tn=1714, fp=90, fn=90, false=180, wps=863.1, ups=1.99, wpb=432.7, bsz=32, num_updates=16800, lr=1.97516e-07, gnorm=17.539, train_wall=50, wall=8673]2022-12-15 19:33:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.99it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.44it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.50it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.52it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  36% 10/28 [00:03<00:06,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  54% 15/28 [00:05<00:05,  2.59it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 20/28 [00:07<00:03,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.57it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 28/28 [00:10<00:00,  2.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 19:34:01 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.316 | nll_loss 0.013 | accuracy 93.3 | tp 396 | tn 418 | fp 26 | fn 32 | false 58 | wps 2009.2 | wpb 778.9 | bsz 31.1 | num_updates 16840 | best_accuracy 93.7\n",
            "2022-12-15 19:34:01 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 19:34:18 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint8.pt (epoch 8 @ 16840 updates, score 93.3) (writing took 16.629155323000305 seconds)\n",
            "2022-12-15 19:34:18 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-12-15 19:34:18 | INFO | train | epoch 008 | loss 0.209 | nll_loss 0.016 | accuracy 94.3 | tp 27903 | tn 35636 | fp 1933 | fn 1877 | false 3810 | wps 830.1 | ups 1.94 | wpb 427.4 | bsz 32 | num_updates 16840 | lr 1.95605e-07 | gnorm 16.842 | train_wall 1049 | wall 8720\n",
            "epoch 009:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 19:34:18 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 2104/2105 [17:41<00:00,  2.00it/s, loss=0.225, nll_loss=0.017, accuracy=94, tp=1324, tn=1685, fp=97, fn=94, false=191, wps=845.1, ups=1.97, wpb=428, bsz=32, num_updates=18900, lr=9.72056e-08, gnorm=17.477, train_wall=50, wall=9759]2022-12-15 19:52:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.96it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.28it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.36it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  14% 4/28 [00:01<00:09,  2.44it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.47it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.50it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.53it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  54% 15/28 [00:05<00:05,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.55it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.52it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  71% 20/28 [00:07<00:03,  2.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.53it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.53it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 25/28 [00:09<00:01,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.55it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.56it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.58it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 19:52:11 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.295 | nll_loss 0.012 | accuracy 93.8 | tp 395 | tn 423 | fp 21 | fn 33 | false 54 | wps 1986.6 | wpb 778.9 | bsz 31.1 | num_updates 18945 | best_accuracy 93.8\n",
            "2022-12-15 19:52:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 19:52:32 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint9.pt (epoch 9 @ 18945 updates, score 93.8) (writing took 20.97305889299969 seconds)\n",
            "2022-12-15 19:52:32 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-12-15 19:52:32 | INFO | train | epoch 009 | loss 0.214 | nll_loss 0.016 | accuracy 94.2 | tp 27871 | tn 35587 | fp 1982 | fn 1909 | false 3891 | wps 822.7 | ups 1.92 | wpb 427.4 | bsz 32 | num_updates 18945 | lr 9.50561e-08 | gnorm 16.668 | train_wall 1054 | wall 9814\n",
            "epoch 010:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 19:52:32 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 2104/2105 [17:44<00:00,  2.00it/s, loss=0.211, nll_loss=0.016, accuracy=94, tp=1292, tn=1715, fp=100, fn=93, false=193, wps=837.4, ups=1.98, wpb=423.5, bsz=32, num_updates=21000, lr=0, gnorm=16.991, train_wall=50, wall=10853]2022-12-15 20:10:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   4% 1/28 [00:00<00:13,  1.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.28it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.41it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  14% 4/28 [00:01<00:10,  2.39it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.37it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  21% 6/28 [00:02<00:09,  2.40it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.45it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  29% 8/28 [00:03<00:08,  2.47it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.50it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.52it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  54% 15/28 [00:06<00:05,  2.52it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.52it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.50it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  71% 20/28 [00:08<00:03,  2.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.52it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.53it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 25/28 [00:10<00:01,  2.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.52it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.52it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.55it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 20:10:28 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.311 | nll_loss 0.012 | accuracy 93.3 | tp 398 | tn 416 | fp 28 | fn 30 | false 58 | wps 1957.7 | wpb 778.9 | bsz 31.1 | num_updates 21050 | best_accuracy 93.8\n",
            "2022-12-15 20:10:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 20:10:43 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint10.pt (epoch 10 @ 21050 updates, score 93.3) (writing took 15.224567809000291 seconds)\n",
            "2022-12-15 20:10:43 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-12-15 20:10:43 | INFO | train | epoch 010 | loss 0.212 | nll_loss 0.016 | accuracy 94.2 | tp 27838 | tn 35631 | fp 1938 | fn 1942 | false 3880 | wps 824.5 | ups 1.93 | wpb 427.4 | bsz 32 | num_updates 21050 | lr 0 | gnorm 16.454 | train_wall 1058 | wall 10905\n",
            "epoch 011:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 20:10:43 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 2104/2105 [17:42<00:00,  1.96it/s, loss=0.205, nll_loss=0.015, accuracy=94.3, tp=1314, tn=1703, fp=93, fn=90, false=183, wps=842.9, ups=1.98, wpb=424.7, bsz=32, num_updates=23100, lr=0, gnorm=16.318, train_wall=50, wall=11941]2022-12-15 20:28:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   4% 1/28 [00:00<00:14,  1.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.25it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.39it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  14% 4/28 [00:01<00:10,  2.39it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.44it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  21% 6/28 [00:02<00:09,  2.42it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.46it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  29% 8/28 [00:03<00:08,  2.50it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.54it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.53it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.53it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  54% 15/28 [00:06<00:05,  2.54it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.47it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.49it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  71% 20/28 [00:08<00:03,  2.53it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.55it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 25/28 [00:10<00:01,  2.56it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.56it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.57it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.60it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 20:28:38 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.309 | nll_loss 0.012 | accuracy 93.3 | tp 397 | tn 417 | fp 27 | fn 31 | false 58 | wps 1970.4 | wpb 778.9 | bsz 31.1 | num_updates 23155 | best_accuracy 93.8\n",
            "2022-12-15 20:28:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 20:28:51 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint11.pt (epoch 11 @ 23155 updates, score 93.3) (writing took 13.484201712000868 seconds)\n",
            "2022-12-15 20:28:51 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-12-15 20:28:51 | INFO | train | epoch 011 | loss 0.212 | nll_loss 0.016 | accuracy 94.2 | tp 27843 | tn 35600 | fp 1969 | fn 1937 | false 3906 | wps 826.9 | ups 1.93 | wpb 427.4 | bsz 32 | num_updates 23155 | lr 0 | gnorm 16.591 | train_wall 1056 | wall 11993\n",
            "epoch 012:   0% 0/2105 [00:00<?, ?it/s]2022-12-15 20:28:51 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 2104/2105 [17:44<00:00,  1.99it/s, loss=0.213, nll_loss=0.016, accuracy=93.8, tp=1350, tn=1650, fp=112, fn=88, false=200, wps=866.3, ups=1.98, wpb=438.3, bsz=32, num_updates=25200, lr=0, gnorm=16.727, train_wall=50, wall=13028]2022-12-15 20:46:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   4% 1/28 [00:00<00:15,  1.79it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   7% 2/28 [00:00<00:11,  2.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  11% 3/28 [00:01<00:10,  2.32it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  14% 4/28 [00:01<00:10,  2.39it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  18% 5/28 [00:02<00:09,  2.43it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  21% 6/28 [00:02<00:08,  2.46it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  25% 7/28 [00:02<00:08,  2.50it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  29% 8/28 [00:03<00:07,  2.51it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  32% 9/28 [00:03<00:07,  2.53it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  36% 10/28 [00:04<00:07,  2.52it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39% 11/28 [00:04<00:06,  2.52it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  43% 12/28 [00:04<00:06,  2.53it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  46% 13/28 [00:05<00:05,  2.53it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 14/28 [00:05<00:05,  2.52it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  54% 15/28 [00:06<00:05,  2.53it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  57% 16/28 [00:06<00:04,  2.50it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  61% 17/28 [00:06<00:04,  2.52it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  64% 18/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  68% 19/28 [00:07<00:03,  2.51it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  71% 20/28 [00:08<00:03,  2.51it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  75% 21/28 [00:08<00:02,  2.50it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  79% 22/28 [00:08<00:02,  2.50it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 23/28 [00:09<00:01,  2.51it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  86% 24/28 [00:09<00:01,  2.54it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 25/28 [00:10<00:01,  2.55it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  93% 26/28 [00:10<00:00,  2.54it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  96% 27/28 [00:10<00:00,  2.55it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 28/28 [00:11<00:00,  2.56it/s]\u001b[A\n",
            "                                                                        \u001b[A2022-12-15 20:46:47 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.285 | nll_loss 0.011 | accuracy 93.2 | tp 393 | tn 420 | fp 24 | fn 35 | false 59 | wps 1968 | wpb 778.9 | bsz 31.1 | num_updates 25260 | best_accuracy 93.8\n",
            "2022-12-15 20:46:47 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2022-12-15 20:47:01 | INFO | fairseq.checkpoint_utils | saved checkpoint outputs/symmetric/SST-2-base/wd0.1_ad0.1_d0.1_lr1e-06/1215-170852_ckpt/checkpoint12.pt (epoch 12 @ 25260 updates, score 93.2) (writing took 13.573357314000532 seconds)\n",
            "2022-12-15 20:47:01 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-12-15 20:47:01 | INFO | train | epoch 012 | loss 0.212 | nll_loss 0.016 | accuracy 94.1 | tp 27875 | tn 35534 | fp 2035 | fn 1905 | false 3940 | wps 825.9 | ups 1.93 | wpb 427.4 | bsz 32 | num_updates 25260 | lr 0 | gnorm 16.593 | train_wall 1057 | wall 13083\n",
            "2022-12-15 20:47:01 | INFO | fairseq_cli.train | done training in 13080.2 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_cola_quantall.pt',\n",
        "    data_name_or_path='CoLA-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "\n",
        "def demo(sent1):\n",
        "  start = time.time()\n",
        "  tokens = roberta.encode(sent1)\n",
        "  prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "  prediction_label = label_fn(prediction)\n",
        "  end = time.time()\n",
        "  return prediction_label, end-start\n",
        "\n",
        "sentence = input(\"Enter sentence:\")\n",
        "prediction,infernece_time = demo(sentence)\n",
        "print(prediction, inference_time)\n",
        "  \n"
      ],
      "metadata": {
        "id": "rCtNqvksWe1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute infereance time and accuracy"
      ],
      "metadata": {
        "id": "K4RREKXKWFzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_cola_quantall.pt',\n",
        "    data_name_or_path='CoLA-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/CoLA/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[3], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hrcUQGZnIuH",
        "outputId": "fc5bc6e1-bd22-458e-81df-e778d053149a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "average inference time 0.47691840171813965\n",
            "\n",
            "| Accuracy:  0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_cola_nonlinear.pt',\n",
        "    data_name_or_path='CoLA-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/CoLA/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[3], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXjOAJ21qj4d",
        "outputId": "7ed8d38b-e02a-4ffc-fa0a-b3be0fe974d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "average inference time 0.2642598366737366\n",
            "\n",
            "| Accuracy:  0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_cola_gelu.pt',\n",
        "    data_name_or_path='CoLA-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/CoLA/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[3], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBbCfZbCs7tI",
        "outputId": "225566ee-1f4e-45f0-867c-d1408868da05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "average inference time 0.35001373291015625\n",
            "\n",
            "| Accuracy:  0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_cola_softmax.pt',\n",
        "    data_name_or_path='CoLA-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/CoLA/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[3], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9OIHqlptSEY",
        "outputId": "4b093b83-4c60-4ec1-a3f2-047e92f8d6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "average inference time 0.4488652729988098\n",
            "\n",
            "| Accuracy:  0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_cola_layernorm.pt',\n",
        "    data_name_or_path='CoLA-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/CoLA/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[3], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VcqZfxyt743",
        "outputId": "c547d52e-c51c-4d69-e24e-a9d6867c9745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'CoLA-bin'}\n",
            "average inference time 0.38112245082855223\n",
            "\n",
            "| Accuracy:  0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_sst_layernorm.pt',\n",
        "    data_name_or_path='SST-2-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/SST-2/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[0], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsXvVQ5HuZzY",
        "outputId": "9b887cfd-6a93-498e-df7d-2e53d6bb5fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "average inference time 0.38961639881134036\n",
            "\n",
            "| Accuracy:  0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_sst_softmax.pt',\n",
        "    data_name_or_path='SST-2-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/SST-2/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[0], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf4wagezwvZu",
        "outputId": "56b08a00-a12b-446c-b8a0-a940798839e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "average inference time 0.4357535481452942\n",
            "\n",
            "| Accuracy:  0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_sst_quantall.pt',\n",
        "    data_name_or_path='SST-2-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/SST-2/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[0], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZK9jOhv0Ej5",
        "outputId": "c24376c3-4390-48e0-e4eb-44975598a1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "average inference time 0.4504515981674194\n",
            "\n",
            "| Accuracy:  0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_sst_gelu.pt',\n",
        "    data_name_or_path='SST-2-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/SST-2/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[0], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApcUWbg00gNp",
        "outputId": "8f5c8128-03ae-4624-83e1-cfe3333ddaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "average inference time 0.35047110795974734\n",
            "\n",
            "| Accuracy:  0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "import time\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_sst_nonlinear.pt',\n",
        "    data_name_or_path='SST-2-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "count = 0 \n",
        "total_time = 0\n",
        "with open('glue_data/SST-2/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    start = time.time()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, target = tokens[0], tokens[1]\n",
        "        start = time.time()\n",
        "        tokens = roberta.encode(sent1)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        end = time.time()\n",
        "        total_time += end - start\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "        end = time.time()\n",
        "        count += 1\n",
        "        if  count >= 100:\n",
        "          break\n",
        "print('average inference time', (total_time)/count)\n",
        "\n",
        "print()\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I64bG0Nh03hd",
        "outputId": "94a4991d-6b7e-4fd6-e860-24b5427e9385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test1\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "dddddddddd\n",
            "{'bpe': 'gpt2', 'load_checkpoint_heads': True, 'data': 'SST-2-bin'}\n",
            "average inference time 0.2842299175262451\n",
            "\n",
            "| Accuracy:  0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-JaklgrFEK6",
        "outputId": "2de34e86-7f32-41e4-951b-c3911d2a3a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/coms6998_project/I-BERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute model size\n",
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/.shortcut-targets-by-id/1RujN8KS4gYHaZ5MKM6blAy_V-kAsq3Rr/I-BERT')\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "model = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best_cola_quantall.pt',\n",
        "    data_name_or_path='CoLA-bin'\n",
        ")\n",
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXHeVo2dj23L",
        "outputId": "4c9af013-608e-40ed-fc78-d91ad5efc5a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 477.939MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model.cuda()\n",
        "memory_summary = torch.cuda.memory_summary()\n",
        "print(memory_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF2GJNWIvV7_",
        "outputId": "03f09b06-c5bf-4c3a-cc6c-cfb1636c3a79"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |     953 MB |     953 MB |    1101 MB |  151552 KB |\n",
            "|       from large pool |     951 MB |     951 MB |    1099 MB |  151552 KB |\n",
            "|       from small pool |       1 MB |       1 MB |       1 MB |       0 KB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |     953 MB |     953 MB |    1101 MB |  151552 KB |\n",
            "|       from large pool |     951 MB |     951 MB |    1099 MB |  151552 KB |\n",
            "|       from small pool |       1 MB |       1 MB |       1 MB |       0 KB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    1038 MB |    1038 MB |    1038 MB |       0 B  |\n",
            "|       from large pool |    1036 MB |    1036 MB |    1036 MB |       0 B  |\n",
            "|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   86970 KB |   91814 KB |  509433 KB |  422463 KB |\n",
            "|       from large pool |   86516 KB |   91124 KB |  507386 KB |  420870 KB |\n",
            "|       from small pool |     454 KB |    2047 KB |    2047 KB |    1593 KB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     937    |     937    |     938    |       1    |\n",
            "|       from large pool |     150    |     150    |     151    |       1    |\n",
            "|       from small pool |     787    |     787    |     787    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     937    |     937    |     938    |       1    |\n",
            "|       from large pool |     150    |     150    |     151    |       1    |\n",
            "|       from small pool |     787    |     787    |     787    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      39    |      39    |      39    |       0    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      38    |      38    |       0    |\n",
            "|       from large pool |      37    |      37    |      37    |       0    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MujswF9I0IZS",
        "outputId": "04974773-b4cd-48b9-c423-ed00230ab4ab"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 96.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "model = RobertaModel.from_pretrained(\"textattack/roberta-base-CoLA\")\n",
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "#for buffer in model.buffers():\n",
        " #  buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172,
          "referenced_widgets": [
            "290aeb45914f497aa8700c8290ac6008",
            "51b6e6b262b24294a38f893035bcfc03",
            "f0819b7d217e4d3cb65fab3caf769d6c",
            "d040f4276e634bdb93126dd60620fa71",
            "35a41e8fbd674e06ae0c824333e93527",
            "796252f9a2eb4a84b555357cfe97f58e",
            "983ba8d66a1e453887c8387840277161",
            "ec77aa6645b04f1583a7d41c68d7de36",
            "134fa6661d2a468bab8d0323a8b8a86f",
            "a5895df2e9c44edc9e6c407e59d5ca96",
            "7f5fc58257254ba68e3f38964f29cc68",
            "ecd17d8e38234647b8fd7364d3b294a2",
            "70f17e2c0c0643569c4eddceb97c463c",
            "2791cb7f72fc493fbfd3fd45ec6b54a0",
            "70223a61d9914e4794b1ce33e6518b28",
            "9f8c6a2f2a9149cb897cc7991169f80a",
            "e9115b02005549e6904d3d9e61cc5c84",
            "5f04439def0f4e05807e99717bce778b",
            "4b285a8c35c24b27a0fa2cc4097e51c9",
            "c26728c586a84cad9e7077252652c5d0",
            "deace6bca19f405a81aa29d180b656d7",
            "6db309adaf934a988754f7115cbf6e1f"
          ]
        },
        "id": "p-4K2-Pg0Qws",
        "outputId": "e705223a-9873-4ab4-a374-ab42330079b8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/564 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "290aeb45914f497aa8700c8290ac6008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecd17d8e38234647b8fd7364d3b294a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 475.485MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print roberta base model size\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "model = RobertaModel.from_pretrained(\"kssteven/ibert-roberta-base\")\n",
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "#for buffer in model.buffers():\n",
        " #  buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "7b1e64a1f76b4961a211cd54bad1d3ac",
            "a79e04e9606f490aa3814597819f0fc3",
            "d23984593ba542f394c502d57e100223",
            "ee666649a07f479d8a0855bd9fa3794f",
            "afea9d4e464f4a32a1655dbf352d1f31",
            "c1eb47127bc544d0beebcf7d59d4981a",
            "85c2226819f84539bad12d33014b9958",
            "2ae61b2f8c5842188936f22a96e85066",
            "032cf2dbc30c44d9b05a86acb89ffee6",
            "1bf64dc46b6640fba627a22bf9f5e180",
            "28450846eb104d41a221038d0601cc64",
            "17d441a42de94b43a990dee935f48f08",
            "44bb45691c1148798ff23d20366597f7",
            "97710f0ec4bf4885a7ca9c9082466edb",
            "20bbec34fca44551938995ff49ce7a40",
            "0c4e21eb21744e8a916eb9b954f12569",
            "dd194acdba8f476fb131f1002114abce",
            "bc73f4912b964ba58ec0d71dd554e7f9",
            "133995023cad43519c012f960576a757",
            "fcc72afc182a40eea1c8997cd14d8f8a",
            "a19e45c91feb4e419ce728c0ee62ad04",
            "0dd79776568949d9aba92cf128c5199d"
          ]
        },
        "id": "y8NTrA1w1UXY",
        "outputId": "67359e03-837b-417c-a1c4-dc2d419e8018"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/541 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b1e64a1f76b4961a211cd54bad1d3ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type ibert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17d441a42de94b43a990dee935f48f08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at kssteven/ibert-roberta-base were not used when initializing RobertaModel: ['ibert.embeddings.LayerNorm.bias', 'ibert.encoder.layer.6.attention.self.key.bias', 'ibert.encoder.layer.10.output.LayerNorm.bias', 'ibert.encoder.layer.0.attention.self.value.bias', 'ibert.encoder.layer.0.attention.self.value.weight', 'ibert.encoder.layer.2.output.LayerNorm.bias', 'ibert.encoder.layer.6.attention.output.dense.weight', 'ibert.encoder.layer.3.attention.self.query.bias', 'ibert.encoder.layer.5.attention.self.value.weight', 'ibert.encoder.layer.7.attention.self.value.bias', 'ibert.encoder.layer.0.output.LayerNorm.bias', 'ibert.encoder.layer.9.output.LayerNorm.bias', 'ibert.encoder.layer.2.attention.self.query.weight', 'ibert.encoder.layer.1.attention.output.dense.weight', 'ibert.encoder.layer.10.intermediate.dense.bias', 'ibert.encoder.layer.4.attention.output.dense.weight', 'ibert.encoder.layer.10.attention.self.value.bias', 'ibert.encoder.layer.11.output.dense.weight', 'lm_head.layer_norm.weight', 'ibert.encoder.layer.4.attention.output.dense.bias', 'ibert.encoder.layer.8.attention.output.dense.weight', 'ibert.encoder.layer.5.attention.self.key.weight', 'ibert.encoder.layer.6.attention.output.LayerNorm.bias', 'ibert.encoder.layer.0.attention.output.LayerNorm.bias', 'ibert.encoder.layer.1.attention.self.value.weight', 'ibert.encoder.layer.7.output.dense.weight', 'ibert.encoder.layer.6.output.LayerNorm.weight', 'ibert.encoder.layer.9.attention.self.key.weight', 'ibert.encoder.layer.0.attention.self.query.bias', 'ibert.encoder.layer.9.attention.output.dense.weight', 'ibert.encoder.layer.11.attention.self.key.bias', 'ibert.encoder.layer.11.output.LayerNorm.bias', 'ibert.encoder.layer.10.attention.output.dense.weight', 'ibert.encoder.layer.10.output.dense.bias', 'ibert.encoder.layer.0.attention.self.key.bias', 'ibert.encoder.layer.8.attention.self.value.weight', 'ibert.encoder.layer.9.intermediate.dense.bias', 'ibert.pooler.dense.bias', 'ibert.encoder.layer.0.attention.output.LayerNorm.weight', 'ibert.encoder.layer.8.attention.output.dense.bias', 'ibert.encoder.layer.2.attention.self.query.bias', 'ibert.encoder.layer.8.attention.self.key.bias', 'ibert.encoder.layer.2.intermediate.dense.weight', 'ibert.embeddings.position_embeddings.weight', 'ibert.encoder.layer.5.intermediate.dense.weight', 'ibert.encoder.layer.7.attention.self.key.weight', 'ibert.encoder.layer.6.intermediate.dense.bias', 'ibert.encoder.layer.11.attention.self.value.bias', 'lm_head.decoder.weight', 'ibert.encoder.layer.7.attention.self.query.weight', 'ibert.encoder.layer.6.attention.output.LayerNorm.weight', 'ibert.encoder.layer.2.attention.self.key.bias', 'ibert.encoder.layer.11.attention.output.LayerNorm.weight', 'ibert.encoder.layer.2.attention.self.value.bias', 'ibert.encoder.layer.10.output.LayerNorm.weight', 'ibert.encoder.layer.3.attention.self.value.weight', 'ibert.encoder.layer.7.attention.output.LayerNorm.weight', 'ibert.encoder.layer.3.attention.output.LayerNorm.bias', 'ibert.encoder.layer.9.attention.self.value.weight', 'ibert.encoder.layer.6.attention.self.value.weight', 'ibert.encoder.layer.1.intermediate.dense.weight', 'ibert.encoder.layer.9.attention.self.key.bias', 'lm_head.layer_norm.bias', 'ibert.encoder.layer.5.output.LayerNorm.weight', 'ibert.encoder.layer.1.attention.output.LayerNorm.weight', 'ibert.encoder.layer.4.attention.output.LayerNorm.weight', 'ibert.encoder.layer.7.intermediate.dense.weight', 'ibert.encoder.layer.10.intermediate.dense.weight', 'ibert.embeddings.token_type_embeddings.weight', 'ibert.encoder.layer.0.output.dense.bias', 'ibert.encoder.layer.10.attention.output.LayerNorm.weight', 'ibert.encoder.layer.2.intermediate.dense.bias', 'ibert.encoder.layer.9.attention.output.LayerNorm.weight', 'ibert.encoder.layer.4.attention.self.query.bias', 'ibert.encoder.layer.7.attention.self.key.bias', 'ibert.encoder.layer.5.attention.output.LayerNorm.bias', 'ibert.embeddings.word_embeddings.weight', 'ibert.encoder.layer.11.attention.self.query.weight', 'ibert.encoder.layer.3.output.dense.weight', 'ibert.encoder.layer.0.intermediate.dense.bias', 'ibert.encoder.layer.1.attention.self.value.bias', 'ibert.encoder.layer.6.attention.self.value.bias', 'ibert.encoder.layer.10.attention.self.query.weight', 'ibert.encoder.layer.1.output.LayerNorm.bias', 'ibert.encoder.layer.5.output.dense.bias', 'ibert.encoder.layer.1.attention.self.query.weight', 'ibert.encoder.layer.5.output.dense.weight', 'ibert.encoder.layer.4.attention.self.value.weight', 'ibert.encoder.layer.1.output.LayerNorm.weight', 'ibert.encoder.layer.8.output.dense.bias', 'ibert.encoder.layer.6.attention.output.dense.bias', 'ibert.encoder.layer.7.attention.output.dense.weight', 'ibert.encoder.layer.2.attention.output.LayerNorm.bias', 'ibert.encoder.layer.9.intermediate.dense.weight', 'ibert.encoder.layer.9.output.LayerNorm.weight', 'ibert.encoder.layer.3.attention.self.key.weight', 'ibert.encoder.layer.7.output.LayerNorm.weight', 'ibert.encoder.layer.7.output.dense.bias', 'ibert.encoder.layer.9.attention.output.LayerNorm.bias', 'ibert.encoder.layer.8.output.dense.weight', 'ibert.encoder.layer.11.attention.output.LayerNorm.bias', 'ibert.encoder.layer.11.output.dense.bias', 'ibert.encoder.layer.4.attention.output.LayerNorm.bias', 'ibert.encoder.layer.9.attention.self.value.bias', 'ibert.encoder.layer.0.attention.self.key.weight', 'ibert.encoder.layer.9.attention.self.query.weight', 'ibert.encoder.layer.6.output.dense.bias', 'ibert.encoder.layer.1.attention.self.key.weight', 'ibert.encoder.layer.0.attention.output.dense.weight', 'ibert.encoder.layer.11.intermediate.dense.weight', 'ibert.encoder.layer.2.output.dense.bias', 'ibert.encoder.layer.1.attention.self.query.bias', 'ibert.encoder.layer.2.output.dense.weight', 'ibert.encoder.layer.7.attention.self.query.bias', 'ibert.encoder.layer.3.attention.self.value.bias', 'ibert.encoder.layer.8.output.LayerNorm.weight', 'ibert.encoder.layer.4.output.LayerNorm.weight', 'ibert.encoder.layer.11.output.LayerNorm.weight', 'ibert.encoder.layer.1.output.dense.bias', 'ibert.encoder.layer.3.attention.output.LayerNorm.weight', 'ibert.encoder.layer.1.attention.self.key.bias', 'ibert.encoder.layer.7.attention.output.dense.bias', 'ibert.encoder.layer.8.output.LayerNorm.bias', 'ibert.encoder.layer.9.attention.self.query.bias', 'ibert.encoder.layer.9.output.dense.weight', 'ibert.encoder.layer.0.attention.output.dense.bias', 'ibert.encoder.layer.8.attention.output.LayerNorm.weight', 'ibert.encoder.layer.7.attention.self.value.weight', 'ibert.encoder.layer.9.attention.output.dense.bias', 'ibert.encoder.layer.4.intermediate.dense.bias', 'ibert.encoder.layer.8.intermediate.dense.bias', 'ibert.encoder.layer.10.attention.self.query.bias', 'ibert.encoder.layer.2.attention.output.dense.bias', 'ibert.encoder.layer.8.attention.self.query.bias', 'ibert.encoder.layer.1.attention.output.LayerNorm.bias', 'ibert.encoder.layer.1.intermediate.dense.bias', 'ibert.encoder.layer.11.attention.self.key.weight', 'ibert.encoder.layer.10.attention.self.value.weight', 'ibert.pooler.dense.weight', 'ibert.encoder.layer.7.intermediate.dense.bias', 'ibert.encoder.layer.8.attention.self.value.bias', 'ibert.encoder.layer.5.output.LayerNorm.bias', 'ibert.encoder.layer.3.attention.self.query.weight', 'ibert.encoder.layer.0.intermediate.dense.weight', 'ibert.encoder.layer.8.intermediate.dense.weight', 'ibert.encoder.layer.2.attention.self.value.weight', 'ibert.encoder.layer.4.attention.self.value.bias', 'ibert.encoder.layer.0.attention.self.query.weight', 'ibert.encoder.layer.5.attention.self.key.bias', 'ibert.encoder.layer.7.output.LayerNorm.bias', 'ibert.encoder.layer.0.output.LayerNorm.weight', 'ibert.encoder.layer.11.attention.self.query.bias', 'ibert.encoder.layer.3.output.dense.bias', 'ibert.encoder.layer.5.attention.self.query.bias', 'ibert.encoder.layer.11.attention.output.dense.bias', 'ibert.encoder.layer.2.output.LayerNorm.weight', 'ibert.encoder.layer.4.output.LayerNorm.bias', 'ibert.encoder.layer.6.output.LayerNorm.bias', 'ibert.encoder.layer.2.attention.output.LayerNorm.weight', 'ibert.embeddings.LayerNorm.weight', 'ibert.encoder.layer.4.attention.self.key.bias', 'ibert.encoder.layer.3.attention.self.key.bias', 'ibert.encoder.layer.10.output.dense.weight', 'ibert.encoder.layer.1.attention.output.dense.bias', 'ibert.encoder.layer.11.attention.output.dense.weight', 'ibert.encoder.layer.6.output.dense.weight', 'ibert.encoder.layer.2.attention.self.key.weight', 'ibert.encoder.layer.3.output.LayerNorm.bias', 'ibert.encoder.layer.3.attention.output.dense.bias', 'ibert.encoder.layer.3.output.LayerNorm.weight', 'ibert.encoder.layer.4.output.dense.weight', 'ibert.encoder.layer.6.attention.self.query.weight', 'ibert.encoder.layer.8.attention.output.LayerNorm.bias', 'ibert.encoder.layer.2.attention.output.dense.weight', 'ibert.encoder.layer.4.intermediate.dense.weight', 'ibert.encoder.layer.4.output.dense.bias', 'ibert.encoder.layer.0.output.dense.weight', 'ibert.encoder.layer.10.attention.self.key.weight', 'ibert.encoder.layer.4.attention.self.key.weight', 'ibert.encoder.layer.3.intermediate.dense.weight', 'ibert.encoder.layer.6.intermediate.dense.weight', 'ibert.encoder.layer.8.attention.self.query.weight', 'ibert.encoder.layer.1.output.dense.weight', 'ibert.encoder.layer.6.attention.self.key.weight', 'ibert.encoder.layer.3.intermediate.dense.bias', 'ibert.encoder.layer.4.attention.self.query.weight', 'ibert.encoder.layer.5.attention.output.LayerNorm.weight', 'ibert.encoder.layer.10.attention.self.key.bias', 'ibert.encoder.layer.3.attention.output.dense.weight', 'ibert.encoder.layer.5.attention.output.dense.weight', 'ibert.encoder.layer.5.intermediate.dense.bias', 'ibert.encoder.layer.8.attention.self.key.weight', 'ibert.encoder.layer.10.attention.output.dense.bias', 'ibert.encoder.layer.10.attention.output.LayerNorm.bias', 'ibert.encoder.layer.5.attention.self.query.weight', 'lm_head.bias', 'lm_head.dense.bias', 'ibert.encoder.layer.5.attention.self.value.bias', 'ibert.encoder.layer.6.attention.self.query.bias', 'ibert.encoder.layer.9.output.dense.bias', 'ibert.encoder.layer.7.attention.output.LayerNorm.bias', 'ibert.encoder.layer.11.intermediate.dense.bias', 'lm_head.dense.weight', 'ibert.encoder.layer.5.attention.output.dense.bias', 'ibert.encoder.layer.11.attention.self.value.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at kssteven/ibert-roberta-base and are newly initialized: ['encoder.layer.7.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 475.485MB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "290aeb45914f497aa8700c8290ac6008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51b6e6b262b24294a38f893035bcfc03",
              "IPY_MODEL_f0819b7d217e4d3cb65fab3caf769d6c",
              "IPY_MODEL_d040f4276e634bdb93126dd60620fa71"
            ],
            "layout": "IPY_MODEL_35a41e8fbd674e06ae0c824333e93527"
          }
        },
        "51b6e6b262b24294a38f893035bcfc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_796252f9a2eb4a84b555357cfe97f58e",
            "placeholder": "​",
            "style": "IPY_MODEL_983ba8d66a1e453887c8387840277161",
            "value": "Downloading: 100%"
          }
        },
        "f0819b7d217e4d3cb65fab3caf769d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec77aa6645b04f1583a7d41c68d7de36",
            "max": 564,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_134fa6661d2a468bab8d0323a8b8a86f",
            "value": 564
          }
        },
        "d040f4276e634bdb93126dd60620fa71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5895df2e9c44edc9e6c407e59d5ca96",
            "placeholder": "​",
            "style": "IPY_MODEL_7f5fc58257254ba68e3f38964f29cc68",
            "value": " 564/564 [00:00&lt;00:00, 42.1kB/s]"
          }
        },
        "35a41e8fbd674e06ae0c824333e93527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "796252f9a2eb4a84b555357cfe97f58e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "983ba8d66a1e453887c8387840277161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec77aa6645b04f1583a7d41c68d7de36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "134fa6661d2a468bab8d0323a8b8a86f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5895df2e9c44edc9e6c407e59d5ca96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f5fc58257254ba68e3f38964f29cc68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecd17d8e38234647b8fd7364d3b294a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70f17e2c0c0643569c4eddceb97c463c",
              "IPY_MODEL_2791cb7f72fc493fbfd3fd45ec6b54a0",
              "IPY_MODEL_70223a61d9914e4794b1ce33e6518b28"
            ],
            "layout": "IPY_MODEL_9f8c6a2f2a9149cb897cc7991169f80a"
          }
        },
        "70f17e2c0c0643569c4eddceb97c463c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9115b02005549e6904d3d9e61cc5c84",
            "placeholder": "​",
            "style": "IPY_MODEL_5f04439def0f4e05807e99717bce778b",
            "value": "Downloading: 100%"
          }
        },
        "2791cb7f72fc493fbfd3fd45ec6b54a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b285a8c35c24b27a0fa2cc4097e51c9",
            "max": 501003010,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c26728c586a84cad9e7077252652c5d0",
            "value": 501003010
          }
        },
        "70223a61d9914e4794b1ce33e6518b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deace6bca19f405a81aa29d180b656d7",
            "placeholder": "​",
            "style": "IPY_MODEL_6db309adaf934a988754f7115cbf6e1f",
            "value": " 501M/501M [00:23&lt;00:00, 22.7MB/s]"
          }
        },
        "9f8c6a2f2a9149cb897cc7991169f80a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9115b02005549e6904d3d9e61cc5c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f04439def0f4e05807e99717bce778b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b285a8c35c24b27a0fa2cc4097e51c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26728c586a84cad9e7077252652c5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "deace6bca19f405a81aa29d180b656d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6db309adaf934a988754f7115cbf6e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b1e64a1f76b4961a211cd54bad1d3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a79e04e9606f490aa3814597819f0fc3",
              "IPY_MODEL_d23984593ba542f394c502d57e100223",
              "IPY_MODEL_ee666649a07f479d8a0855bd9fa3794f"
            ],
            "layout": "IPY_MODEL_afea9d4e464f4a32a1655dbf352d1f31"
          }
        },
        "a79e04e9606f490aa3814597819f0fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1eb47127bc544d0beebcf7d59d4981a",
            "placeholder": "​",
            "style": "IPY_MODEL_85c2226819f84539bad12d33014b9958",
            "value": "Downloading: 100%"
          }
        },
        "d23984593ba542f394c502d57e100223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ae61b2f8c5842188936f22a96e85066",
            "max": 541,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_032cf2dbc30c44d9b05a86acb89ffee6",
            "value": 541
          }
        },
        "ee666649a07f479d8a0855bd9fa3794f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf64dc46b6640fba627a22bf9f5e180",
            "placeholder": "​",
            "style": "IPY_MODEL_28450846eb104d41a221038d0601cc64",
            "value": " 541/541 [00:00&lt;00:00, 41.8kB/s]"
          }
        },
        "afea9d4e464f4a32a1655dbf352d1f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1eb47127bc544d0beebcf7d59d4981a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85c2226819f84539bad12d33014b9958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ae61b2f8c5842188936f22a96e85066": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "032cf2dbc30c44d9b05a86acb89ffee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bf64dc46b6640fba627a22bf9f5e180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28450846eb104d41a221038d0601cc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17d441a42de94b43a990dee935f48f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44bb45691c1148798ff23d20366597f7",
              "IPY_MODEL_97710f0ec4bf4885a7ca9c9082466edb",
              "IPY_MODEL_20bbec34fca44551938995ff49ce7a40"
            ],
            "layout": "IPY_MODEL_0c4e21eb21744e8a916eb9b954f12569"
          }
        },
        "44bb45691c1148798ff23d20366597f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd194acdba8f476fb131f1002114abce",
            "placeholder": "​",
            "style": "IPY_MODEL_bc73f4912b964ba58ec0d71dd554e7f9",
            "value": "Downloading: 100%"
          }
        },
        "97710f0ec4bf4885a7ca9c9082466edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_133995023cad43519c012f960576a757",
            "max": 501212779,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcc72afc182a40eea1c8997cd14d8f8a",
            "value": 501212779
          }
        },
        "20bbec34fca44551938995ff49ce7a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a19e45c91feb4e419ce728c0ee62ad04",
            "placeholder": "​",
            "style": "IPY_MODEL_0dd79776568949d9aba92cf128c5199d",
            "value": " 501M/501M [00:23&lt;00:00, 22.5MB/s]"
          }
        },
        "0c4e21eb21744e8a916eb9b954f12569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd194acdba8f476fb131f1002114abce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc73f4912b964ba58ec0d71dd554e7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "133995023cad43519c012f960576a757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc72afc182a40eea1c8997cd14d8f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a19e45c91feb4e419ce728c0ee62ad04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dd79776568949d9aba92cf128c5199d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}